{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "API_KEYS = ['LANGCHAIN_API_KEY', 'OPENAI_API_KEY', 'LANGCHAIN_TRACING_V2', 'LANGCHAIN_ENDPOINT', \n",
    "            'LANGCHAIN_PROJECT', 'TAVILY_API_KEY', 'GROQ_API_KEY']\n",
    "for api_key in API_KEYS:\n",
    "    os.environ[api_key] = os.getenv(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are some recent advancements in attention mechanisms within machine learning and how do they enhance model performance across various domains?'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import operator\n",
    "from typing import Annotated\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "model = ChatOpenAI()\n",
    "vector_db = Chroma(embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "class ResearchStateInput(TypedDict):\n",
    "    question: str\n",
    "    content: Annotated[list, operator.add]\n",
    "    new_questions: list\n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    question: str\n",
    "    flattened_docs: list\n",
    "    summary: str\n",
    "    citations: list\n",
    "    fact_check: str\n",
    "    errors: list = []\n",
    "\n",
    "\n",
    "def get_user_input(state: ResearchStateInput):\n",
    "    return {'question': state['question']}\n",
    "\n",
    "def wikiloader(state: ResearchStateInput):\n",
    "    \"\"\"To load the information from Wikipedia based on the user input question.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wiki_result = wikipedia.run(question)\n",
    "    doc = Document(page_content=wiki_result, metadata={\"source\": \"Wikipedia\", \"title\": question})\n",
    "    return {'content': [doc]}\n",
    "\n",
    "def arxiv_loader(state: ResearchStateInput):\n",
    "    \"\"\"To load the information from Arxiv based on the user input question.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    arxiv = ArxivLoader(\n",
    "                        query=question,\n",
    "                        load_max_docs=2)\n",
    "    content = arxiv.load()\n",
    "\n",
    "    return {'content': [content]}\n",
    "\n",
    "def query_translation(state: ResearchStateInput):\n",
    "    \"\"\"To translate user input queries into better queries/questions that can be used to retrieve information from the web.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    query_translation_prompt = \"\"\"You are expert at translating user input queries into a better queries/questions that \n",
    "                        can be used to retrieve information from the web. Here is your user-input question: {question}\\n\n",
    "                        Output (2 queries):\"\"\"\n",
    "\n",
    "    question_template = query_translation_prompt.format(question=question)\n",
    "    response = model.invoke(question_template)\n",
    "\n",
    "    return {'new_questions': response.content.split('\\n')}\n",
    "\n",
    "def tavily_search_docs(state: ResearchStateInput):\n",
    "    \"\"\"To search the web for the information based on the translated queries/questions and get the url details.\"\"\"\n",
    "    \n",
    "    result_doc = []\n",
    "    new_questions = state['new_questions']\n",
    "    web_search_prompt = \"\"\"You are expert at retrieving information from the web. \n",
    "                        You are asked to find the following information: {question}\"\"\"\n",
    "\n",
    "    tavily = TavilySearchResults(max_results=1)\n",
    "    for question in new_questions:\n",
    "        prompt = web_search_prompt.format(question=question)\n",
    "        tavily_search = tavily.invoke({'query': prompt})\n",
    "        result_doc.extend([Document(page_content=tavily['content'], metadata={\"source\": tavily['url'], \"title\": question}) \n",
    "                            for tavily in (tavily_search)])\n",
    "        \n",
    "    return {'content': result_doc}\n",
    "\n",
    "def retrieve(state: ResearchStateInput)->ResearchState:\n",
    "    content = state['content']\n",
    "    question = state['question']\n",
    "\n",
    "    flattened_docs = [item for sublist in content for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    vector = vector_db.add_documents(flattened_docs)\n",
    "\n",
    "    return {'flattened_docs': flattened_docs, 'question': question}\n",
    "\n",
    "\n",
    "def summarization(state: ResearchState):\n",
    "    \"\"\"To summarize the retrieved information from the web.\"\"\"\n",
    "    flattened_docs = state['flattened_docs']\n",
    "    question = state['question']\n",
    "\n",
    "    summarization_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    summarization_prompt = \"\"\"You are expert at summarizing the retrieved information from the different sources. \n",
    "                            You are asked to summarize the following information and retain the citations: {documents}\"\"\"\n",
    "\n",
    "    summarization_prompt = summarization_prompt.format(documents=flattened_docs)\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(summarization_prompt)\n",
    "    except Exception as _:\n",
    "        response = summarization_llm.invoke(summarization_prompt)\n",
    "\n",
    "    citations = [doc.metadata.get('source', 'Arxiv') for doc in flattened_docs]\n",
    "    return {'summary': response.content, 'citations': list(set(citations))}\n",
    "\n",
    "def fact_check_agent(state: ResearchState):\n",
    "    \"\"\"To fact-check the summarized information.\"\"\"\n",
    "    summary = state['summary']\n",
    "\n",
    "    fact_check_prompt = \"\"\"You are expert at fact-checking the summarized information. \n",
    "                        You are asked to fact-check the following information: {summary}\"\"\"\n",
    "\n",
    "    fact_check_prompt = fact_check_prompt.format(summary=summary)\n",
    "    response = model.invoke(fact_check_prompt)\n",
    "\n",
    "    return {'fact_check': response.content}\n",
    "\n",
    "def error_detection_agent(state: ResearchState):\n",
    "    \"\"\"To detect the errors in the fact-checked information.\"\"\"\n",
    "    fact_check = state['fact_check']\n",
    "    errors = []\n",
    "\n",
    "    if \"conflicting\" in fact_check.lower():\n",
    "        errors.append(\"Conflict identified in the fact-checked information.\")\n",
    "\n",
    "    return {'errors': errors}\n",
    "\n",
    "def error_checker(state: ResearchState):\n",
    "    errors = state['errors']\n",
    "    \n",
    "    if errors:\n",
    "        return ['get_user_input']\n",
    "\n",
    "    return END\n",
    "    \n",
    "\n",
    "query_modifier({'errors': ['present'], 'question': 'Attention is all you need', 'summary': out['summary']})\n",
    "\n",
    "# tavily_search_docs({'new_questions': ['What is Attention is all you need?']})\n",
    "# wk = wikiloader({'question': 'Attention mechanism'})\n",
    "# ar = arxiv_loader({'question': 'Attention mechanism'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "builder = StateGraph(ResearchStateInput, output=ResearchState)\n",
    "builder.add_node('get_user_input', get_user_input)\n",
    "builder.add_node('wikiloader', wikiloader)\n",
    "builder.add_node('arxiv_loader', arxiv_loader)\n",
    "builder.add_node('query_translation', query_translation)\n",
    "builder.add_node('tavily_search_docs', tavily_search_docs)\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('summarization', summarization)\n",
    "builder.add_node('fact_check_agent', fact_check_agent)\n",
    "builder.add_node('error_detection_agent', error_detection_agent)\n",
    "\n",
    "builder.add_edge(START, 'get_user_input')\n",
    "builder.add_edge('get_user_input', 'wikiloader')\n",
    "builder.add_edge('get_user_input', 'arxiv_loader')\n",
    "builder.add_edge('get_user_input', 'query_translation')\n",
    "builder.add_edge('query_translation', 'tavily_search_docs')\n",
    "\n",
    "builder.add_edge('arxiv_loader', 'retrieve')\n",
    "builder.add_edge('wikiloader', 'retrieve')\n",
    "builder.add_edge('tavily_search_docs', 'retrieve')\n",
    "builder.add_edge('retrieve', 'summarization')\n",
    "builder.add_edge('summarization', 'fact_check_agent')\n",
    "builder.add_edge('fact_check_agent', 'error_detection_agent')\n",
    "\n",
    "builder.add_conditional_edges('error_detection_agent', error_checker, ['get_user_input', END])\n",
    "\n",
    "graph = builder.compile()\n",
    "Image(graph.get_graph(xray=1).draw_mermaid_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = graph.invoke({'question':'Attention is all you need'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Attention is all you need',\n",
       " 'flattened_docs': [Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Dont Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i {1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Dont Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {(Attentioni, ) | i {L k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n\\n{1, 2, . . . , L k}} \\n{(, MLPi) | i {L k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {() | i {L k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layers features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Dont Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Dont Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Dont Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and Re, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:87478759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 2213722176. PMLR, 2329 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., ancucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Dont Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:1745617472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 72877301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n1401114023, 2020.\\n6\\n'),\n",
       "  Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'),\n",
       "  Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster  with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\nCorresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe models scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as ) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n The Number of Groups N. In RITA, the number of groups\\nis a crucial factor that balances the speed up and the quality of\\nattention approximation. A small will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate is essential to the performance of group attention.\\nHowever, depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of . In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate \\nalmost impossible.\\n Batch Size. Moreover, as we want to dynamically adjust \\nduring training, a fixed batch size is sub-optimal: as decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate for each group attention layer during the\\ntraining process. It starts with a large and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups and the batch size .\\nThis model is used to predict for a given when training RITA.\\nSpecifically, we first sample some values in a reasonable range.\\nFor each sampled , we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length 2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes hidden\\nembedding vectors Ras input, then projects them to\\nqueries (), keys () and values () and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state , is computed by:\\n= , = ,= \\n= = ( \\n\\n\\n)\\n(1)\\nWhere R,R,Rare projection\\nmatrices for generating , ,. Ris also regarded as the\\npacking of query vectors {1, ...,} with dimension into a\\nmatrix. R,Rare regarded as the packing of key\\nvectors {1, ...,} and value vectors {1, ..., } in the same way.\\nGiven a matrix R, the softmax function normalizes \\nto ensure the sum of each row equals to 1, as shown below.\\n(,) =\\n(,)\\n1\\n=0 (,)\\n(2)\\nNote the attention matrix A is an matrix, where represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length \\nand withvariables as an n  m matrix, RITA usesconvolution\\nkernels to chunkinto n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w  m matrix, where defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of \\nsemantic units 1,2, ...,() as input (e.g. embeddings of\\nwindows for a timeseries), then models the correlations between\\nthe semantic units and outputs 1, ...,() as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a (2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the cloze text pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate . The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, , the attention score of window onto\\nwindow , is determined by the inner product between the query\\nvector of window and the key vector of window , that is,  .\\nGiven another window , if window has the similar key vector\\nto window , that is, , then   . In other words,\\nwhen .\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window , we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping windows into groups, group atten-\\ntion compresses the attention matrix from anmatrix to an\\nmatrix. Because (number of groups) tends to be much smaller\\nthan (number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesnt hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix , canonical self-attention computes the output\\nembedding as O = AV. Because is an  matrix and is an\\nmatrix, the matrix product operation still produces an \\nmatrix . That is, it produces a dimensional feature vector for\\neach window. However, our group attention will produce an  \\nattention matrix e\\n, where corresponds to the number of groups.\\nIn this case the matrix product will produce a matrix e\\n. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix from the group attention matrix e\\n. For example,\\ngiven one group composed of and , we map its group\\nattention vector in e\\ninto two rows that correspond to and\\nin . However, in this case we again get a  attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix .\\nGiven an element,ofcorresponding to the dimension of\\ns feature vector,,= , where vector ai Rn denotes the\\nrow of the attention matrix and vector vj Rn denotes the \\ndimension of all the feature vectors. Given ai =< a1\\ni , a2\\ni ,    , an\\ni >\\nand vj =< v1\\nj , v2\\nj ,    , vn\\nj >, ,= n\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 1 and 2 belong to the same group\\n1. Then 1\\n= 2\\n= e1\\n, where e1\\ne\\ncorresponds to the attention\\nof group 1 onto . Therefore, 1\\n1\\n+ 2\\n2\\n= e1\\n(1\\n+ 2\\n).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector into a -dimensional group fea-\\nture vectorebeforehand, we could directly use the group attention\\nvector eand the group feature vector eto compute ,.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\nthat is identical to the embedding produced\\nby using the full attention matrix and the embedding matrix .\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix is computed as = SoftMax( QKT\\n\\ndk ). To compute ,\\nwe have to first compute (denoted as ) which is an  \\nmatrix. Then normalizing the matrix with softmax produces the\\nattention matrix .\\nGroup attention follows the same procedure. But after grouping\\nkeys into e, eproduces an  matrix e. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e\\nwill result in a group attention matrix e\\nfrom which we are not able\\nto recover a full attention matrix that is identical to first restoring\\neto and then applying softmax on . The matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall  ematrix is not memory efficient, as it will end up with\\na full  matrix .\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n(g\\n,) =\\n(,)\\n1\\n=0 (,)\\n(3)\\nIn Eq. 3, represents the number of windows that Group\\ncontains. Compared to the original softmax, our group softmax\\nconsiders each group as elements and counts it \\ntimes when summing up the exponential of each groups ,. In\\nthis way, the group softmax function operating on the small e\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is () and\\nthe space complexity is(), while the time and space complexity\\nof the original self-attention mechanism are (2) and (2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\nwhich approxi-\\nmates the attention matrixproduced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let be the radius of the ball where all key vectors\\nlive; ebe the representative of the group that contains key . Let \\ndenote the full attention matrix restored from e\\n. Suppose the distance\\nbetween eand (||ekk||) satisfies: ||ekk|| d.\\nThen > 1, if d ln()\\n2R , 1\\nAi,j\\nAi,j \\nLemma 1 shows that the error bound of the group attention is\\ndetermined by the distance . As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups  the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (()).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given centers, in each iteration the time and space\\ncomplexity of K-means is (). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi cj| =\\n\\n(vi cj)2, i [1, n], j [1, N]. The performance bot-\\ntleneck is . We instead use a different formulation: |\\n| = |vi cj| =\\n\\n|vi|2 + |cj|2 2vi  cj, i [1, n], j [1, N]. This is\\nbecause in this formulation, the performance bottleneck is  ,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\nand accordingly the batch size , as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts and based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines . Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate .\\nThe adaptive scheduler of RITA starts with a large and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase .\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard  as difficult as setting an appropriate .\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound , and then uses Lemma 1 to translate to a distance\\nthreshold . RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold .\\nLemma 2. Denote to be the cluster center of . Assume\\nthe existing grouping satisfies k,\\nmax\\nxclusterk\\n|ck x| d , thus satis-\\nfying an error bound by Lemma 1. If there exist clusters, namely,\\n1,2, ...,, satisfying that:\\n\\n\\n|| + || ,, [1,]\\n(4)\\nmerging them into one cluster still meets the error bound .\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if and satisfy:\\n\\n\\n||+|| , and\\n\\n\\n||+|| \\nthere is an undirected edge between and ;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 1,2;\\n(2) If 1 and 2 satisfy:\\n\\n\\n|| + || ,\\n\\n\\n|| + || \\n2\\n(5)\\nis marked.\\n(3) Decrease the number of clusters by counting the masks in 2.\\nIn this solution, clusters in 1 can be regarded as transfer nodes.\\nIf (5) holds for (1,1 2) and (\\n1,2 2), respectively, we have,\\n\\n1\\n|1 2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 | + |2 | \\n(6)\\nThus (4) holds when merging several clusters in 2 with one\\ncluster in 1. As a result, we can greedily merge clusters in 2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by after merging,\\nwe apply a momentum update [42] on the number of clusters , as\\nis commonly used in machine learning to smooth the changing of\\nand avoid sample selection bias. To be specific: = (\\n) + (1 ), where is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batchs GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups , RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries and the average\\ngroup number among all attention module . So RITA samples\\nseveral (, ) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n, we randomly sample integral points (, ) from plane\\n{1 , 1 }. Then we use a binary search based\\nalgorithm to find the maximal batch size that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables and .\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n= (, ) on plane {1 , 1 }.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function , we can estimate a proper\\nbatch size for any (, ) during training, even if it is not seen in\\nthe sampled (, ) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the models robustness.\\n RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In pretraining + few-label finetun-\\ning scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITAs time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate = 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of = 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITAs advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attentions\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITAs advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attentions scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITAs adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number . We\\nvary and the error bound threshold used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed . Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing . More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed . On the ECG dataset,\\nalthough fixed is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when varies, while the results of\\nfixed vary significantly when the value of changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the threshold,\\nwhile it is hard to find an appropriate for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the models ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the cloze test pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a words attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 18771901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T OConnor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241260.\\n[15] Angus Dempster, Franois Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 14541495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n19361962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117128.\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263286.\\n[26] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 11921209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series dataa survey. Pattern recognition\\n38, 11 (2005), 18571874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 13681373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 17621777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 13101318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 11811191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, Franois Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjrgaard, Anind Dey, Tobias Sonne, and Mads Mller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 28282837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 19.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 59986008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD 21). Association for Computing Machinery, New York,\\nNY, USA, 23282337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 2241922430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 55195522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 1728317297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD 21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 21142124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 14. In full-label training scenario, we train the models for\\n100 epochs. In pretraining + few-label finetuning scenario, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPUs capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: ,, ,, \\nEnsure: ,R,R,N,N\\n1: function group_attention(,, )\\n2:\\nfor = 0 1 do\\n3:\\ne1\\n=0 (== )\\n4:\\ne\\n5:\\nfor = 0 1 do\\n6:\\nfor = 0 1 do\\n7:\\n,(e,)\\n8:\\nfor = 0 1 do\\n9:\\n1\\n=0 ,\\n10:\\nfor = 0 1 do\\n11:\\n1\\n=0\\n( e,)\\n\\ne\\n12:\\nreturn \\nIn Alg. 1, we denoteto be the size of the group, to\\nbe the number of groups, rto be the representative key of the \\ngroup and R to be the matrix consisting of all r, to be\\nthe group that kbelongs to. ,are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix for new feature emebddings {1, ...,}, where \\ncorresponds to the feature embedding of . Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: , \\nEnsure: 1 , 1 \\n1: function binary_search(, )\\n2:\\n1\\n3:\\n\\n4:\\n\\n5:\\n\\n6:\\nwhile do\\n7:\\n \\n8:\\n()\\n9:\\n\\n10:\\n\\n\\n11:\\nif 0.9 > then\\n12:\\n+ 1\\n13:\\n\\n14:\\nelse\\n15:\\n1\\n16:\\n+\\n2\\n17:\\nreturn \\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: , , , \\nEnsure: 1 , 1 \\n1: function cost(S)\\n2:\\nif || < then return +\\n3:\\n, , \\n4:\\n(|, )\\nreturn (, , |)\\n5: function dynamic_programming(, , )\\n6:\\nfor 1 = 1 do\\n7:\\nfor 2 = 1 1 do\\n8:\\nfor = 1 1 do\\n9:\\n{2 1, }\\n10:\\n() ()\\n11:\\nfor = 1 do\\n12:\\n{2 1,}\\n13:\\n() ((),() + ())\\n14:\\n2,1 (1)\\n15:\\n16:\\nfor = 1 do\\n17:\\n() (1,)\\n18:\\nfor = 1 do\\n19:\\n() ((),() + (,))\\nreturn ()\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset . When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). () denotes the minimal\\nestimation error for points in sub-plane {2 1, }. In\\nLines 11-13, we enumerate all possible ways of cutting {2 \\n1, } horizontally into two sub-plane {2 1, } and\\n{2 1,} by iterating from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a(1) with\\nminimal estimation error for sub-plane {2 1, 1}, which\\nis recorded as 1,2 in Line 14. () denotes the minimal estimation\\nerror for sub-plane {}. We enumerate all the possible ways\\nof cutting {} vertically into two sub-plane {} and {\\n} by iterating from 1 to (Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as (). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group \\nhave the same key vector, i.e. = (), then the feature\\nembedding produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\nto be the representative vectors of , i.e. e\\n=\\n= (). Algorithm 1 gives that\\ne=\\n1\\n\\n=0\\n(== )v, e,= q r\\n=\\n1\\n\\n=0\\n(e,), e=\\n1\\n\\n=0\\ne,\\n\\ne\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n,= q kj, ,=\\n(,)\\n1\\n=0 (,)\\n, o=\\n1\\n\\n=0\\n,v\\n(8)\\nWith 7 and 8, we have\\n1\\n\\n=0\\n(,) =\\n1\\n\\n=0\\n(q k)\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )(q k)\\n=\\n1\\n\\n=0\\n(q r)\\n1\\n\\n=0\\n(== )\\n=\\n1\\n\\n=0\\n(q r)\\n=\\n1\\n\\n=0\\n(e,)\\n= \\n(9)\\n12\\nFurther,\\no=\\n1\\n\\n=0\\n,vj\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== ),v\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(,)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q k)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q rj)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\n1\\n\\n=0\\n(== )v\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\ne\\n(10)\\nCombining (7), (9) (10), we have oi = N 1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attentions.\\n\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n(,)\\n(,) = (q ek)\\n(q k) = (q (ekk))\\n= (||q||  ||ekk||  (q,ekk))\\n(11)\\nSo\\n() (,)\\n(,) ()\\n(12)\\nThen we have:\\n,\\n,\\n=\\n(,)\\n\\n=1 (,)\\n/\\n(,)\\n\\n=1 (,)\\n= (,)\\n(,)\\n\\n=1 (,)\\n\\n=1 (,)\\n(13)\\nCombining (12) (13), the error is bounded by\\n(2) ,\\n,\\n(2)\\n(14)\\nThus, if d ln()\\n2R , 1\\nAi,j\\nAi,j . This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of to be .After merge-\\ning, the new center will be:\\n =\\n\\n=1 \\n\\n=1 \\nFor [1,], , it holds that:\\n|| || + || ()\\n= || + |\\n\\n=1 \\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n|\\n= || + |\\n\\n=1 ()\\n\\n=1 \\n|\\n= || +\\n| \\n=1 () |\\n\\n=1 \\n|| +\\n\\n=1 ||\\n\\n=1 \\n=\\n\\n=1 (|| + ||)\\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n= \\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where [] Ris\\nthe output representation of [CLS], C is the number of classes, and\\nWcls RCd, Bcls RC are learnable parameters for classification\\ntask. The result vector Rrepresents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nC\\ni=1 y(i)log(y(i)), where is a binary\\nindicator for ground truth label:\\n() =\\n(\\n1\\nis ground truth label\\n0\\n\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries asR, the observed timeseries\\nwith missing values as R, and the set of missing values\\npositions as . We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n(, ) =\\n(\\n1\\n(, ) \\n(, )\\n(, ) \\n(17)\\nis fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +Z2 +... +Zn), where\\nRis the recovered timeseries, and Ris the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n=\\n1\\n||\\n\\n(,)((, ) (, ))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n(, ) =\\n(\\n(, )\\n\\n1\\n\\n(18)\\nWhere is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n'),\n",
       "  Document(metadata={'source': 'Wikipedia', 'title': 'Attention is all you need'}, page_content='Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 140,000 times.\\n\\nPage: All You Need Is Kill\\nSummary: All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Rysuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\nPage: All You Need Is Love\\nSummary: \"All You Need Is Love\" is a song by the English rock band the Beatles that was released as a non-album single in July 1967, with \"Baby, You\\'re a Rich Man\" as its B-side. It was written by John Lennon and credited to the LennonMcCartney partnership. The song was Britain\\'s contribution to Our World, the first live global television link, for which the band were shown performing it at EMI Studios in London on 25 June. The programme was broadcast via satellite and seen by an audience of over 400 million in 25 countries. Lennon\\'s lyrics were deliberately simplistic, to allow for broad appeal to the show\\'s international audience, and captured the utopian ideals associated with the Summer of Love. The single topped sales charts in Britain, the United States and many other countries, and became an anthem for the counterculture\\'s embrace of flower power philosophy.\\nOur World coincide'),\n",
       "  Document(metadata={'source': 'https://myscale.com/blog/key-concepts-transformer-architecture-attention-need/', 'title': '1. What is the significance of \"Attention is all you need\" in the context of machine learning and artificial intelligence?'}, page_content='MYSCALE Product Docs Pricing Resources Contact By leveraging attention mechanisms, transformers revolutionized natural language processing (NLP) (opens new window) by enabling machines to focus on relevant information effectively. The self-attention mechanism (opens new window) within transformers allows them to analyze relationships between words in a sentence comprehensively. In the realm of artificial intelligence, the transformer architecture introduces a compelling concept known as multi-head attention (opens new window), emphasizing the power of diverse perspectives in enhancing machine learning capabilities. By incorporating multiple attention heads, transformers can capture varied aspects of input sequences (opens new window), allowing for nuanced interpretations (opens new window) and enriched representations. Multi-head attention mechanisms play a pivotal role in language translation tasks by enabling transformers to capture diverse linguistic patterns and semantic nuances present in source and target languages.'),\n",
       "  Document(metadata={'source': 'https://medium.com/@adachoudhry26/deep-dive-into-ai-analyzing-attention-is-all-you-need-a37a2a3758d4', 'title': '2. How has the \"Attention is all you need\" approach revolutionized natural language processing and neural network architectures?'}, page_content='These layers use multi-head attention, positional encodings, residual connections, and layer normalization. Cross-attention is when the queries are produced from the input dataset while the values and keys are generated from another dataset or layer in the transformers, to train the model on additional context. The multi-head attention computes the context between the input token by producing key, query, and value vectors. In a self-attention layer all of the keys, values, and queries come from the same place, in this case, the output of the previous layer in the encoder (which would be just the positional encodings if it is the first encoder layer). After the self-attention layer, the input is passed to a fully connected feed-forward neural network for more linear transformations.')],\n",
       " 'summary': 'Here\\'s a summarized version of the provided information regarding the various documents related to attention mechanisms in machine learning, specifically in the context of transformer architectures:\\n\\n1. **\"Attention Is All You Need But You Don\\'t Need All Of It For Inference of Large Language Models\" (Tyukin et al., 2024)**: This paper addresses the increasing demand for inference in large language models (LLMs) while highlighting the challenges posed by the quadratic complexity of attention mechanisms in transformer architectures. The authors investigate how dropping specific attention and MLP layers during inference can improve efficiency with minimal impact on performance. For instance, removing 33% of attention layers in a Llama-v2 model results in only a 1.8% decrease in average performance but significantly enhances speed. They conclude that greater efficiency can be achieved without sacrificing quality (Tyukin et al., 2024).\\n\\n2. **\"All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval\" (Song et al., 2021)**: This work introduces a global-local attention module (GLAM) designed for instance-level image retrieval. By integrating all forms of attentionglobal and local, spatial and channelGLAM enhances the representation of images and improves retrieval effectiveness. The authors show that using their method can significantly improve performance on standard benchmarks compared to prior approaches, thus providing a comprehensive treatment of various attention forms in visual tasks (Song et al., 2021).\\n\\n3. **\"RITA: Group Attention is All You Need for Timeseries Analytics\" (Liang et al., 2023)**: RITA leverages a novel attention mechanism called group attention to address the scalability issues of traditional transformers on long timeseries data. The method efficiently clusters similar segments, allowing for reduced time and space complexities while maintaining high-quality feature embeddings necessary for various timeseries analytics tasks. RITA proves to outperform existing models in both accuracy and processing speed, achieving speedups of up to 63 times on challenging datasets (Liang et al., 2023).\\n\\n4. **\"Attention Is All You Need\" (Vaswani et al., 2017)**: This seminal paper introduced the transformer architecture, emphasizing the self-attention mechanism to process sequences of data, such as language. This work laid the foundation for many subsequent advancements in natural language processing and artificial intelligence, showing significant improvements in tasks like translation and context understanding across multiple applications. The paper\\'s impact is highlighted by its wide adoption in creating various large language models (Vaswani et al., 2017).\\n\\nThese documents collectively underscore the transformative role of attention mechanisms in enhancing model performance across a variety of domains including language processing, image retrieval, and timeseries analytics, setting a precedent for future research and applications in artificial intelligence.',\n",
       " 'citations': ['Arxiv',\n",
       "  'Arxiv',\n",
       "  'Arxiv',\n",
       "  'Wikipedia',\n",
       "  'https://myscale.com/blog/key-concepts-transformer-architecture-attention-need/',\n",
       "  'https://medium.com/@adachoudhry26/deep-dive-into-ai-analyzing-attention-is-all-you-need-a37a2a3758d4'],\n",
       " 'fact_check': 'Based on the provided summarized information, the following fact-checking points can be made:\\n\\n1. \"Attention Is All You Need But You Don\\'t Need All Of It For Inference of Large Language Models\" (Tyukin et al., 2024):\\n- The paper addresses the challenges of the quadratic complexity of attention mechanisms in transformer architectures in the context of large language models (LLMs).\\n- The authors propose dropping specific attention and MLP layers during inference to improve efficiency without sacrificing much performance.\\n- Removing 33% of attention layers in a specific model leads to a 1.8% decrease in performance but enhances speed significantly.\\n- The claim that greater efficiency can be achieved without sacrificing quality is supported by the results presented in the paper.\\n\\n2. \"All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval\" (Song et al., 2021):\\n- The paper introduces a global-local attention module (GLAM) that integrates various forms of attention for instance-level image retrieval.\\n- The authors demonstrate that using GLAM improves performance on standard benchmarks compared to prior approaches.\\n- The claim that the method significantly improves retrieval effectiveness is supported by the results presented in the paper.\\n\\n3. \"RITA: Group Attention is All You Need for Timeseries Analytics\" (Liang et al., 2023):\\n- The paper introduces RITA, a method that leverages group attention to improve scalability on long timeseries data.\\n- The authors claim that RITA outperforms existing models in accuracy and processing speed, achieving significant speedups on challenging datasets.\\n- The claim that RITA achieves speedups of up to 63 times on challenging datasets is supported by the results presented in the paper.\\n\\n4. \"Attention Is All You Need\" (Vaswani et al., 2017):\\n- This seminal paper introduced the transformer architecture and highlighted the importance of the self-attention mechanism for processing sequences of data like language.\\n- The paper\\'s impact on natural language processing and artificial intelligence is acknowledged, and its wide adoption for creating large language models is noted.\\n\\nOverall, the summarized information accurately represents the key points and findings from the provided documents related to attention mechanisms in machine learning, specifically in the context of transformer architectures.',\n",
       " 'errors': []}"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summarized version of the provided information regarding the various documents related to attention mechanisms in machine learning, specifically in the context of transformer architectures:\n",
      "\n",
      "1. **\"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\" (Tyukin et al., 2024)**: This paper addresses the increasing demand for inference in large language models (LLMs) while highlighting the challenges posed by the quadratic complexity of attention mechanisms in transformer architectures. The authors investigate how dropping specific attention and MLP layers during inference can improve efficiency with minimal impact on performance. For instance, removing 33% of attention layers in a Llama-v2 model results in only a 1.8% decrease in average performance but significantly enhances speed. They conclude that greater efficiency can be achieved without sacrificing quality (Tyukin et al., 2024).\n",
      "\n",
      "2. **\"All the Attention You Need: Global-Local, Spatial-Channel Attention for Image Retrieval\" (Song et al., 2021)**: This work introduces a global-local attention module (GLAM) designed for instance-level image retrieval. By integrating all forms of attentionglobal and local, spatial and channelGLAM enhances the representation of images and improves retrieval effectiveness. The authors show that using their method can significantly improve performance on standard benchmarks compared to prior approaches, thus providing a comprehensive treatment of various attention forms in visual tasks (Song et al., 2021).\n",
      "\n",
      "3. **\"RITA: Group Attention is All You Need for Timeseries Analytics\" (Liang et al., 2023)**: RITA leverages a novel attention mechanism called group attention to address the scalability issues of traditional transformers on long timeseries data. The method efficiently clusters similar segments, allowing for reduced time and space complexities while maintaining high-quality feature embeddings necessary for various timeseries analytics tasks. RITA proves to outperform existing models in both accuracy and processing speed, achieving speedups of up to 63 times on challenging datasets (Liang et al., 2023).\n",
      "\n",
      "4. **\"Attention Is All You Need\" (Vaswani et al., 2017)**: This seminal paper introduced the transformer architecture, emphasizing the self-attention mechanism to process sequences of data, such as language. This work laid the foundation for many subsequent advancements in natural language processing and artificial intelligence, showing significant improvements in tasks like translation and context understanding across multiple applications. The paper's impact is highlighted by its wide adoption in creating various large language models (Vaswani et al., 2017).\n",
      "\n",
      "These documents collectively underscore the transformative role of attention mechanisms in enhancing model performance across a variety of domains including language processing, image retrieval, and timeseries analytics, setting a precedent for future research and applications in artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "print(out['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Dont Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i {1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Dont Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {(Attentioni, ) | i {L k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n\\n{1, 2, . . . , L k}} \\n{(, MLPi) | i {L k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {() | i {L k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layers features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Dont Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Dont Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Dont Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and Re, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:87478759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 2213722176. PMLR, 2329 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., ancucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Dont Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:1745617472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 72877301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n1401114023, 2020.\\n6\\n'),\n",
       " Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'),\n",
       " Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster  with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\nCorresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe models scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as ) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n The Number of Groups N. In RITA, the number of groups\\nis a crucial factor that balances the speed up and the quality of\\nattention approximation. A small will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate is essential to the performance of group attention.\\nHowever, depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of . In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate \\nalmost impossible.\\n Batch Size. Moreover, as we want to dynamically adjust \\nduring training, a fixed batch size is sub-optimal: as decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate for each group attention layer during the\\ntraining process. It starts with a large and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups and the batch size .\\nThis model is used to predict for a given when training RITA.\\nSpecifically, we first sample some values in a reasonable range.\\nFor each sampled , we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length 2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes hidden\\nembedding vectors Ras input, then projects them to\\nqueries (), keys () and values () and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state , is computed by:\\n= , = ,= \\n= = ( \\n\\n\\n)\\n(1)\\nWhere R,R,Rare projection\\nmatrices for generating , ,. Ris also regarded as the\\npacking of query vectors {1, ...,} with dimension into a\\nmatrix. R,Rare regarded as the packing of key\\nvectors {1, ...,} and value vectors {1, ..., } in the same way.\\nGiven a matrix R, the softmax function normalizes \\nto ensure the sum of each row equals to 1, as shown below.\\n(,) =\\n(,)\\n1\\n=0 (,)\\n(2)\\nNote the attention matrix A is an matrix, where represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length \\nand withvariables as an n  m matrix, RITA usesconvolution\\nkernels to chunkinto n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w  m matrix, where defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of \\nsemantic units 1,2, ...,() as input (e.g. embeddings of\\nwindows for a timeseries), then models the correlations between\\nthe semantic units and outputs 1, ...,() as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a (2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the cloze text pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate . The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, , the attention score of window onto\\nwindow , is determined by the inner product between the query\\nvector of window and the key vector of window , that is,  .\\nGiven another window , if window has the similar key vector\\nto window , that is, , then   . In other words,\\nwhen .\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window , we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping windows into groups, group atten-\\ntion compresses the attention matrix from anmatrix to an\\nmatrix. Because (number of groups) tends to be much smaller\\nthan (number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesnt hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix , canonical self-attention computes the output\\nembedding as O = AV. Because is an  matrix and is an\\nmatrix, the matrix product operation still produces an \\nmatrix . That is, it produces a dimensional feature vector for\\neach window. However, our group attention will produce an  \\nattention matrix e\\n, where corresponds to the number of groups.\\nIn this case the matrix product will produce a matrix e\\n. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix from the group attention matrix e\\n. For example,\\ngiven one group composed of and , we map its group\\nattention vector in e\\ninto two rows that correspond to and\\nin . However, in this case we again get a  attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix .\\nGiven an element,ofcorresponding to the dimension of\\ns feature vector,,= , where vector ai Rn denotes the\\nrow of the attention matrix and vector vj Rn denotes the \\ndimension of all the feature vectors. Given ai =< a1\\ni , a2\\ni ,    , an\\ni >\\nand vj =< v1\\nj , v2\\nj ,    , vn\\nj >, ,= n\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 1 and 2 belong to the same group\\n1. Then 1\\n= 2\\n= e1\\n, where e1\\ne\\ncorresponds to the attention\\nof group 1 onto . Therefore, 1\\n1\\n+ 2\\n2\\n= e1\\n(1\\n+ 2\\n).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector into a -dimensional group fea-\\nture vectorebeforehand, we could directly use the group attention\\nvector eand the group feature vector eto compute ,.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\nthat is identical to the embedding produced\\nby using the full attention matrix and the embedding matrix .\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix is computed as = SoftMax( QKT\\n\\ndk ). To compute ,\\nwe have to first compute (denoted as ) which is an  \\nmatrix. Then normalizing the matrix with softmax produces the\\nattention matrix .\\nGroup attention follows the same procedure. But after grouping\\nkeys into e, eproduces an  matrix e. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e\\nwill result in a group attention matrix e\\nfrom which we are not able\\nto recover a full attention matrix that is identical to first restoring\\neto and then applying softmax on . The matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall  ematrix is not memory efficient, as it will end up with\\na full  matrix .\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n(g\\n,) =\\n(,)\\n1\\n=0 (,)\\n(3)\\nIn Eq. 3, represents the number of windows that Group\\ncontains. Compared to the original softmax, our group softmax\\nconsiders each group as elements and counts it \\ntimes when summing up the exponential of each groups ,. In\\nthis way, the group softmax function operating on the small e\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is () and\\nthe space complexity is(), while the time and space complexity\\nof the original self-attention mechanism are (2) and (2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\nwhich approxi-\\nmates the attention matrixproduced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let be the radius of the ball where all key vectors\\nlive; ebe the representative of the group that contains key . Let \\ndenote the full attention matrix restored from e\\n. Suppose the distance\\nbetween eand (||ekk||) satisfies: ||ekk|| d.\\nThen > 1, if d ln()\\n2R , 1\\nAi,j\\nAi,j \\nLemma 1 shows that the error bound of the group attention is\\ndetermined by the distance . As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups  the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (()).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given centers, in each iteration the time and space\\ncomplexity of K-means is (). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi cj| =\\n\\n(vi cj)2, i [1, n], j [1, N]. The performance bot-\\ntleneck is . We instead use a different formulation: |\\n| = |vi cj| =\\n\\n|vi|2 + |cj|2 2vi  cj, i [1, n], j [1, N]. This is\\nbecause in this formulation, the performance bottleneck is  ,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\nand accordingly the batch size , as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts and based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines . Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate .\\nThe adaptive scheduler of RITA starts with a large and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase .\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard  as difficult as setting an appropriate .\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound , and then uses Lemma 1 to translate to a distance\\nthreshold . RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold .\\nLemma 2. Denote to be the cluster center of . Assume\\nthe existing grouping satisfies k,\\nmax\\nxclusterk\\n|ck x| d , thus satis-\\nfying an error bound by Lemma 1. If there exist clusters, namely,\\n1,2, ...,, satisfying that:\\n\\n\\n|| + || ,, [1,]\\n(4)\\nmerging them into one cluster still meets the error bound .\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if and satisfy:\\n\\n\\n||+|| , and\\n\\n\\n||+|| \\nthere is an undirected edge between and ;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 1,2;\\n(2) If 1 and 2 satisfy:\\n\\n\\n|| + || ,\\n\\n\\n|| + || \\n2\\n(5)\\nis marked.\\n(3) Decrease the number of clusters by counting the masks in 2.\\nIn this solution, clusters in 1 can be regarded as transfer nodes.\\nIf (5) holds for (1,1 2) and (\\n1,2 2), respectively, we have,\\n\\n1\\n|1 2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 | + |2 | \\n(6)\\nThus (4) holds when merging several clusters in 2 with one\\ncluster in 1. As a result, we can greedily merge clusters in 2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by after merging,\\nwe apply a momentum update [42] on the number of clusters , as\\nis commonly used in machine learning to smooth the changing of\\nand avoid sample selection bias. To be specific: = (\\n) + (1 ), where is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batchs GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups , RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries and the average\\ngroup number among all attention module . So RITA samples\\nseveral (, ) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n, we randomly sample integral points (, ) from plane\\n{1 , 1 }. Then we use a binary search based\\nalgorithm to find the maximal batch size that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables and .\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n= (, ) on plane {1 , 1 }.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function , we can estimate a proper\\nbatch size for any (, ) during training, even if it is not seen in\\nthe sampled (, ) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the models robustness.\\n RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In pretraining + few-label finetun-\\ning scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITAs time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate = 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of = 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITAs advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attentions\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITAs advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attentions scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITAs adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number . We\\nvary and the error bound threshold used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed . Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing . More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed . On the ECG dataset,\\nalthough fixed is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when varies, while the results of\\nfixed vary significantly when the value of changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the threshold,\\nwhile it is hard to find an appropriate for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the models ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the cloze test pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a words attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 18771901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T OConnor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241260.\\n[15] Angus Dempster, Franois Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 14541495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n19361962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117128.\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263286.\\n[26] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 11921209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series dataa survey. Pattern recognition\\n38, 11 (2005), 18571874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 13681373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 17621777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 13101318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 11811191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, Franois Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjrgaard, Anind Dey, Tobias Sonne, and Mads Mller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 28282837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 19.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 59986008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD 21). Association for Computing Machinery, New York,\\nNY, USA, 23282337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 2241922430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 55195522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 1728317297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD 21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 21142124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 14. In full-label training scenario, we train the models for\\n100 epochs. In pretraining + few-label finetuning scenario, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPUs capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: ,, ,, \\nEnsure: ,R,R,N,N\\n1: function group_attention(,, )\\n2:\\nfor = 0 1 do\\n3:\\ne1\\n=0 (== )\\n4:\\ne\\n5:\\nfor = 0 1 do\\n6:\\nfor = 0 1 do\\n7:\\n,(e,)\\n8:\\nfor = 0 1 do\\n9:\\n1\\n=0 ,\\n10:\\nfor = 0 1 do\\n11:\\n1\\n=0\\n( e,)\\n\\ne\\n12:\\nreturn \\nIn Alg. 1, we denoteto be the size of the group, to\\nbe the number of groups, rto be the representative key of the \\ngroup and R to be the matrix consisting of all r, to be\\nthe group that kbelongs to. ,are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix for new feature emebddings {1, ...,}, where \\ncorresponds to the feature embedding of . Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: , \\nEnsure: 1 , 1 \\n1: function binary_search(, )\\n2:\\n1\\n3:\\n\\n4:\\n\\n5:\\n\\n6:\\nwhile do\\n7:\\n \\n8:\\n()\\n9:\\n\\n10:\\n\\n\\n11:\\nif 0.9 > then\\n12:\\n+ 1\\n13:\\n\\n14:\\nelse\\n15:\\n1\\n16:\\n+\\n2\\n17:\\nreturn \\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: , , , \\nEnsure: 1 , 1 \\n1: function cost(S)\\n2:\\nif || < then return +\\n3:\\n, , \\n4:\\n(|, )\\nreturn (, , |)\\n5: function dynamic_programming(, , )\\n6:\\nfor 1 = 1 do\\n7:\\nfor 2 = 1 1 do\\n8:\\nfor = 1 1 do\\n9:\\n{2 1, }\\n10:\\n() ()\\n11:\\nfor = 1 do\\n12:\\n{2 1,}\\n13:\\n() ((),() + ())\\n14:\\n2,1 (1)\\n15:\\n16:\\nfor = 1 do\\n17:\\n() (1,)\\n18:\\nfor = 1 do\\n19:\\n() ((),() + (,))\\nreturn ()\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset . When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). () denotes the minimal\\nestimation error for points in sub-plane {2 1, }. In\\nLines 11-13, we enumerate all possible ways of cutting {2 \\n1, } horizontally into two sub-plane {2 1, } and\\n{2 1,} by iterating from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a(1) with\\nminimal estimation error for sub-plane {2 1, 1}, which\\nis recorded as 1,2 in Line 14. () denotes the minimal estimation\\nerror for sub-plane {}. We enumerate all the possible ways\\nof cutting {} vertically into two sub-plane {} and {\\n} by iterating from 1 to (Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as (). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group \\nhave the same key vector, i.e. = (), then the feature\\nembedding produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\nto be the representative vectors of , i.e. e\\n=\\n= (). Algorithm 1 gives that\\ne=\\n1\\n\\n=0\\n(== )v, e,= q r\\n=\\n1\\n\\n=0\\n(e,), e=\\n1\\n\\n=0\\ne,\\n\\ne\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n,= q kj, ,=\\n(,)\\n1\\n=0 (,)\\n, o=\\n1\\n\\n=0\\n,v\\n(8)\\nWith 7 and 8, we have\\n1\\n\\n=0\\n(,) =\\n1\\n\\n=0\\n(q k)\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )(q k)\\n=\\n1\\n\\n=0\\n(q r)\\n1\\n\\n=0\\n(== )\\n=\\n1\\n\\n=0\\n(q r)\\n=\\n1\\n\\n=0\\n(e,)\\n= \\n(9)\\n12\\nFurther,\\no=\\n1\\n\\n=0\\n,vj\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== ),v\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(,)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q k)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q rj)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\n1\\n\\n=0\\n(== )v\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\ne\\n(10)\\nCombining (7), (9) (10), we have oi = N 1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attentions.\\n\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n(,)\\n(,) = (q ek)\\n(q k) = (q (ekk))\\n= (||q||  ||ekk||  (q,ekk))\\n(11)\\nSo\\n() (,)\\n(,) ()\\n(12)\\nThen we have:\\n,\\n,\\n=\\n(,)\\n\\n=1 (,)\\n/\\n(,)\\n\\n=1 (,)\\n= (,)\\n(,)\\n\\n=1 (,)\\n\\n=1 (,)\\n(13)\\nCombining (12) (13), the error is bounded by\\n(2) ,\\n,\\n(2)\\n(14)\\nThus, if d ln()\\n2R , 1\\nAi,j\\nAi,j . This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of to be .After merge-\\ning, the new center will be:\\n =\\n\\n=1 \\n\\n=1 \\nFor [1,], , it holds that:\\n|| || + || ()\\n= || + |\\n\\n=1 \\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n|\\n= || + |\\n\\n=1 ()\\n\\n=1 \\n|\\n= || +\\n| \\n=1 () |\\n\\n=1 \\n|| +\\n\\n=1 ||\\n\\n=1 \\n=\\n\\n=1 (|| + ||)\\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n= \\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where [] Ris\\nthe output representation of [CLS], C is the number of classes, and\\nWcls RCd, Bcls RC are learnable parameters for classification\\ntask. The result vector Rrepresents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nC\\ni=1 y(i)log(y(i)), where is a binary\\nindicator for ground truth label:\\n() =\\n(\\n1\\nis ground truth label\\n0\\n\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries asR, the observed timeseries\\nwith missing values as R, and the set of missing values\\npositions as . We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n(, ) =\\n(\\n1\\n(, ) \\n(, )\\n(, ) \\n(17)\\nis fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +Z2 +... +Zn), where\\nRis the recovered timeseries, and Ris the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n=\\n1\\n||\\n\\n(,)((, ) (, ))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n(, ) =\\n(\\n(, )\\n\\n1\\n\\n(18)\\nWhere is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n'),\n",
       " Document(metadata={'source': 'Wikipedia', 'title': 'Attention is all you need'}, page_content='Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 140,000 times.\\n\\nPage: All You Need Is Kill\\nSummary: All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Rysuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\nPage: All You Need Is Love\\nSummary: \"All You Need Is Love\" is a song by the English rock band the Beatles that was released as a non-album single in July 1967, with \"Baby, You\\'re a Rich Man\" as its B-side. It was written by John Lennon and credited to the LennonMcCartney partnership. The song was Britain\\'s contribution to Our World, the first live global television link, for which the band were shown performing it at EMI Studios in London on 25 June. The programme was broadcast via satellite and seen by an audience of over 400 million in 25 countries. Lennon\\'s lyrics were deliberately simplistic, to allow for broad appeal to the show\\'s international audience, and captured the utopian ideals associated with the Summer of Love. The single topped sales charts in Britain, the United States and many other countries, and became an anthem for the counterculture\\'s embrace of flower power philosophy.\\nOur World coincide'),\n",
       " Document(metadata={'source': 'https://arxiv.org/abs/2501.06425', 'title': '1. What is the concept of \"Attention is all you need\" in the context of machine learning or artificial intelligence?'}, page_content='In just 3 minutes help us improve arXiv: cs arXiv:2501.06425 arXiv identifier arXiv author ID In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, significantly shrinking KV cache size at inference time. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Subjects:   Computation and Language (cs.CL); Artificial Intelligence (cs.AI); Machine Learning (cs.LG) Cite as:    arXiv:2501.06425 [cs.CL] (or arXiv:2501.06425v1 [cs.CL] for this version) https://doi.org/10.48550/arXiv.2501.06425 From: Yifan Zhang [view email] Access Paper: cs.CL cs cs.AI Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status '),\n",
       " Document(metadata={'source': 'https://iq.opengenus.org/attention-is-all-you-need-summary/', 'title': '2. Can you provide an overview of the \"Attention is all you need\" model and its significance in natural language processing?'}, page_content='The model has achieved state-of-the-art performance on many of these tasks and has become one of the most widely used models in NLP.\\nOne of the most important applications of the Transformer model is in machine translation. Applications of the Transformer Model\\nThe Transformer architecture introduced in the paper has had a significant impact on the field of NLP and has been applied to a wide range of tasks, including machine translation, language modeling, question answering, and text summarization. The feedforward neural network layer applies a non-linear transformation to the output of the self-attention layer, allowing the model to capture complex relationships between words in the sequence.\\n The authors of the paper argue that attention-based models are superior to traditional models for NLP tasks because they allow the model to selectively attend to different parts of the input sequence, thus capturing important contextual relationships between words. The authors showed that the Transformer model outperforms the previous state-of-the-art models in machine translation tasks on the WMT 2014 English-to-German and English-to-French datasets.')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['flattened_docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': [[Document(metadata={'Published': '2022-03-27', 'Title': 'A General Survey on Attention Mechanisms in Deep Learning', 'Authors': 'Gianni Brauwers, Flavius Frasincar', 'Summary': 'Attention is an important mechanism that can be employed for a variety of\\ndeep learning models across many different domains and tasks. This survey\\nprovides an overview of the most important attention mechanisms proposed in the\\nliterature. The various attention mechanisms are explained by means of a\\nframework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various\\nmeasures for evaluating attention models are reviewed, and methods to\\ncharacterize the structure of attention models based on the proposed framework\\nare discussed. Last, future work in the field of attention models is\\nconsidered.'}, page_content='1\\nA General Survey on Attention Mechanisms in\\nDeep Learning\\nGianni Brauwers and Flavius Frasincar\\nAbstractAttention is an important mechanism that can be employed for a variety of deep learning models across many different\\ndomains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The\\nvarious attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a\\ncomprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed,\\nand methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in\\nthe eld of attention models is considered.\\nIndex TermsAttention models, deep learning, introductory and survey, neural nets, supervised learning\\n!\\n1\\nINTRODUCTION\\nT\\nHE idea of mimicking human attention rst arose in the\\neld of computer vision [1], [2] in an attempt to reduce\\nthe computational complexity of image processing while\\nimproving performance by introducing a model that would\\nonly focus on specic regions of images instead of the entire\\npicture. Although, the true starting point of the attention\\nmechanisms we know today is often attributed to originate\\nin the eld of natural language processing [3]. Bahdanau et\\nal. [3] implement attention in a machine translation model to\\naddress certain issues with the structure of recurrent neural\\nnetworks. After Bahdanau et al. [3] emphasized the advan-\\ntages of attention, the attention techniques were rened [4]\\nand quickly became popular for a variety of tasks, such as\\ntext classication [5], [6], image captioning [7], [8], sentiment\\nanalysis [6], [9], and speech recognition [10], [11], [12].\\nAttention has become a popular technique in deep learn-\\ning for several reasons. Firstly, models that incorporate\\nattention mechanisms attain state-of-the-art results for all\\nof the previously mentioned tasks, and many others. Fur-\\nthermore, most attention mechanisms can be trained jointly\\nwith a base model, such as a recurrent neural network or\\na convolutional neural network using regular backpropa-\\ngation [3]. Additionally, attention introduces a certain type\\nof interpretation into neural network models [8] that are\\ngenerally known to be highly complicated to interpret.\\nMoreover, the popularity of attention mechanisms was ad-\\nditionally boosted after the introduction of the Transformer\\nmodel [13] that further proved how effective attention can\\nbe. Attention was originally introduced as an extension to\\nrecurrent neural networks [14]. However, the Transformer\\nmodel proposed in [13] poses a major development in at-\\ntention research as it demonstrates that the attention mech-\\nanism is sufcient to build a state-of-the-art model. This\\nmeans that disadvantages, such as the fact that recurrent\\nneural networks are particularly difcult to parallelize, can\\nG. Brauwers and F. Frasincar are with the Erasmus School of Economics,\\nErasmus University Rotterdam, 3000 DR, Rotterdam, the Netherlands (e-\\nmail: {frasincar, brauwers}@ese.eur.nl).\\nManuscript received July 6, 2020; revised June 21, 2021; Corresponding\\nauthor: F. Frasincar\\nbe circumvented. As was the case for the introduction\\nof the original attention mechanism [3], the Transformer\\nmodel was created for machine translation, but was quickly\\nadopted to be used for other tasks, such as image processing\\n[15], video processing [16], and recommender systems [17].\\nThe purpose of this survey is to explain the general\\nform of attention, and provide a comprehensive overview\\nof attention techniques in deep learning. Other surveys have\\nalready been published on the subject of attention models.\\nFor example, in [18], a survey is presented on attention in\\ncomputer vision, [19] provides an overview of attention in\\ngraph models, and [20], [21], [22] are all surveys on attention\\nin natural language processing. This paper partly builds\\non the information presented in the previously mentioned\\nsurveys. Yet, we provide our own signicant contributions.\\nThe main difference between this survey and the previously\\nmentioned ones is that the other surveys generally focus\\non attention models within a certain domain. This survey,\\nhowever, provides a cross-domain overview of attention\\ntechniques. We discuss the attention techniques in a general\\nway, allowing them to be understood and applied in a\\nvariety of domains. Furthermore, we found the taxonomies\\npresented in previous surveys to be lacking the depth and\\nstructure needed to properly distinguish the various atten-\\ntion mechanisms. Additionally, certain signicant attention\\ntechniques have not yet been properly discussed in previ-\\nous surveys, while other presented attention mechanisms\\nseem to be lacking either technical details or intuitive ex-\\nplanations. Therefore, in this paper, we present important\\nattention techniques by means of a single framework using\\na uniform notation, a combination of both technical and in-\\ntuitive explanations for each presented attention technique,\\nand a comprehensive taxonomy of attention mechanisms.\\nThe structure of this paper is as follows. Section 2 in-\\ntroduces a general attention model that provides the reader\\nwith a basic understanding of the properties of attention\\nand how it can be applied. One of the main contributions\\nof this paper is the taxonomy of attention techniques pre-\\nsented in Section 3. In this section, attention mechanisms\\nare explained and categorized according to the presented\\narXiv:2203.14263v1  [cs.LG]  27 Mar 2022\\n2\\nFeature Model\\nQuery Model\\nAttention Model\\nOutput Model\\nFig. 1. An illustration of the general structure of the task model.\\ntaxonomy. Section 4 provides an overview of performance\\nmeasures and methods for evaluating attention models.\\nFurthermore, the taxonomy is used to evaluate the structure\\nof various attention models. Lastly, in Section 5, we give our\\nconclusions and suggestions for further research.\\n2\\nGENERAL ATTENTION MODEL\\nThis section presents a general form of attention with cor-\\nresponding notation. The notation introduced here is based\\non the notation that was introduced in [23] and popularized\\nin [13]. The framework presented in this section is used\\nthroughout the rest of this paper.\\nTo implement a general attention model, it is necessary\\nto rst describe the general characteristics of a model that\\ncan employ attention. First of all, we will refer to the\\ncomplete model as the task model, of which the structure is\\npresented in Fig. 1. This model simply takes an input, carries\\nout the specied task, and produces the desired output. For\\nexample, the task model can be a language model that takes\\nas input a piece of text, and produces as output a summary\\nof the contents, a classication of the sentiment, or the text\\ntranslated word for word to another language. Alternatively,\\nthe task model can take an image, and produce a caption\\nor segmentation for that image. The task model consists of\\nfour submodels: the feature model, the query model, the\\nattention model, and the output model. In Subsection 2.1,\\nthe feature model and query model are discussed, which\\nare used to prepare the input for the attention calculation.\\nIn Subsection 2.2, the attention model and output model are\\ndiscussed, which are concerned with producing the output.\\n2.1\\nAttention Input\\nSuppose the task model takes as input the matrix X \\nRdxnx, where dx represents the size of the input vectors\\nand nx represents the amount of input vectors. The columns\\nin this matrix can represent the words in a sentence,\\nthe pixels in an image, the characteristics of an acoustic\\nsequence, or any other collection of inputs. The feature\\nmodel is then employed to extract the nf feature vectors\\nf1, . . . , fnf Rdf from X, where df represents the size of\\nthe feature vectors. The feature model can be a recurrent\\nneural network (RNN), a convolutional neural network\\n(CNN), a simple embedding layer, a linear transformation\\nof the original data, or no transformation at all. Essentially,\\nthe feature model consists of all the steps that transform the\\noriginal input X into the feature vectors f1, . . . , fnf that the\\nattention model will attend to.\\nAttention\\nAlignment\\nAttention\\nScores\\nWeighted\\nAverage\\nKeys\\nValues\\nFig. 2. The inner mechanisms of the general attention module.\\nTo determine which vectors to attend to, the attention\\nmodel requires the query q Rdq, where dq indicates the\\nsize of the query vector. This query is extracted by the\\nquery model, and is generally designed based on the type\\nof output that is desired of the model. A query tells the\\nattention model which feature vectors to attend to. It can be\\ninterpreted literally as a query, or a question. For example,\\nfor the task of image captioning, suppose that one uses a\\ndecoder RNN model to produce the output caption based\\non feature vectors obtained from the image by a CNN. At\\neach prediction step, the hidden state of the RNN model can\\nbe used as a query to attend to the CNN feature vectors. In\\neach step, the query is a question in the sense that it asks for\\nthe necessary information from the feature vectors based on\\nthe current prediction context.\\n2.2\\nAttention Output\\nThe feature vectors and query are used as input for the\\nattention model. This model consists of a single, or a\\ncollection of general attention modules. An overview of a\\ngeneral attention module is presented in Fig. 2. The input of\\nthe general attention module is the query q Rdq, and the\\nmatrix of feature vectors F = [f1, . . . , fnf ] Rdf nf . Two\\nseparate matrices are extracted from the matrix F : the keys\\nmatrix K = [k1, . . . , knf ] Rdknf , and the values matrix\\nV = [v1, . . . , vnf ] Rdvnf , where dk and dv indicate,\\nrespectively, the dimensions of the key vectors (columns\\nof K) and value vectors (columns of V ). The general way\\nof obtaining these matrices is through a linear transforma-\\ntion of F using the weight matrices WK Rdkdf and\\nWV Rdvdf , for K and V , respectively. The calculations\\nof K and V are presented in (1). Both weight matrices can\\nbe learned during training or predened by the researcher.\\nFor example, one can choose to dene both WK and WV\\nas equal to the identity matrix to retain the original feature\\nvectors. Other ways of dening the keys and the values are\\nalso possible, such as using completely separate inputs for\\nthe keys and values. The only constraint to be obeyed is that\\nthe number of columns in K and V remains the same.\\nK\\ndknf\\n= WK\\ndkdf\\n\\nF\\ndf nf\\n,\\nV\\ndvnf\\n= WV\\ndvdf\\n\\nF\\ndf nf\\n.\\n(1)\\nThe goal of the attention module is to produce a\\nweighted average of the value vectors in V . The weights\\nused to produce this output are obtained via an attention\\nscoring and alignment step. The query q and the keys\\nmatrix K are used to calculate the vector of attention scores\\n3\\ne = [e1, . . . , enf ] Rnf . This is done via the score function\\nscore(), as illustrated in (2).\\nel\\n11 = score( q\\ndq1, kl\\ndk1).\\n(2)\\nAs discussed before, the query symbolizes a request for in-\\nformation. The attention score el represents how important\\nthe information contained in the key vector kl is according\\nto the query. If the dimensions of the query and key vectors\\nare the same, an example of a score function would be to\\ntake the dot-product of the vectors. The different types of\\nscore functions are further discussed in Section 3.2.1.\\nNext, the attention scores are processed further through\\nan alignment layer. The attention scores can generally have\\na wide range outside of [0, 1]. However, since the goal is to\\nproduce a weighted average, the scores are redistributed via\\nan alignment function align() as dened in (3).\\nal\\n11 = align( el\\n11; e\\nnf 1),\\n(3)\\nwhere al R1 is the attention weight corresponding to\\nthe lth value vector. One example of an alignment function\\nwould be to use a softmax function, but the various other\\nalignment types are discussed in Section 3.2.2. The attention\\nweights provide a rather intuitive interpretation for the\\nattention module. Each weight is a direct indication of how\\nimportant each feature vector is relative to the others for\\nthis particular problem. This can provide us with a more\\nin-depth understanding of the model behaviour, and the re-\\nlations between inputs and outputs. The vector of attention\\nweights a = [a1, . . . , anf ] Rnf is used to produce the\\ncontext vector c Rdv by calculating a weighted average of\\nthe columns of the values matrix V , as shown in (4).\\nc\\ndv1\\n=\\nnf\\nX\\nl=1\\nal\\n11\\n vl\\ndv1\\n.\\n(4)\\nAs illustrated in Fig. 1, the context vector is then used in\\nthe output model to create the output y. This output model\\ntranslates the context vector into an output prediction. For\\nexample, it could be a simple softmax layer that takes as\\ninput the context vector c, as shown in (5).\\ny\\ndy1 = softmax( Wc\\ndydv\\n\\nc\\ndv1 + bc\\ndy1),\\n(5)\\nwhere dy is the number of output choices or classes, and\\nWc Rdydv and bc Rdy are trainable weights.\\n2.3\\nAttention Applications\\nAttention is a rather general mechanism that can be used in\\na wide variety of problem domains. Consider the task of ma-\\nchine translation using an RNN model. Also, consider the\\nproblem of image classication using a basic CNN model.\\nWhile an RNN produces a sequence of hidden state vectors,\\na CNN creates feature maps, where each region in the image\\nis represented by a feature vector. The RNN hidden states\\nare organized sequentially, while the CNN feature maps\\nare organized spatially. Yet, attention can still be applied\\nin both situations, since the attention mechanism does not\\ninherently depend on the organization of the feature vectors.\\nThis characteristic makes attention easy to implement in a\\nwide variety of models in different domains.\\nAnother domain where attention can be applied is audio\\nprocessing [24], [25]. Acoustic sequences can be represented\\nby a sequence of feature vectors that relate to certain time\\nperiods of the audio sample. These vectors could simply\\nbe the raw input audio, or they can be extracted via, for\\nexample, an RNN or CNN. Video processing is another\\ndomain where attention can be applied intuitively [26], [27].\\nVideo data consists of sequences of images, so attention can\\nbe applied to the individual images, as well as the entire\\nsequence. Recommender systems often incorporate a users\\ninteraction history to produce recommendations. Feature\\nvectors can be extracted based on, for example, the ids\\nor other characteristics of the products the user interacted\\nwith, and attention can be applied to them [28]. Attention\\ncan generally also be applied to many problems that use\\na time series as input, be it medical [29], nancial [30], or\\nanything else, as long as feature vectors can be extracted.\\nThe fact that attention does not rely on the organization\\nof the feature vectors allows it to be applied to various\\nproblems that each use data with different structures, as\\nillustrated by the previous domain examples. Yet, this can be\\ntaken even further by applying attention to data where there\\nis irregular structure. For example, protein structures, city\\ntrafc ows, and communication networks cannot always\\nbe represented using neatly structured organizations, such\\nas sequences, like time series, or grids, like images. In such\\ncases, the different aspects of the data are often represented\\nas nodes in a graph. These nodes can be represented by\\nfeature vectors, meaning that attention can be applied in\\ndomains that use graph-structured data as well [19], [31].\\nIn general, attention can be applied to any problem for\\nwhich a set of feature vectors can be dened or extracted.\\nAs such, the general attention model presented in Fig. 2 is\\napplicable to a wide range of domains. The problem, how-\\never, is that there is a large variety of different applications\\nand extensions of the general attention module. As such,\\nin Section 3, a comprehensive overview is provided of a\\ncollection of different attention mechanisms.\\n3\\nATTENTION TAXONOMY\\nThere are many different types of attention mechanisms\\nand extensions, and a model can use different combina-\\ntions of these attention techniques. As such, we propose\\na taxonomy that can be used to classify different types of\\nattention mechanisms. Fig. 3 provides a visual overview of\\nthe different categories and subcategories that the attention\\nmechanisms can be organized in. The three major categories\\nare based on whether an attention technique is designed\\nto handle specic types of feature vectors (feature-related),\\nspecic types of model queries (query-related), or whether\\nit is simply a general mechanism that is related to neither\\nthe feature model, nor the query model (general). Further\\nexplanations of these categories and their subcategories are\\nprovided in the following subsections. Each mechanism\\ndiscussed in this section is either a modication to the\\nexisting inner mechanisms of the general attention module\\npresented in Section 2, or an extension of it.\\nThe presented taxonomy can also be used to analyze\\nthe architecture of attention models. Namely, the major\\ncategories and their subcategories can be interpreted as\\n4\\nAttention Mechanisms\\nFeature-Related\\nQuery-Related\\nGeneral\\nMultiplicity\\nLevels\\nRepresentations\\nScoring\\nAlignment\\nMultiplicity\\nType\\nDimensionality\\nSingular Features Attention\\nCoarse-Grained Co-Attention\\nFine-Grained Co-Attention\\nMulti-Grained Co-Attention\\nRotatory Attention\\nSingle-Level Attention\\nAttention-via-Attention\\nHierarchical Attention\\nSingle-Representational Attention\\nMulti-Representational Attention\\nAdditive Scoring\\nMultiplicative Scoring\\nScaled Multiplicative Scoring\\nGeneral Scoring\\nBiased General Scoring\\nActivated General Scoring\\nSimilarity Scoring\\nGlobal/Soft Alignment\\nHard Alignment\\nLocal Alignment\\nReinforced Alignment\\nSingle-Dimensional Attention\\nMulti-Dimensional Attention\\nBasic Queries\\nSpecialized Queries\\nSelf-Attentive Queries\\nAlternating Co-Attention\\nInteractive Co-Attention\\nParallel Co-Attention\\nSingular Query Attention\\nMulti-Head Attention\\nMulti-Hop Attention\\nCapsule-Based Attention\\nFig. 3. A taxonomy of attention mechanisms.\\nTABLE 1\\nNotation.\\nSymbol\\nDescription\\nF\\nMatrix of size df  nf containing the feature vectors\\nf1, . . . , fnf Rdf as columns. These feature vectors\\nare extracted by the feature model.\\nK\\nMatrix of size dk  nf containing the key vectors\\nk1, . . . , knf Rdk as columns. These vectors are used\\nto calculate the attention scores.\\nV\\nMatrix of size dv  nf containing the value vectors\\nv1, . . . , vnf Rdv as columns. These vectors are used\\nto calculate the context vector.\\nWK\\nWeights matrix of size dk  df used to create the K\\nmatrix from the F matrix.\\nWV\\nWeights matrix of size dv  df used to create the V\\nmatrix from the F matrix.\\nq\\nQuery vector of size dq. This vector essentially repre-\\nsents a question, and is used to calculate the attention\\nscores.\\nc\\nContext vector of size dv. This vector is the output of the\\nattention model.\\ne\\nScore vector of size dnf containing the attention scores\\ne1, . . . , enf R1. These are used to calculate the atten-\\ntion weights.\\na\\nAttention weights vector of size dnf containing the at-\\ntention weights a1, . . . , anf R1. These are the weights\\nused in the calculation of the context vector.\\northogonal dimensions of an attention model. An attention\\nmodel can consist of a combination of techniques taken\\nfrom any or all categories. Some characteristics, such as\\nthe scoring and alignment functions, are generally required\\nfor any attention model. Other mechanisms, such as multi-\\nhead attention or co-attention are not necessary in every\\nsituation. Lastly, in Table 1, an overview of used notation\\nwith corresponding descriptions is provided.\\n3.1\\nFeature-Related Attention Mechanisms\\nBased on a particular set of input data, a feature model\\nextracts feature vectors so that the attention model can\\nattend to these various vectors. These features may have\\nspecic structures that require special attention mechanisms\\nto handle them. These mechanisms can be categorized to\\ndeal with one of the following feature characteristics: the\\nmultiplicity of features, the levels of features, or the repre-\\nsentations of features.\\n3.1.1\\nMultiplicity of Features\\nFor most tasks, a model only processes a single input, such\\nas an image, a sentence, or an acoustic sequence. We refer\\nto such a mechanism as singular features attention. Other\\nmodels are designed to use attention based on multiple\\ninputs to allow one to introduce more information into the\\nmodel that can be exploited in various ways. However, this\\ndoes imply the presence of multiple feature matrices that\\nrequire special attention mechanisms to be fully used. For\\nexample, [32] introduces a concept named co-attention to\\nallow the proposed visual question answering (VQA) model\\nto jointly attend to both an image and a question.\\nCo-attention mechanisms can generally be split up\\ninto two groups [33]: coarse-grained co-attention and\\nne-grained co-attention. The difference between the two\\ngroups is the way attention scores are calculated based on\\nthe two feature matrices. Coarse-grained attention mecha-\\nnisms use a compact representation of one feature matrix\\nas a query when attending to the other feature vectors.\\nFine-grained co-attention, on the other hand, uses all feature\\nvectors of one input as queries. As such, no information is\\nlost, which is why these mechanisms are called ne-grained.\\nAs an example of coarse-grained co-attention, [32] pro-\\nposes an alternating co-attention mechanism that uses the\\ncontext vector (which is a compact representation) from one\\nattention module as the query for the other module, and\\nvice versa. Alternating co-attention is presented in Fig. 4.\\nGiven a set of two input matrices X(1) and X(2), features\\nare extracted by a feature model to produce the feature\\nmatrices F (1) Rd(1)\\nf\\nn(1)\\nf\\nand F (2) Rd(2)\\nf\\nn(2)\\nf , where d(1)\\nf\\n5\\nAttention\\nModule1\\nAttention\\nModule2\\n\\x97(1)\\n\\x97(2)\\n(0)\\n(2)\\nAttention\\nModule1\\n\\x97(1)\\n(1)\\nFig. 4. An illustration of alternating co-attention.\\nand d(2)\\nf\\nrepresent, respectively, the dimension of the feature\\nvectors extracted from the rst and second inputs, while\\nn(1)\\nf\\nand n(2)\\nf\\nrepresent, respectively, the amount of feature\\nvectors extracted from the rst and second inputs. In [32],\\nco-attention is used for VQA, so the two input matrices are\\nthe image data and the question data, for which the feature\\nmodel for the image consists of a CNN model, and the\\nfeature model for the question consists of word embeddings,\\na convolutional layer, a pooling layer, and an LSTM model.\\nFirstly, attention is calculated for the rst set of features F (1)\\nwithout the use of a query (Attention Module1 in Fig. 4). In\\n[32], an adjusted additive attention score function is used for\\nthis attention mechanism. The general form of the regular\\nadditive score function can be seen in (6).\\nscore( q\\ndq1\\n, kl\\ndk1\\n) = wT\\n1dw\\nact(W1\\ndwdq\\n q\\ndq1\\n+ W2\\ndwdk\\n kl\\ndk1\\n+ b\\ndw1\\n), (6)\\nwhere act() is a non-linear activation function, and w \\nRdw, W1 Rdwdq, W2 Rdwdk, and b Rdw are\\ntrainable weights matrices, for which dw is a predened\\ndimension of the weight matrices. A variant of this score\\nfunction adapted to be calculated without a query for the\\napplication at hand can be seen in (7).\\ne(0)\\nl\\n11\\n= w(1)T\\n1dw\\n act(W (1)\\ndwd(1)\\nk\\n k(1)\\nl\\nd(1)\\nk\\n1\\n+ b(1)\\ndw1\\n),\\n(7)\\nwhere w(1) Rdw, W (1) Rdwd(1)\\nk , and b(1) Rdw are\\ntrainable weight matrices for Attention Module1, k(1)\\nl\\n\\nRd(1)\\nk\\nis the lth column of the keys matrix K(1) that was\\nobtained from F (1) via a linear transformation (see (1)), for\\nwhich dw is a prespecied dimension of the weight matrices\\nand d(1)\\nk\\nis a prespecied dimension of the key vectors.\\nPerhaps one may wonder why the query is absent when\\ncalculating attention in this manner. Essentially, the query in\\nthis attention model is learned alongside the other trainable\\nparameters. As such, the query can be interpreted as a\\ngeneral question: Which feature vectors contain the most\\nimportant information?. This is also known as a self-\\nattentive mechanism, since attention is calculated based\\nonly on the feature vectors themselves. Self-attention is\\nexplained in more detail in Subsection 3.3.1.\\nThe scores are combined with an alignment function\\n(see (3)), such as the softmax function, to create attention\\nweights used to calculate the context vector c(0) Rd(1)\\nv\\n(see (4)). This context vector is not used as the output of\\nthe attention model, but rather as a query for calculating\\nthe context vector c(2) Rd(2)\\nv , based on the second feature\\nmatrix F (2), where d(2)\\nv\\nis the dimension of the value vectors\\nobtained from F (2) via a linear transformation (see (1)). For\\nAttention\\nModule1\\nAttention\\nModule2\\n\\x97(1)\\n\\x97(2)\\n\\x9c(1)\\n\\x9c(2)\\n(1)\\n(2)\\nMean\\nMean\\n (1)\\n (2)\\nFig. 5. An illustration of interactive co-attention.\\nthis module (Attention Module2 in Fig. 4), attention scores\\nare calculated using another score function with c0 as query\\ninput, as presented in (8). Any function can be used in this\\nsituation, but an additive function is used in [32].\\ne(2)\\nl\\n11\\n= score( c(0)\\nd(1)\\nv\\n1\\n, k(2)\\nl\\nd(2)\\nk\\n1\\n).\\n(8)\\nThese attention scores are then used to calculate attention\\nweights using, for example, a softmax function as alignment\\nfunction, after which the context vector c(2) can be derived\\nas a weighted average of the second set of value vectors.\\nFinally, the context vector c(2) is used as a query for the rst\\nattention module, which will produce the context vector\\nc(1) for the rst feature matrix F (1). Attention scores are\\ncalculated according to (9). In [32], the same function and\\nweight matrices as seen in (7) are used, but with an added\\nquery making it the same as the general additive score\\nfunction (see (6)). The rest of the attention calculation is\\nsimilar as before.\\ne(1)\\nl\\n11\\n= score( c(2)\\nd(2)\\nv\\n1\\n, k(1)\\nl\\nd(1)\\nk\\n1\\n).\\n(9)\\nThe produced context vectors c(1) and c(2) are concatenated\\nand used for prediction in the output model. Alternating\\nco-attention inherently contains a form of sequentiality due\\nto the fact that context vectors need to be calculated one\\nafter another. This may come with a computational dis-\\nadvantage since it is not possible to parallelize. Instead of\\nusing a sequential mechanism like alternating co-attention,\\n[34] proposes the interactive co-attention mechanism that\\ncan calculate attention on both feature matrices in parallel,\\nas depicted in Fig. 5. Instead of using the context vectors as\\nqueries, unweighted averages of the key vectors are used as\\nqueries. The calculation of the average keys are provided in\\n(10), and the calculation of the attention scores are shown\\nin (11). Any score function can be used in this case, but an\\nadditive score function is used in [34].\\nk(1)\\nd(1)\\nk\\n1\\n=\\n1\\nn(1)\\nf\\nn(1)\\nf\\nX\\nl=1\\nk(1)\\nl\\nd(1)\\nk\\n1\\n,\\nk(2)\\nd(2)\\nk\\n1\\n=\\n1\\nn(2)\\nf\\nn(2)\\nf\\nX\\nl=1\\nk(2)\\nl\\nd(2)\\nk\\n1\\n;\\n(10)\\ne(1)\\nl\\n11\\n= score( k(2)\\nd(2)\\nk\\n1\\n, k(1)\\nl\\nd(1)\\nk\\n1\\n), e(2)\\nl\\n11\\n= score( k(1)\\nd(1)\\nk\\n1\\n, k(2)\\nl\\nd(2)\\nk\\n1\\n). (11)\\nFrom the attention scores, attention weights are created via\\nan alignment function, and are used to produce the context\\nvectors c(1) and c(2).\\nWhile coarse-grained co-attention mechanisms use a\\ncompact representation of one input to use as a query when\\n6\\ncalculating attention for another input, ne-grained co-\\nattention considers every element of each input individually\\nwhen calculating attention scores. In this case, the query\\nbecomes a matrix. An example of ne-grained co-attention\\nis parallel co-attention [32]. Similarly to interactive co-\\nattention, parallel co-attention calculates attention on the\\ntwo feature matrices at the same time, as shown in Fig. 6. We\\nstart by evaluating the keys matrices K(1) Rd(1)\\nk n(1)\\nf\\nand\\nK(2) Rd(2)\\nk n(2)\\nf\\nthat are obtained by linearly transforming\\nthe feature matrices F (1) and F (2), where d(1)\\nk\\nand d(2)\\nk\\nare\\nprespecied dimensions of the keys. The idea is to use the\\nkeys matrix from one input as the query for calculating\\nattention on the other input. However, since K(1) and K(2)\\nhave completely different dimensions, an afnity matrix\\nA Rn(1)\\nf\\nn(2)\\nf\\nis calculated that is used to essentially\\ntranslate one keys matrix to the space of the other keys.\\nIn [32], A is calculated as shown in (12).\\nA\\nn(1)\\nf\\nn(2)\\nf\\n= act( K(1)T\\nn(1)\\nf\\nd(1)\\nk\\n\\nWA\\nd(1)\\nk\\nd(2)\\nk\\n K(2)\\nd(2)\\nk\\nn(2)\\nf\\n),\\n(12)\\nwhere WA Rd(1)\\nk d(2)\\nk\\nis a trainable weights matrix and\\nact() is an activation function for which the tanh() function\\nis used in [32]. [35] proposes a different way of calculating\\nthis matrix, i.e., one can use (13) to calculate each individual\\nelement Ai,j of the matrix A.\\nAi,j\\n11\\n= wT\\nA\\n13dk\\n concat(k(1)\\ni\\ndk1\\n, k(2)\\nj\\ndk1\\n, k(1)\\ni\\ndk1\\nk(2)\\nj\\ndk1\\n),\\n(13)\\nwhere wA R3dk denotes a trainable vector of weights,\\nconcat() denotes vector concatenation, and denotes\\nelement-wise multiplication, also known as the Hadamard\\nproduct. Note that the keys of each keys matrix in this\\ncase must have the same dimension dk for the element-\\nwise multiplication to work. The afnity matrix can be\\ninterpreted as a similarity matrix for the columns of the two\\nkeys matrices, and helps translate, for example, image keys\\nto the same space as the keys of the words in a sentence,\\nand vice versa. The vectors of attention scores e(1) and e(2)\\ncan be calculated using an altered version of the additive\\nscore function as presented in (14) and (15). The previous\\nattention score examples in this survey all used a score func-\\ntion to calculate each attention score for each value vector\\nindividually. However, (14) and (15) are used to calculate\\nthe complete vector of all attention scores. Essentially, the\\nattention scores are calculated in an aggregated form.\\ne(1)\\n1n(1)\\nf\\n= w1\\n1dw\\nact(W2\\ndwd(2)\\nk\\n K(2)\\nd(2)\\nk\\nn(2)\\nf\\n AT\\nn(2)\\nf\\nn(1)\\nf\\n+ W1\\ndwd(1)\\nk\\n K(1)\\nd(1)\\nk\\nn(1)\\nf\\n); (14)\\ne(2)\\n1n(2)\\nf\\n= w2\\n1dw\\nact(W1\\ndwd(1)\\nk\\n K(1)\\nd(1)\\nk\\nn(1)\\nf\\n\\nA\\nn(1)\\nf\\nn(2)\\nf\\n+ W2\\ndwd(2)\\nk\\n K(2)\\nd(2)\\nk\\nn(2)\\nf\\n), (15)\\nwhere w1\\n\\nRdw, w2\\n\\nRdw, W1\\n\\nRdwd(1)\\nk , and\\nW2 Rdwd(2)\\nk\\nare trainable weight matrices, for which\\ndw is a prespecied dimension of the weight matrices. Note\\nthat tanh() is used in [32] for the activation function, and the\\nfeature matrices are used as the key matrices. In that case,\\nthe afnity matrix A can be seen as a translator between\\nfeature spaces. As mentioned before, the afnity matrix is\\nessentially a similarity matrix for the key vectors of the two\\nAttention\\nModule1\\nAttention\\nModule2\\n\\x97(1)\\n\\x97(2)\\n\\x9c(1)\\n\\x9c(2)\\n(1)\\n(2)\\nAfnity\\nMatrix\\n\\x92\\nFig. 6. An illustration of parallel co-attention.\\ninputs. In [33], this fact is used to propose a different way\\nof determining attention scores. Namely, one could take\\nthe maximum similarity value in a row or column as the\\nattention score, as shown in (16).\\ne(1)\\ni\\n11 =\\nmax\\nj=1,...,n(2)\\nf\\nAi,j\\n11 ,\\ne(2)\\nj\\n11 =\\nmax\\ni=1,...,n(1)\\nf\\nAi,j\\n11 .\\n(16)\\nNext, the attention scores are used to calculate attention\\nweights using an alignment function, so that two context\\nvectors c(1) and c(2) can be derived as weighted averages of\\nthe value vectors that are obtained from linearly transform-\\ning the features. For the alignment function, [32] proposes\\nto use a softmax function, and the value vectors are simply\\nset equal to the feature vectors. The resulting context vectors\\ncan be either concatenated or added together.\\nFinally, coarse-grained and ne-grained co-attention can\\nbe combined to create an even more complex co-attention\\nmechanism. [33] proposes the multi-grained co-attention\\nmechanism that calculates both coarse-grained and ne-\\ngrained co-attention for two inputs. Each mechanism pro-\\nduces one context vector per input. The four resulting\\ncontext vectors are concatenated and used in the output\\nmodel for prediction.\\nA mechanism separate from co-attention that still uses\\nmultiple inputs is the rotatory attention mechanism [36].\\nThis technique is typically used in a text sentiment analysis\\nsetting where there are three inputs involved: the phrase for\\nwhich the sentiment needs to be determined (target phrase),\\nthe text before the target phrase (left context), and the text af-\\nter the target phrase (right context). The words in these three\\ninputs are all encoded by the feature model, producing the\\nfollowing feature matrices: F t = [f t\\n1, . . . , f t\\nnt\\nf ] Rdt\\nf nt\\nf ,\\nF l = [f l\\n1, . . . , f l\\nnl\\nf ] Rdl\\nf nl\\nf , and F r = [f r\\n1 , . . . , f r\\nnr\\nf ] \\nRdr\\nf nr\\nf , for the target phrase words, left context words,\\nand right context words, respectively, where dt\\nf, dl\\nf, and\\ndr\\nf represent the dimensions of the feature vectors for the\\ncorresponding inputs, and nt\\nf, nl\\nf, and nr\\nf represent the\\nnumber of feature vectors for the corresponding inputs. The\\nfeature model used in [36] consists of word embeddings\\nand separate Bi-LSTM models for the target phrase, the left\\ncontext, and the right context. This means that the feature\\nvectors are in fact the hidden state vectors obtained from the\\nBi-LSTM models. Using these features, the idea is to extract\\na single vector r from the inputs such that a softmax layer\\ncan be used for classication. As such, we are now faced\\nwith two challenges: how to represent the inputs as a single\\nvector, and how to incorporate the information from the left\\nand right context into that vector. [36] proposes to use the\\nrotatory attention mechanism for this purpose.\\n7\\nFirstly, a single target phrase representation is created\\nby using a pooling layer that takes the average over the\\ncolumns of F t, as shown in (17).\\nrt\\ndt\\nf 1\\n= 1\\nnt\\nf\\nnt\\nf\\nX\\ni=1\\nf t\\ni\\ndt\\nf 1\\n.\\n(17)\\nrt is then used as a query to create a context vector out of\\nthe left and right contexts, separately. For example, for the\\nleft context, the key vectors kl\\n1, . . . , kl\\nnl\\nf Rdl\\nk and value\\nvectors vl\\n1, . . . , vl\\nnl\\nf Rdl\\nv are extracted from the left context\\nfeature vectors f l\\n1, . . . , f l\\nnl\\nf Rdl\\nf , similarly as before, where\\ndl\\nk and dl\\nv are the dimensions of the key and value vectors,\\nrespectively. Note that [36] proposes to use the original\\nfeature vectors as keys and values, meaning that the linear\\ntransformation consists of a multiplication by an identity\\nmatrix. Next, the scores are calculated using (18).\\nel\\ni\\n11 = score( rt\\ndt\\nf 1, kl\\ni\\ndl\\nk1).\\n(18)\\nFor the score function, [36] proposes to use an activated\\ngeneral score function [34] with a tanh activation function.\\nThe attention scores can be combined with an alignment\\nfunction and the corresponding value vectors to produce\\nthe context vector rl Rdl\\nv. The alignment function used in\\n[36] takes the form of a softmax function. An analogous pro-\\ncedure can be performed to obtain the representation of the\\nright context, rr. These two context representations can then\\nbe used to create new representations of the target phrase,\\nagain, using attention. Firstly, the key vectors kt\\n1, . . . , kt\\nnt\\nf \\nRdt\\nk and value vectors vt\\n1, . . . , vt\\nnt\\nf\\nRdt\\nv are extracted\\nfrom the target phrase feature vectors f t\\n1, . . . , f t\\nnt\\nf Rdt\\nf ,\\nsimilarly as before, using a linear transformation, where dt\\nk\\nand dt\\nv are the dimensions of the key and value vectors,\\nrespectively. Note, again, that the original feature vectors as\\nkeys and values in [36]. The attention scores for the left-\\naware target representation are then calculated using (19).\\nelt\\ni\\n11\\n= score( rl\\ndl\\nv1\\n, kt\\ni\\ndt\\nk1\\n).\\n(19)\\nThe attention scores can be combined with an alignment\\nfunction and the corresponding value vectors to produce\\nthe context vector rlt Rdt\\nv. For this attention calculation,\\n[34] proposes to use the same score and alignment functions\\nas before. The right-aware target representation rrt can be\\ncalculated in a similar manner. Finally, to obtain the full\\nrepresentation vector r that is used to determine the clas-\\nsication, the vectors rl, rr, rlt, and rrt are concatenated\\ntogether, as shown in (20).\\nr\\n(dl\\nv+dr\\nv+dt\\nv+dt\\nv)1 = concat( rl\\ndl\\nv1, rr\\ndr\\nv1, rlt\\ndt\\nv1, rrt\\ndt\\nv1).\\n(20)\\nTo summarize, rotatory attention uses the target phrase\\nto compute new representations for the left and right context\\nusing attention, and then uses these left and right repre-\\nsentations to calculate new representations for the target\\nphrase. The rst step is designed to capture the words\\nin the left and right contexts that are most important to\\nthe target phrase. The second step is there to capture the\\nmost important information in the actual target phrase itself.\\nEssentially, the mechanism rotates attention between the\\ntarget and the contexts to improve the representations.\\nThere are many applications where combining informa-\\ntion from different inputs into a single model can be highly\\nbenecial. For example, in the eld of medical data, there\\nare often many different types of data available, such as\\nvarious scans or documents, that can provide different types\\nof information. In [37], a co-attention mechanism is used\\nfor automatic medical report generation to attend to both\\nimages and semantic tags simultaneously. Similarly, in [38],\\na co-attention model is proposed that combines general de-\\nmographics features and patient medical history features to\\npredict future health information. Additionally, an ablation\\nstudy is used in [38] to show that the co-attention part of\\nthe model specically improves performance. A eld where\\nmulti-feature attention has been extensively explored is the\\ndomain of recommender systems. For example, in [39], a co-\\nattention network is proposed that attends to both product\\nreviews and the reviews a user has written. In [40], a model\\nis proposed for video recommendation that attends to both\\nuser features and video features. Co-attention techniques\\nhave also been used in combination with graph networks for\\nthe purpose of, for example, reading comprehension across\\nmultiple documents [41] and fake news detection [42]. In\\ncomparison to co-attention, rotatory attention has typically\\nbeen explored only in the eld of sentiment analysis, which\\nis most likely due to the specic structure of the data that\\nis necessary to use this technique. An implementation of\\nrotatory attention is proposed in [43] for sentiment analysis,\\nwhere the mechanism is extended by repeating the attention\\nrotation to iteratively further improve the representations.\\n3.1.2\\nFeature Levels\\nThe previously discussed attention mechanisms process\\ndata at a single level. We refer to these attention techniques\\nas single-level attention mechanisms. However, some data\\ntypes can be analyzed and represented on multiple levels.\\nFor example, when analyzing documents, one can analyze\\nthe document at the sentence level, word level, or even\\nthe character level. When representations or embeddings\\nof all these levels are available, one can exploit the extra\\nlevels of information. For example, one could choose to\\nperform translation based on either just the characters, or\\njust the words of the sentence. However, in [44], a technique\\nnamed attention-via-attention is introduced that allows one\\nto incorporate information from both the character, and the\\nword levels. The idea is to predict the sentence translation\\ncharacter-by-character, while also incorporating information\\nfrom a word-level attention module.\\nTo begin with, a feature model (consisting of, for ex-\\nample, word embeddings and RNNs) is used to encode\\nthe input sentence into both a character-level feature ma-\\ntrix F (c) Rd(c)\\nf\\nn(c)\\nf , and a word-level feature matrix\\nF (w) Rd(w)\\nf\\nn(w)\\nf\\n, where d(c)\\nf\\nand n(c)\\nf\\nrepresent, respec-\\ntively, the dimension of the embeddings of the characters,\\nand the number of characters, while d(w)\\nf\\nand n(w)\\nf\\nrepresent\\nthe same but at the word level. It is crucial for this method\\nthat each level in the data can be represented or embedded.\\nWhen attempting to predict a character in the translated\\n8\\nAttention\\nModuleW\\nAttention\\nModuleC\\nFig. 7. An illustration of attention-via-attention.\\nsentence, a query q(c) Rdq is created by the query model\\n(like a character-level RNN), where dq is the dimension of\\nthe query vectors. As illustrated in Fig. 7, the query is used\\nto calculate attention on the word-level feature vectors F (w).\\nThis generates the context vector c(w) Rd(w)\\nv\\n, where d(w)\\nv\\nrepresents the dimension of the value vectors for the word-\\nlevel attention module. This context vector summarizes\\nwhich words contain the most important information for\\npredicting the next character. If we know which words are\\nmost important, then it becomes easier to identify which\\ncharacters in the input sentence are most important. Thus,\\nthe next step is to attend to the character-level features in\\nF (c), with an additional query input: the word-level context\\nvector c(w). The actual query input for the attention model\\nwill therefore be the concatenation of the query q(c) and the\\nword context vector c(w). The output of this character-level\\nattention module is the context vector c(c). The complete\\ncontext output of the attention model is the concatenation\\nof the word-level, and character-level context vectors.\\nThe attention-via-attention technique uses representa-\\ntions for each level. However, accurate representations may\\nnot always be available for each level of the data, or it\\nmay be desirable to let the model create the representations\\nduring the process by building them from lower level repre-\\nsentations. A technique referred to as hierarchical attention\\n[5] can be used in this situation. Hierarchical attention is\\nanother technique that allows one to apply attention on\\ndifferent levels of the data. Yet, the exact mechanisms work\\nquite differently compared to attention-via-attention. The\\nidea is to start at the lowest level, and then create repre-\\nsentations, or summaries, of the next level using attention.\\nThis process is repeated till the highest level is reached. To\\nmake this a little clearer, suppose one attempts to create\\na model for document classication, similarly to the im-\\nplementation from [5]. We analyze a document containing\\nnS sentences, with the sth sentence containing ns words,\\nfor s = 1, . . . , nS. One could use attention based on just\\nthe collection of words to classify the document. However,\\na signicant amount of important context is then left out\\nof the analysis, since the model will consider all words\\nas a single long sentence, and will therefore not consider\\nthe context within the separate sentences. Instead, one can\\nuse the hierarchical structure of a document (words form\\nsentences, and sentences form the document).\\nFig. 8 illustrates the structure of hierarchical attention.\\nFor each sentence in the document, a sentence representa-\\ntion c(s) Rd(S)\\nv\\nis produced, for s = 1, . . . , nS, where d(S)\\nv\\nis the dimension of the value vectors used in the attention\\nmodel for sentence representations (Attention ModuleS in\\n\\x97(1)\\nAttention\\nModuleS\\n(\\n)\\n\\x83o\\nAttention\\nModuleS\\nAttention\\nModuleS\\n\\nAttention\\nModuleD\\n\\x97(2)\\n\\x97(\\n)\\n\\x83o\\n\\x94\\n(2)\\n(1)\\n(`)\\nFig. 8. An illustration of hierarchical attention.\\nFig. 8). The representation is a context vector from an\\nattention module that essentially summarizes the sentence.\\nEach sentence is rst put through a feature model to extract\\nthe feature matrix F (s) Rd(S)\\nf\\nns, for s = 1, . . . , nS, where\\nd(S)\\nf\\nrepresents the dimension of the feature vector for each\\nword, and ns represents the amount of words in sentence s.\\nFor extra clarication, the columns of F (s) are feature vec-\\ntors that correspond to the words in sentence s. As shown in\\nFig. 8, each feature matrix F (s) is used as input for an atten-\\ntion model, which produces the context vector c(s), for each\\ns = 1, . . . , nS. No queries are used in this step, so it can be\\nconsidered a self-attentive mechanism. The context vectors\\nare essentially summaries of the words in the sentences. The\\nmatrix of context vectors C = [c(1), . . . , c(nS)] Rd(S)\\nv\\nnS\\nis constructed by grouping all the obtained context vectors\\ntogether as columns. Finally, attention is calculated using C\\nas feature input, producing the representation of the entire\\ndocument in the context vector c(D) Rd(D)\\nv\\n, where d(D)\\nv\\nis\\nthe dimension of the value vectors in the attention model\\nfor document representation (Attention ModuleD in Fig. 8).\\nThis context vector can be used to classify the document,\\nsince it is essentially a summary of all the sentences (and\\ntherefore also the words) in the document.\\nMulti-level models can be used in a variety of tasks.\\nFor example, in [28], hierarchical attention is used in a\\nrecommender system to model user preferences at the long-\\nterm level and the short-term level. Similarly, [45] proposes\\na hierarchical model for recommending social media images\\nbased on user preferences. Hierarchical attention has also\\nbeen successfully applied in other domains. For example,\\n[46] proposes to use hierarchical attention in a video action\\nrecognition model to capture motion information at the the\\nlong-term level and the short-term level. Furthermore, [47]\\nproposes a hierarchical attention model for cross-domain\\nsentiment classication. In [48], a hierarchical attention\\nmodel for chatbot response generation is proposed. Lastly,\\nusing image data, [49] proposes a hierarchical attention\\nmodel for crowd counting.\\n3.1.3\\nFeature Representations\\nIn a basic attention model, a single embedding or rep-\\nresentation model is used to produce feature representa-\\ntions for the model to attend to. This is referred to as\\nsingle-representational attention. Yet, one may also opt\\nto incorporate multiple representations into the model. In\\n[50], it is argued that allowing a model access to multiple\\n9\\nembeddings can allow one to create even higher quality\\nrepresentations. Similarly, [51] incorporates multiple repre-\\nsentations of the same book (textual, syntactic, semantic,\\nvisual etc.) into the feature model. Feature representations\\nare an important part of the attention model, but attention\\ncan also be an important part of the feature model. The\\nidea is to create a new representation by taking a weighted\\naverage of multiple representations, where the weights are\\ndetermined via attention. This technique is referred to as\\nmulti-representational attention, and allows one to create\\nso-called meta-embeddings. Suppose one wants to create a\\nmeta-embedding for a word x for which E embeddings\\nx(e1), . . . , x(eE) are available. Each embedding x(ei) is of\\nsize dei, for i = 1, . . . , E. Since not all embeddings are of the\\nsame size, a transformation is performed to normalize the\\nembedding dimensions. Using embedding-specic weight\\nparameters, each embedding x(ei) is transformed into the\\nsize-normalized embedding x(ti) Rdt, where dt is the size\\nof every transformed word embedding, as shown in (21).\\nx(ti)\\ndt1 = Wei\\ndtdei\\n x(ei)\\ndei 1 + bei\\ndt1,\\n(21)\\nwhere Wei\\n\\nRdtdei , and bei\\n\\nRdt are trainable,\\nembedding-specic weights matrices. The nal embedding\\nx(e) Rdt is a weighted average of the previously calcu-\\nlated transformed representations, as shown in (22).\\nx(e)\\ndt1 =\\nE\\nX\\ni=1\\nai\\n11  x(ti)\\ndt1 .\\n(22)\\nThe nal representation x(e) can be interpreted as the\\ncontext vector from an attention model, meaning that the\\nweights a1, . . . , aE\\nR1 are attention weights. Atten-\\ntion can be calculated as normally, where the columns of\\nthe features matrix F are the transformed representations\\nx(t1), . . . , x(tE). The query in this case can be ignored since\\nit is constant in all cases. Essentially, the query is Which\\nrepresentations are the most important? in every situation.\\nAs such, this is a self-attentive mechanism.\\nWhile\\nan\\ninteresting\\nidea,\\napplications\\nof\\nmulti-\\nrepresentational attention are limited. One example of the\\napplication of this technique is found in [52], where a multi-\\nrepresentational attention mechanism has been applied to\\ngenerate multi-lingual meta-embeddings. Another example\\nis [53], where a multi-representational text classication\\nmodel is proposed that incorporates different representa-\\ntions of the same text. For example, the proposed model uses\\nembeddings from part-of-speech tagging, named entity rec-\\nognizers, and character-level and word-level embeddings.\\n3.2\\nGeneral Attention Mechanisms\\nThis major category consists of attention mechanisms that\\ncan be applied in any type of attention model. The structure\\nof this component can be broken down into the following\\nsub-aspects: the attention score function, the attention align-\\nment, and attention dimensionality.\\n3.2.1\\nAttention Scoring\\nThe attention score function is a crucial component in how\\nattention is calculated. Various approaches have been de-\\nveloped that each have their own advantages and disadvan-\\ntages. An overview of these functions is provided in Table 2.\\nTABLE 2\\nOverview of score function (score(q, kl)) forms.\\nName\\nFunction\\nParameters\\nAdditive\\n(Concatenate) [3]\\nwT  act(W1  q + W2  kl) + b)\\nw Rdw\\nW1 Rdwdq\\nW2 Rdwdk\\nb Rdw\\nMultiplicative\\n(Dot-Product) [4]\\nqT  kl\\n-\\nScaled Multiplicative [13]\\nqT kl\\n\\ndk\\n-\\nGeneral [4]\\nkT\\nl  W  q\\nW Rdkdq\\nBiased General [54]\\nkT\\nl  (W  q + b)\\nW Rdkdq\\nb Rdk\\nActivated General [34]\\nact(kT\\nl  W  q + b)\\nW Rdkdq,\\nb R1\\nSimilarity [55]\\nsimilarity(q, kl)\\n-\\nEach row of Table 2 presents a possible form for the function\\nscore(q, kl), as seen in (23), where q is the query vector, and\\nkl is the lth column of K. Note that the score functions\\npresented in this section can be more efciently calculated\\nin matrix form using K instead of each column separately.\\nNevertheless, the score functions are presented using kl to\\nmore clearly illustrate the relation between a key and query.\\nel\\n11 = score( q\\ndq1, kl\\ndk1).\\n(23)\\nDue to their simplicity, the most popular choices for the\\nscore function are the concatenate score function [3] and the\\nmultiplicative score function [4]. The multiplicative score\\nfunction has the advantage of being computationally inex-\\npensive due to highly optimized vector operations. How-\\never, the multiplicative function may produce non-optimal\\nresults when the dimension dk is too large [56]. When dk\\nis large, the dot-product between q and kl can grow large\\nin magnitude. To illustrate this, in [13], an example is used\\nwhere the elements of q and kl are all normally distributed\\nwith a mean equal to zero, and a variance equal to one.\\nThen, the dot-product of the vectors has a variance of dk.\\nA higher variance means a higher chance of numbers that\\nare large in magnitude. When the softmax function of the\\nalignment step is then applied using these large numbers,\\nthe gradient will become very small, meaning the model\\nwill have trouble converging [13]. To adjust for this, [13]\\nproposes to scale the multiplicative function by the factor\\n1\\ndk , producing the scaled multiplicative score function.\\nIn [4], the multiplicative score function is extended by\\nintroducing a weights matrix W . This form, referred to\\nas the general score function, allows for an extra trans-\\nformation of kl. The biased general score function [54] is\\na further extension of the general function that introduces\\na bias weight vector b. A nal extension on this function\\nnamed the activated general score function is introduced in\\n[34], and includes the use of both a bias weight b, and an\\nactivation function act().\\nThe previously presented score functions are all based on\\ndetermining a type of similarity between the key vector and\\nthe query vector. As such, more typical similarity measures,\\nsuch as the Euclidean (L2) distance and cosine similarity,\\ncan also be implemented [55]. These scoring methods are\\nsummarized under the similarity score function which is\\nrepresented by the similarity() function.\\n10\\nThere typically is no common usage across domains\\nregarding score functions. The choice of score function for\\na particular task is most often based on empirical experi-\\nments. However, there are exceptions when, for example,\\nefciency is vital. In models where this is the case, the mul-\\ntiplicative or scaled multiplicative score functions are typi-\\ncally the best choice. An example of this is the Transformer\\nmodel, which is generally computationally expensive.\\n3.2.2\\nAttention Alignment\\nThe attention alignment is the step after the attention scor-\\ning. This alignment process directly determines which parts\\nof the input data the model will attend to. The alignment\\nfunction is denoted as align() and has various forms. The\\nalign() function takes as input the previously calculated\\nattention score vector e and calculates for each element el of\\ne the attention weight al. These attention weights can then\\nbe used to create the context vector c by taking a weighted\\naverage of the value vectors v1, . . . , vnf :\\nc\\ndv1 =\\nnf\\nX\\nl=1\\nal\\n11  vl\\ndv1.\\n(24)\\nThe most popular alignment method to calculate these\\nweights is a simple softmax function, as depicted in (25).\\nal\\n11 = align( el\\n11; e\\nnf 1) =\\nexp(el)\\nPnf\\nj=1 exp(ej).\\n(25)\\nThis alignment method is often referred to as soft alignment\\nin computer vision settings [8], or global alignment for se-\\nquence data [4]. Nevertheless, both these terms represent the\\nsame function and can be interpreted similarly. Soft/global\\nalignment can be interpreted as the model attending to\\nall feature vectors. For example, the model attends to all\\nregions in an image, or all words in a sentence. Even though\\nthe attention model generally does focus more on specic\\nparts of the input, every part of the input will receive at least\\nsome amount of attention due to the nature of the softmax\\nfunction. Furthermore, an advantage of the softmax function\\nis that it introduces a probabilistic interpretation to the input\\nvectors. This allows one to easily analyze which parts of the\\ninput are important to the output predictions.\\nIn contrast to soft/global alignment, other methods aim\\nto achieve a more focused form of alignment. For example,\\nhard alignment [8], also known as hard attention or non-\\ndeterministic attention, is an alignment type that forces\\nthe attention model to focus on exactly one feature vector.\\nFirstly, this method implements the softmax function in the\\nexact same way as global alignment. However, the outputs\\na1, . . . , anf are not used as weights for the context vector\\ncalculation. Instead, these values are used as probabilities\\nto draw the choice of the one value vector from. A value\\nm R1 is drawn from a multinomial distribution with\\na1, . . . , anf as parameters for the probabilities. Then, the\\ncontext vector is simply dened as follows:\\nc\\ndv1 = vm\\ndv1.\\n(26)\\nHard alignment is typically more efcient at inference\\ncompared to soft alignment. On the other hand, the main\\ndisadvantage of hard attention is that, due to the stochastic\\nalignment of attention, the training of the model cannot\\nbe done via the regular backpropagation method. Instead,\\nsimulation and sampling, or reinforcement learning [57]\\nare required to calculate the gradient at the hard attention\\nlayer. As such, soft/global attention is generally preferred.\\nHowever, a compromise can be made in certain situations.\\nLocal alignment [4] is a method that implements a softmax\\ndistribution, similarly to soft/global alignment. But, the\\nsoftmax distribution is calculated based only on a subset\\nof the inputs. This method is generally used in combination\\nwith sequence data. One has to specify a variable p R1\\nthat determines the position of the region. Feature vectors\\nclose to p will be attended to by the model, and vectors\\ntoo far from p will be ignored. The size of the subset\\nwill be determined by the variable D R1. Summarizing,\\nthe attention model will apply a softmax function on the\\nattention scores in the subset [p D, p + D]. In other words,\\na window is placed on the input and soft/global attention\\nis calculated within that window:\\nal\\n11\\n= align( el\\n11\\n; e\\nnf 1\\n) =\\nexp(el)\\nPp+D\\nj=pD exp(ej)\\n.\\n(27)\\nThe question that remains is how to determine the location\\nparameter p. The rst method is referred to as monotonic\\nalignment. This straightforward method entails simply set-\\nting the location parameter equal to the location of the\\nprediction in the output sequence. Another method of deter-\\nmining the position of the region is referred to as predictive\\nalignment. As the name entails, the model attempts to\\nactually predict the location of interest in the sequence:\\np\\n11 = S\\n11  sigmoid(wT\\np\\n1dp\\n tanh( Wp\\ndpdq\\n q\\ndq1)),\\n(28)\\nwhere S R1 is the length of the input sequence, and\\nwp Rdp and Wp Rdpdq are both trainable weights\\nparameters. The sigmoid function multiplied by S makes\\nsure that p is in the range [0, S]. Additionally, in [4], it is\\nrecommended to add an additional term to the alignment\\nfunction to favor alignment around p:\\nal\\n11 = align( el\\n11; e\\nnf 1)exp((l p)2)\\n22\\n),\\n(29)\\nwhere  R1 is empirically set equal to D\\n2 according to\\n[4]. Another proposed method for compromising between\\nsoft and hard alignment is reinforced alignment [58]. Sim-\\nilarly to local alignment, a subset of the feature vectors is\\ndetermined, for which soft alignment is calculated. How-\\never, instead of using a window to determine the subset,\\nreinforced alignment uses a reinforcement learning agent\\n[57], similarly to hard alignment, to choose the subset of\\nfeature vectors. The attention calculation based on these\\nchosen feature vectors is the same as regular soft alignment.\\nSoft alignment is often regarded as the standard align-\\nment function for attention models in practically every do-\\nmain. Yet, the other alignment methods have also seen inter-\\nesting uses in various domains. For example, hard attention\\nis used in [59] for the task of visual question answering.\\nIn [60], both soft and hard attention are used in a graph\\nattention model for multi-agent game abstraction. Similarly,\\nin [61], both global and local alignment are used for review\\nrating predictions. Reinforced alignment has been employed\\n11\\nin combination with a co-attention structure in [62] for the\\ntask of aspect sentiment classication. In [63], reinforced\\nalignment is used for the task of person re-identication\\nusing surveillance images.\\n3.2.3\\nAttention Dimensionality\\nAll previous model specications of attention use a scalar\\nweight al for each value vector vl. This technique is referred\\nto as single-dimensional attention. However, instead of\\ndetermining a single attention score and weight for the\\nentire vector, [64] proposes to calculate weights for every\\nsingle feature in those vectors separately. This technique\\nis referred to as multi-dimensional attention, since the\\nattention weights now become higher dimensional vectors.\\nThe idea is that the model no longer has to attend to entire\\nvectors, but it can instead pick and choose specic elements\\nfrom those vectors. More specically, attention is calculated\\nfor each dimension. As such, the model must create a vector\\nof attention weights al Rdv for each value vector vl Rdv.\\nThe context vector can then be calculated by summing\\nthe element-wise multiplications () of the value vectors\\nv1, . . . , vnf Rdv and the corresponding attention weight\\nvectors a1, . . . , anf Rdv, as follows:\\nc\\ndv1 =\\nnf\\nX\\nl=1\\nal\\ndv1 vl\\ndv1.\\n(30)\\nHowever, since one needs to create attention weight vectors,\\nthis technique requires adjusted attention score and weight\\ncalculations. For example, the concatenate score function\\nfound in Table 2 can be adjusted by changing the w Rdw\\nweights vector to the weight matrix Wd Rdwdv:\\nel\\ndv1 = W T\\nd\\ndvdw\\n act( W1\\ndwdq\\n q\\ndq1 + W2\\ndwdk\\n kl\\ndk1 +\\nb\\ndw1). (31)\\nThis new score function produces the attention score vectors\\ne1, . . . , enf Rdv. These score vectors can be combined\\ninto a matrix of scores e = [e1, . . . , enf ] Rdvnf . To\\nproduce multi-dimensional attention weights, the alignment\\nfunction stays the same, but it is applied for each feature\\nacross the attention score columns. To illustrate, when im-\\nplementing soft attention, the attention weight produced\\nfrom the ith element of score vector el is dened as follows:\\nal,i\\n11 = align(el,i\\n11;\\ne\\ndvnf\\n) =\\nexp(el,i)\\nPnf\\nj=1 exp(ej,i),\\n(32)\\nwhere el,i represents the ith element of score vector el,\\nand al,i is the ith element of the attention weights vector\\nal. Finally, these attention weight vectors can be used to\\ncompute the context vector as presented in (30).\\nMulti-dimensional attention is a very general mecha-\\nnism that can be applied in practically every attention\\nmodel, but actual applications of the technique have been\\nrelatively sparse. One application example is [65], where\\nmulti-dimensional attention is used in a model for named\\nentity recognition based on text and visual context from\\nmultimedia posts. In [66], multi-dimensional attention is\\nused in a model for answer selection in community question\\nanswering. In [67], the U-net model for medical image seg-\\nmentation is extended with a multi-dimensional attention\\nmechanism. Similarly, in [68], the Transformer model is\\nextended with the multi-dimensional attention mechanism\\nfor the task of dialogue response generation. In [69], multi-\\ndimensional attention is used to extend graph attention\\nnetworks for dialogue state tracking. Lastly, for the task\\nof next-item recommendation, [70] proposes a model that\\nincorporates multi-dimensional attention.\\n3.3\\nQuery-Related Attention Mechanisms\\nQueries are an important part of any attention model, since\\nthey directly determine which information is extracted from\\nthe feature vectors. These queries are based on the desired\\noutput of the task model, and can be interpreted as literal\\nquestions. Some queries have specic characteristics that\\nrequire specic types of mechanisms to process them. As\\nsuch, this category encapsulates the attention mechanisms\\nthat deal with specic types of query characteristics. The\\nmechanisms in this category deal with one of the two\\nfollowing query characteristics: the type of queries or the\\nmultiplicity of queries.\\n3.3.1\\nType of Queries\\nDifferent attention models employ attention for different\\npurposes, meaning that distinct query types are necessary.\\nThere are basic queries, which are queries that are typically\\nstraightforward to dene based on the data and model. For\\nexample, the hidden state for one prediction in an RNN\\nis often used as the query for the next prediction. One\\ncould also use a vector of auxiliary variables as query. For\\nexample, when doing medical image classication, general\\npatient characteristics can be incorporated into a query.\\nSome attention mechanisms, such as co-attention, rota-\\ntory attention, and attention-over-attention, use specialized\\nqueries. For example, rotatory attention uses the context\\nvector from another attention module as query, while in-\\nteractive co-attention uses an averaged keys vector based\\non another input. Another case one can consider is when\\nattention is calculated based purely on the feature vectors.\\nThis concept has been mentioned before and is referred to\\nas self-attention or intra-attention [71]. We say that the\\nmodels use self-attentive queries. There are two ways of in-\\nterpreting such queries. Firstly, one can say that the query is\\nconstant. For example, document classication requires only\\na single classication as the output of the model. As such,\\nthe query is always the same, namely: What is the class\\nof the document?. The query can be ignored and attention\\ncan be calculated based only on the features themselves.\\nScore functions can be adjusted for this by making the query\\nvector a vector of constants or removing it entirely:\\nscore(kl\\ndk1\\n) = wT\\n1dw\\n act(W\\ndwdk\\n kl\\ndk1\\n+ b\\ndw1\\n).\\n(33)\\nAdditionally, one can also interpret self-attention as learning\\nthe query along the way, meaning that the query can be\\ndened as a trainable vector of weights. For example, the\\ndot-product score function may take the following form:\\nscore(kl\\ndk1\\n) = qT\\n1dk\\n kl\\ndk1\\n,\\n(34)\\nwhere q Rdk is a trainable vector of weights. One could\\nalso interpret vector b Rdw as the query in (33). Another\\n12\\nuse of self-attention is to uncover the relations between\\nthe feature vectors f1, . . . , fnf . These relations can then\\nbe used as additional information to incorporate into new\\nrepresentations of the feature vectors. With basic attention\\nmechanisms, the keys matrix K, and the values matrix V\\nare extracted from the features matrix F , while the query\\nq is produced separately. For this type of self-attention, the\\nquery vectors are extracted in a similar process as the keys\\nand values, via a transformation matrix of trainable weights\\nWQ Rdqdf . We dene the matrix Q = [q1, . . . , qnf ] \\nRdqnf , which can be obtained as follows:\\nQ\\ndqnf\\n= WQ\\ndqdf\\n\\nF\\ndf nf\\n.\\n(35)\\nEach column of Q can be used as the query for the atten-\\ntion model. When attention is calculated using a query q, the\\nresulting context vector c will summarize the information\\nin the feature vectors that is important to the query. Since\\nthe query, or a column of Q, is now also a feature vector\\nrepresentation, the context vector contains the information\\nof all feature vectors that are important to that specic\\nfeature vector. In other words, the context vectors capture\\nthe relations between the feature vectors. For example, self-\\nattention allows one to extract the relations between words:\\nwhich verbs refer to which nouns, which pronouns refer to\\nwhich nouns, etc. For images, self-attention can be used to\\ndetermine which image regions relate to each other.\\nWhile self-attention is placed in the query-related cat-\\negory, it is also very much related to the feature model.\\nNamely, self-attention is a technique that is often used in\\nthe feature model to create improved representations of the\\nfeature vectors. For example, the Transformer model for\\nlanguage processing [13], and the Transformer model for\\nimage processing [15], both use multiple rounds of (multi-\\nhead) self-attention to improve the representation of the\\nfeature vectors. The relations captured by the self-attention\\nmechanism are incorporated into new representations. A\\nsimple method of determining such a new representation\\nis to simply set the feature vectors equal to the acquired\\nself-attention context vectors [71], as presented in (36).\\nf (new)\\ndf 1 =\\nc\\ndf 1,\\n(36)\\nwhere f (new) is the updated feature vector. Another possi-\\nbility is to add the context vectors to the previous feature\\nvectors with an additional normalization layer [13]:\\nf (new)\\ndf 1 = Normalize(f (old)\\ndf 1 +\\nc\\ndf 1),\\n(37)\\nwhere f (old) is the previous feature vector, and Normalize()\\nis a normalization layer [72]. Using such techniques, self-\\nattention has been used to create improved word or sentence\\nembeddings that enhance model accuracy [71].\\nSelf-attention is arguably one of the more important\\ntypes of attention, partly due to its vital role in the highly\\npopular Transformer model. Self-attention is a very general\\nmechanism and can be applied to practically any problem.\\nAs such, self-attention has been extensively explored in\\nmany different elds in both Transformer-based architec-\\ntures and other types of models. For example, in [73], self-\\nattention is explored for image recognition tasks, and results\\nindicate that the technique may have substantial advantages\\nwith regards to robustness and generalization. In [74], self-\\nattention is used in a generative adversarial network (GAN)\\n[75] to determine which regions of the input image to focus\\non when generating the regions of a new image. In [76], self-\\nattention is used to design a state-of-the-art medical image\\nsegmentation model. Naturally, self-attention can also be\\nused for video processing. In [77], a self-attention model\\nis proposed for the purpose of video summarization that\\nreaches state-of-the-art results. In other elds, like audio\\nprocessing, self-attention has been explored as well. In [78],\\nself-attention is used to create a speech recognition model.\\nSelf-attention has also been explored in overlapping do-\\nmains. For example, in [79], the self-attention Transformer\\narchitecture is used to create a model that can recognize\\nphrases from audio and by lip-reading from a video. For\\nthe problem of next item recommendation, [80] proposes\\na Transformer model that explicitly captures item-item re-\\nlations using self-attention. Self-attention also has applica-\\ntions in any natural language processing elds. For example,\\nin [81], self-attention is used for sentiment analysis. Self-\\nattention is also highly popular for graph models. For\\nexample, self-attention is explored in [82] for the purpose\\nof representation learning in communication networks and\\nrating networks. Additionally, the rst attention model for\\ngraph networks was based on self-attention [83].\\n3.3.2\\nMultiplicity of Queries\\nIn previous examples, the attention model generally used\\na single query for a prediction. We say that such models\\nuse singular query attention. However, there are attention\\narchitectures that allow the model to compute attention\\nusing multiple queries. Note that this is different from, for\\nexample, an RNN that may involve multiple queries to\\nproduce a sequence of predictions. Namely, such a model\\nstill requires only a single query per prediction.\\nOne example of a technique that incorporates multiple\\nqueries is multi-head attention [13], as presented in Fig.\\n9. Multi-head attention works by implementing multiple\\nattention modules in parallel by utilizing multiple different\\nversions of the same query. The idea is to linearly transform\\nthe query q using different weight matrices. Each newly\\nformed query essentially asks for a different type of relevant\\ninformation, allowing the attention model to introduce more\\ninformation into the context vector calculation. An attention\\nmodel implements d 1 heads with each attention head\\nhaving its own query vector, keys matrix, and values matrix:\\nq(j), K(j) and V (j), for j = 1, . . . , d. The query q(j) is\\nobtained by linearly transforming the original query q,\\nwhile the matrices K(j) and V (j) are obtained through\\nlinear transformations of F . As such, each attention head\\nhas its own learnable weights matrices W (j)\\nq\\n, W (j)\\nK\\nand\\nW (j)\\nV\\nfor these transformations. The calculation of the query,\\nkeys, and values for the jth head are dened as follows:\\nq(j)\\ndq1 = W (j)\\nq\\ndqdq\\n q\\ndq1,\\nK(j)\\ndknf\\n= W (j)\\nK\\ndkdf\\n\\nF\\ndf nf\\n,\\nV (j)\\ndvnf\\n= W (j)\\nV\\ndvdf\\n\\nF\\ndf nf\\n.\\n(38)\\nThus, each head creates its own representations of the\\nquery q, and the input matrix F . Each head can therefore\\n13\\nAttention\\nHead1\\nAttention\\nHead2\\nAttention \\nHeadd-1\\nAttention\\nHeadd\\nFig. 9. An illustration of multi-head attention.\\nlearn to focus on different parts of the inputs, allowing\\nthe model to attend to more information. For example,\\nwhen training a machine translation model, one attention\\nhead can learn to focus on which nouns (e.g., student, car,\\napple) do certain verbs (e.g., walking, driving, buying) refer\\nto, while another attention head learns to focus on which\\nnouns refer to certain pronouns (e.g., he, she, it) [13]. Each\\nhead will also create its own vector of attention scores\\ne(j) = [e(j)\\n1 , . . . , e(j)\\nnf ] Rnf , and a corresponding vector\\nof attention weights a(j) = [a(j)\\n1 , . . . , a(j)\\nnf ] Rnf . As can\\nbe expected, each attention model produces its own context\\nvector c(j) Rdv, as follows:\\nc(j)\\ndv1 =\\nnf\\nX\\nl=1\\na(j)\\nl\\n11  v(j)\\nl\\ndv1.\\n(39)\\nThe goal is still to create a single context vector as output\\nof the attention model. As such, the context vectors pro-\\nduced by the individual attention heads are concatenated\\ninto a single vector. Afterwards, a linear transformation is\\napplied using the weight matrix WO Rdcdvd to make\\nsure the resulting context vector c Rdc has the desired\\ndimension. This calculation is presented in (40). The dimen-\\nsion dc can be pre-specied by, for example, setting it equal\\nto dv, so that the context vector dimension is unchanged.\\nc\\ndc1 = WO\\ndcdvd  concat(c(1)\\ndv1, ..., c(d)\\ndv1).\\n(40)\\nMulti-head attention processes multiple attention mod-\\nules in parallel, but attention modules can also be imple-\\nmented sequentially to iteratively adjust the context vec-\\ntors. Each of these attention modules are referred to as\\nrepetitions or rounds of attention. Such attention ar-\\nchitectures are referred to as multi-hop attention models,\\nalso known as multi-step attention models. An impor-\\ntant note to consider is the fact that multi-hop attention\\nis a mechanism that has been proposed in various forms\\nthroughout various works. While the mechanism always\\ninvolves multiple rounds of attention, the multi-hop im-\\nplementation proposed in [84] differs from the mechanism\\nproposed in [85] or [86]. Another interesting example is\\n[87], where a multi-hop attention model is proposed that\\nwould actually be considered alternating co-attention in this\\nsurvey, as explained in Subsection 3.1.1.\\nWe present a general form of multi-hop attention that\\nis largely a generalization of the techniques introduced in\\n[85] and [88]. Fig. 10 provides an example implementation\\nof a multi-hop attention mechanism. The general idea is\\nto iteratively transform the query, and use the query to\\ntransform the context vector, such that the model can extract\\ndifferent information in each step. Remember that a query\\nAttention\\nModule\\nAttention\\nModule\\nQuery\\nModel\\nQuery\\nModel\\nAttention\\nModule\\nQuery\\nModel\\nFig. 10. An example illustration of multi-hop attention. Solid arrows\\nrepresent the base multi-hop model structure, while dotted arrows rep-\\nresent optional connections.\\nis similar to a literal question. As such, one can interpret\\nthe transformed queries as asking the same question in a\\ndifferent manner or from a different perspective, similarly\\nto the queries in multi-head attention. The query that was\\npreviously denoted by q is now referred to as the initial\\nquery, and is denoted by q(0). At hop s, the current query\\nq(s) is transformed into a new query representation q(s+1),\\npossibly using the current context vector c(s) as another\\ninput, and some transformation function transform():\\nq(s+1)\\ndq1\\n= transform(q(s)\\ndq1\\n, c(s)\\ndv1\\n).\\n(41)\\nFor the\\nspecic form of\\nthe transformation function\\ntransform(), [85] proposes to use a mechanism similar to\\nself-attention. Essentially, the queries used by the question\\nanswer matching model proposed in [85] were originally\\nbased on a set of feature vectors extracted from a question.\\n[85] also denes the original query q(0) as the unweighted\\naverage of these feature vectors. At each hop s, attention\\ncan be calculated on these feature vectors using the previous\\nquery q(s) as the query in this process. The resulting context\\nvector of this calculation is the next query vector. Using\\nthe context vector c(s) instead of q(s) as the query for this\\nprocess is also a possibility, which is similar to the LCR-\\nRot-hop model proposed in [43] and the multi-step model\\nproposed in [88]. Such a connection is represented by the\\ndotted arrows in Fig. 10. The transformation mechanism\\nuses either the q(s) or the context vector c(s) as query, but a\\ncombination via concatenation is also possible.\\nEach query representation is used as input for the at-\\ntention module to compute attention on the columns of the\\nfeature matrix F , as seen previously. One main difference,\\nhowever, is that the context vector c(s) is also used as input,\\nso that the actual query input for the attention model is\\nthe concatenation of c(s) and q(s+1). The adjusted attention\\nscore function is presented in (42). Note that the initial\\ncontext vector c(0) is predened. One way of doing this is\\nby setting it equal to the unweighted average of the value\\nvectors v1, . . . , vnf Rdv extracted from F .\\ne(s)\\nl\\n11 = score(concat(q(s+1)\\ndq1 , c(s)\\ndv1), kl\\ndk1).\\n(42)\\nAn alignment function and the value vectors are then used\\nto produce the next context vector c(s+1). One must note\\nthat in [85], the weights used in each iteration are the same\\nweights, meaning that the number of parameters do not\\nscale with the number of repetitions. Yet, using multiple\\nhops with different weight matrices can also be viable, as\\nshown by the Transformer model [13] and in [88]. It may be\\n14\\ndifcult to grasp why c(s) is part of the query input for the\\nattention model. Essentially, this technique is closely related\\nto self-attention in the sense that, in each iteration, a new\\ncontext representation is created from the feature vectors\\nand the context vector. The essence of this mechanism is\\nthat one wants to iteratively alter the query and the context\\nvector, while attending to the feature vectors. In the process,\\nthe new representations of the context vector absorb more\\ndifferent kinds of information. This is also the main differ-\\nence between this type of attention and multi-head atten-\\ntion. Multi-head attention creates multiple context vectors\\nfrom multiple queries and combines them to create a nal\\ncontext vector as output. Multi-hop attention iteratively\\nrenes the context vector by incorporating information from\\nthe different queries. This does have the disadvantage of\\nhaving to calculate attention sequentially.\\nInterestingly, due to the variations in which multi-hop\\nattention has been proposed, some consider the Transformer\\nmodels encoder and decoder to consist of several single-\\nhop attention mechanisms [84] instead of being a multi-\\nhop model. However, in the context of this survey, we\\nconsider the Transformer model to be an alternative form\\nof the multi-hop mechanism, as the features matrix F is not\\ndirectly reused in each step. Instead, F is only used as an\\ninput for the rst hop, and is transformed via self-attention\\ninto a new representation. The self-attention mechanism\\nuses each feature vector in F as a query, resulting in a matrix\\nof context vectors as output of each attention hop. The\\nintermediate context vectors are turned into matrices and\\nrepresent iterative transformations of the matrix F , which\\nare used in the consecutive steps. Thus, the Transformer\\nmodel iteratively renes the features matrix F by extracting\\nand incorporating new information.\\nWhen dealing with a classication task, another idea is\\nto use a different query for each class. This is the basic\\nprinciple behind capsule-based attention [89], as inspired\\nby the capsule networks [90]. Suppose we have the feature\\nvectors f1, . . . , fnf Rdf , and suppose there are are dy\\nclasses that the model can predict. Then, a capsule-based\\nattention model denes a capsule for each of the dy classes\\nthat each take as input the feature vectors. Each capsule\\nconsists of, in order, an attention module, a probability\\nmodule, and a reconstruction module, which are depicted in\\nFig. 11. The attention modules all use self-attentive queries,\\nso each module learns its own query: Which feature vectors\\nare important to identify this class?. In [89], a self-attentive\\nmultiplicative score function is used for this purpose:\\nec,l\\n11\\n= qT\\nc\\n1dk\\n kl\\ndk1\\n,\\n(43)\\nwhere ec,l R1 is the attention score for vector l in capsule\\nc, and qc Rdk is a trainable query for capsule c, for\\nc = 1, . . . , dy. Each attention module then uses an alignment\\nfunction, and uses the produced attention weights to deter-\\nmine a context vector cc Rdv. Next, the context vector\\ncc is fed through a probability layer consisting of a linear\\ntransformation with a sigmoid activation function:\\npc\\n11\\n= sigmoid(wT\\nc\\n1dv\\n cc\\ndv1\\n+ bc\\n11\\n),\\n(44)\\n\\x97\\nAttention\\nModule1\\nAttention\\nModule2\\nAttention\\xa0\\nModuledy-1\\nAttention\\nModuledy\\n\\nz\\x8e\\n\\n1\\nz\\x8e\\n2\\n1\\nProbability\\nModule1\\nProbability\\nModule2\\nProbability\\nModuledy-1\\nProbability\\nModuledy\\nReconstruction\\nModule1\\nReconstruction\\nModule2\\n\\x851\\n1\\n2\\n\\x852\\nReconstruction\\nModuledy-1\\n\\n1\\nz\\x8e\\n\\x85\\n1\\nz\\x8e\\nReconstruction\\nModuledy\\nz\\x8e\\n\\x85z\\x8e\\nFig. 11. An illustration of capsule-based attention.\\nwhere wc Rdv and bc R1 are trainable capsule-specic\\nweights parameters, and pc R1 is the predicted probability\\nthat the correct class is class c. The nal layer is the recon-\\nstruction module that creates a class vector representation.\\nThis representation rc Rdv is determined by simply\\nmultiplying the context vector cc by the probability pc:\\nrc\\ndv1\\n= pc\\n11\\n cc\\ndv1\\n.\\n(45)\\nThe capsule representation is used when training the model.\\nFirst of all, the model is trained to predict the probabilities\\np1, . . . , pdy as accurately as possible compared to the true\\nvalues. Secondly, via a joint loss function, the model is also\\ntrained to accurately construct the capsule representations\\nr1, . . . , rdy. A features representation f Rdf is dened\\nwhich is simply the unweighted average of the original fea-\\nture vectors. The idea is to train the model such that vector\\nrepresentations from capsules that are not the correct class\\ndiffer signicantly from f while the representation from the\\ncorrect capsule is very similar to f. A dot-product between\\nthe capsule representations and the features representation\\nis used in [89] as a measure of the distance between the vec-\\ntors. Note that dv must equal df in this case, otherwise the\\nvectors would have incompatible dimensions. Interestingly,\\nsince attention is calculated for each class individually, one\\ncan track which specic feature vectors are important for\\nwhich specic class. In [89], this idea is used to discover\\nwhich words correspond to which sentiment class.\\nThe number of tasks that can make use of multiple\\nqueries is substantial, due to how general the mechanisms\\nare. As such, the techniques described in this section have\\nbeen extensively explored in various domains. For exam-\\nple, multi-head attention has been used for speaker recog-\\nnition based on audio spectrograms [91]. In [92], multi-\\nhead attention is used for recommendation of news arti-\\ncles. Additionally, multi-head attention can be benecial\\nfor graph attention models as well [83]. As for multi-hop\\nattention, quite a few papers have been mentioned before,\\nbut there are still many other interesting examples. For\\nexample, in [93], a multi-hop attention model is proposed\\nfor medication recommendation. Furthermore, practically\\nevery Transformer model makes use of both multi-head\\nand multi-hop attention. The Transformer model has been\\nextensively explored in various domains. For example, in\\n[94], a Transformer model is implemented for image cap-\\n15\\ntioning. In [95], Transformers are explored for medical im-\\nage segmentation. In [96], a Transformer model is used for\\nemotion recognition in text messages. A last example of\\nan application of Transformers is [17], which proposes a\\nTransformer model for recommender systems. In compari-\\nson with multi-head and multi-hop attention, capsule-based\\nattention is arguably the least popular of the mechanisms\\ndiscussed for the multiplicity of queries. One example is\\n[97], where an attention-based capsule network is proposed\\nthat also includes a multi-hop attention mechanism for the\\npurpose of visual question answering. Another example is\\n[98], where capsule-based attention is used for aspect-level\\nsentiment analysis of restaurant reviews.\\nThe multiplicity of queries is a particularly interesting\\ncategory due to the Transformer model [13], which com-\\nbines a form of multi-hop and multi-head attention. Due\\nto the initial success of the Transformer model, many im-\\nprovements and iterations of the model have been produced\\nthat typically aim to improve the predictive performance,\\nthe computational efciency, or both. For example, the\\nTransformer-XL [99] is an extension of the original Trans-\\nformer that uses a recurrence mechanism to not be limited\\nby a context window when processing the outputs. This\\nallows the model to learn signicantly longer dependencies\\nwhile also being computationally more efcient during the\\nevaluation phase. Another extension of the Transformer is\\nknown as the Reformer model [100]. This model is signi-\\ncantly more efcient computationally, by means of locality-\\nsensitive hashing, and memory-wise, by means of reversible\\nresidual layers. Such computational improvements are vital,\\nsince one of the main disadvantages of the Transformer\\nmodel is the sheer computational cost due to the complexity\\nof the model scaling quadratically with the amount of input\\nfeature vectors. The Linformer model [101] manages to\\nreduce the complexity of the model to scale linearly, while\\nachieving similar performance as the Transformer model.\\nThis is achieved by approximating the attention weights\\nusing a low-rank matrix. The Lite-Transformer model pro-\\nposed in [102] achieves similar results by implementing\\ntwo branches within the Transformer block that specialize\\nin capturing global and local information. Another inter-\\nesting Transformer architecture is the Synthesizer [103].\\nThis model replaces the pairwise self-attention mechanism\\nwith synthetic attention weights. Interestingly, the per-\\nformance of this model is relatively close to the original\\nTransformer, meaning that the necessity of the pairwise\\nself-attention mechanism of the Transformer model may\\nbe questionable. For a more comprehensive overview of\\nTransformer architectures, we refer to [104].\\n4\\nEVALUATION OF ATTENTION MODELS\\nIn this section, we present various types of evaluation for\\nattention models. Firstly, one can evaluate the structure of\\nattention models using the taxonomy presented in Section 3.\\nFor such an analysis, we consider the attention mechanism\\ncategories (see Fig. 3) as orthogonal dimensions of a model.\\nThe structure of a model can be analyzed by determining\\nwhich mechanism a model uses for each category. Table 3\\nprovides an overview of attention models found in the liter-\\nature with a corresponding analysis based on the attention\\nmechanisms the models implement.\\nSecondly, we discuss various techniques for evaluating\\nthe performance of attention models. The performance of at-\\ntention models can be evaluated using extrinsic or intrinsic\\nperformance measures, which are discussed in Subsections\\n4.1 and 4.2, respectively.\\n4.1\\nExtrinsic Evaluation\\nIn general, the performance of an attention model is mea-\\nsured using extrinsic performance measures. For example,\\nperformance measures typically used in the eld of natural\\nlanguage processing are the BLEU [107], METEOR [108],\\nand Perplexity [109] metrics. In the eld of audio processing,\\nthe Word Error Rate [110] and Phoneme Error Rate [111] are\\ngenerally employed. For general classication tasks, error\\nrates, precision, and recall are generally used. For computer\\nvision tasks, the PSNR [112], SSIM [113], or IoU [114] metrics\\nare used. Using these performance measures, an attention\\nmodel can either be compared to other state-of-the-art mod-\\nels, or an ablation study can be performed. If possible, the\\nimportance of the attention mechanism can be tested by\\nreplacing it with another mechanism and observing whether\\nthe overall performance of the model decreases [105], [115].\\nAn example of this is replacing the weighted average used\\nto produce the context vector with a simple unweighted\\naverage and observing whether there is a decrease in over-\\nall model performance [35]. This ablation method can be\\nused to evaluate whether the attention weights can actually\\ndistinguish important from irrelevant information.\\n4.2\\nIntrinsic Evaluation\\nAttention models can also be evaluated using attention-\\nspecic intrinsic performance measures. In [4], the at-\\ntention weights are formally evaluated via the Alignment\\nError Rate (AER) to measure the accuracy of the attention\\nweights with respect to annotated attention vectors. [116]\\nincorporates this idea into an attention model by supervis-\\ning the attention mechanism using gold attention vectors.\\nA joint loss function consisting of the regular task-specic\\nloss and the attention weights loss function is constructed\\nfor this purpose. The gold attention vectors are based on\\nannotated text data sets where keywords are hand-labelled.\\nHowever, since attention is inspired by human attention,\\none could evaluate attention models by comparing them to\\nthe attention behaviour of humans.\\n4.2.1\\nEvaluation via Human Attention\\nIn [117], the concept of attention correctness is proposed,\\nwhich is a quantitative intrinsic performance metric that\\nevaluates the quality of the attention mechanism based on\\nactual human attention behaviour. Firstly, the calculation\\nof this metric requires data that includes the attention be-\\nhaviour of a human. For example, a data set containing im-\\nages with the corresponding regions that a human focuses\\non when performing a certain task, such as image caption-\\ning. The collection of regions focused on by the human is\\nreferred to as the ground truth region. Suppose an attention\\nmodel attends to the nf feature vectors f1, . . . , fnf Rdf .\\nFeature vector fi corresponds to region Ri of the given\\n16\\nTABLE 3\\nAttention models analyzed based on the proposed taxonomy. A plus sign (+) between two mechanisms indicates that both techniques were\\ncombined in the same model, while a comma (,) indicates that both mechanisms were tested in the same paper, but not necessarily as a\\ncombination in the same model.\\nFeature-Related\\nGeneral\\nQuery-Related\\nMultiplicity\\nLevels\\nRepresentations\\nScoring\\nAlignment\\nDimensionality\\nType\\nMultiplicity\\nBahdanau et al. [3]\\nSingular\\nSingle-Level\\nSingle-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nBasic\\nSingular\\nLuong et al. [4]\\nSingular\\nSingle-Level\\nSingle-Representational\\nMultiplicative,\\nLocation\\nGlobal, Local\\nSingle-Dimensional\\nBasic\\nSingular\\nXu et al. [8]\\nSingular\\nSingle-Level\\nSingle-Representational\\nAdditive\\nSoft, Hard\\nSingle-Dimensional\\nBasic\\nSingular\\nLu et al. [32]\\nParallel\\nCo-attention\\nHierarchical\\nSingle-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nSpecialized\\nSingular\\nYang et al. [5]\\nSingular\\nHierarchical\\nSingle-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nSingular\\nLi et al. [47]\\nSingular\\nHierarchical\\nSingle-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nSingular\\nVaswani et al. [13]\\nSingular\\nSingle-Level\\nSingle-Representational\\nScaled-\\nMultiplicative\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive +\\nBasic\\nMulti-Head +\\nMulti-Hop\\nWallaart and Frasincar [43]\\nRotatory\\nSingle-Level\\nSingle-Representational\\nActivated\\nGeneral\\nGlobal\\nSingle-Dimensional\\nSpecialized\\nMulti-Hop\\nKiela et al. [50]\\nSingular\\nSingle-Level\\nMulti-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nSingular\\nShen et al. [64]\\nSingular\\nSingle-Level\\nSingle-Representational\\nAdditive\\nGlobal\\nMulti-Dimensional\\nSelf-Attentive\\nSingular\\nZhang et al. [74]\\nSingular\\nSingle-Level\\nSingle-Representational\\nMultiplicative\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nSingular\\nLi et al. [105]\\nParallel\\nCo-attention\\nSingle-Level\\nSingle-Representational\\nScaled-\\nMultiplicative\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive +\\nSpecialized\\nSingular\\nYu et al. [106]\\nParallel\\nCo-attention\\nSingle-Level\\nSingle-Representational\\nMultiplicative\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive +\\nSpecialized\\nMulti-Head\\nWang et al. [62]\\nParallel\\nCo-attention\\nSingle-Level\\nSingle-Representational\\nAdditive\\nReinforced\\nSingle-Dimensional\\nSpecialized\\nSingular\\nOktay et al. [67]\\nSingular\\nSingle-Level\\nSingle-Representational\\nAdditive\\nGlobal\\nMulti-Dimensional\\nSelf-Attentive +\\nSpecialized\\nSingular\\nWinata et al. [52]\\nSingular\\nSingle-Level\\nMulti-Representational\\nAdditive\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nMulti-Head\\nWang et al. [89]\\nSingular\\nSingle-Level\\nSingle-Representational\\nMultiplicative\\nGlobal\\nSingle-Dimensional\\nSelf-Attentive\\nCapsule-Based\\nimage, for i = 1, . . . , nf. We dene the set G as the set\\nof regions that belong to the ground truth region, such that\\nRi G if Ri is part of the ground truth region. The attention\\nmodel calculates the attention weights a1, . . . , anf R1 via\\nthe usual attention process. The Attention Correctness (AC)\\nmetric can then be calculated using (46).\\nAC\\n11 =\\nX\\ni:RiG\\nai\\n11.\\n(46)\\nThus, this metric is equal to the sum of the attention weights\\nfor the ground truth regions. Since the attention weights\\nsum up to 1 due to, for example, a softmax alignment\\nfunction, the AC value will be a value between 0 and 1.\\nIf the model attends to only the ground truth regions, then\\nAC is equal to 1, and if the attention model does not attend\\nto any of the ground truth regions, AC will be equal to 0.\\nIn [118], a rank correlation metric is used to compare\\nthe generated attention weights to the attention behaviour\\nof humans. The conclusion of this work is that attention\\nmaps generated by standard attention models generally do\\nnot correspond to human attention. Attention models often\\nfocus on much larger regions or multiple small non-adjacent\\nregions. As such, a technique to improve attention models is\\nto allow the model to learn from human attention patterns\\nvia a joint loss of the regular loss function and an attention\\nweight loss function based on the human gaze behaviour,\\nsimilarly to how annotated attention vectors are used in\\n[116] to supervise the attention mechanism. [117] proposes\\nto use human attention data to supervise the attention\\nmechanism in such a manner. Similarly, a state-of-the-art\\nvideo captioning model is proposed in [119] that learns from\\nhuman gaze data to improve the attention mechanism.\\n4.2.2\\nManual Evaluation\\nA method that is often used to evaluate attention models is\\nthe manual inspection of attention weights. As previously\\nmentioned, the attention weights are a direct indication of\\nwhich parts of the data the attention model nds most\\nimportant. Therefore, observing which parts of the inputs\\nthe model focuses on can be helpful in determining if the\\nmodel is behaving correctly. This allows for some interpre-\\ntation of the behaviour of models that are typically known\\nto be black boxes. However, rather than checking if the\\nmodel focuses on the most important parts of the data, some\\nuse the attention weights to determine which parts of the\\ndata are most important. This would imply that attention\\nmodels provide a type of explanation, which is a subject\\nof contention among researchers. Particularly, in [120], ex-\\ntensive experiments are conducted for various natural lan-\\nguage processing tasks to investigate the relation between\\nattention weights and important information to determine\\nwhether attention can actually provide meaningful expla-\\nnations. In this paper titled Attention is not Explanation,\\nit is found that attention weights do not tend to correlate\\nwith important features. Additionally, the authors are able\\nto replace the produced attention weights with completely\\ndifferent values while keeping the model output the same.\\nThese so-called adversarial attention distributions show\\nthat an attention model may focus on completely different\\ninformation and still come to the same conclusions, which\\nmakes interpretation difcult. Yet, in another paper titled\\nAttention is not not Explanation [121], the claim that\\nattention is not explanation is questioned by challenging\\nthe assumptions of the previous work. It is found that\\nthe adversarial attention distributions do not perform as\\nreliably well as the learned attention weights, indicating that\\nit was not proved that attention is not viable for explanation.\\nIn general, the conclusion regarding the interpretability\\nof attention models is that researchers must be extremely\\ncareful when drawing conclusions based on attention pat-\\nterns. For example, problems with an attention model can be\\ndiagnosed via the attention weights if the model is found to\\nfocus on the incorrect parts of the data, if such information\\nis available. Yet, conversely, attention weights may only be\\nused to obtain plausible explanations for why certain parts\\nof the data are focused on, rather than concluding that those\\n17\\nparts are signicant to the problem [121]. However, one\\nshould still be cautious as the viability of such approaches\\ncan depend on the model architecture [122].\\n5\\nCONCLUSION\\nIn this survey, we have provided an overview of recent\\nresearch on attention models in deep learning. Attention\\nmechanisms have been a prominent development for deep\\nlearning models as they have shown to improve model per-\\nformance signicantly, producing state-of-the-art results for\\nvarious tasks in several elds of research. We have presented\\na comprehensive taxonomy that can be used to categorize\\nand explain the diverse number of attention mechanisms\\nproposed in the literature. The organization of the taxonomy\\nwas motivated based on the structure of a task model that\\nconsists of a feature model, an attention model, a query\\nmodel, and an output model. Furthermore, the attention\\nmechanisms have been discussed using a framework based\\non queries, keys, and values. Last, we have shown how\\none can use extrinsic and intrinsic measures to evaluate the\\nperformance of attention models, and how one can use the\\ntaxonomy to analyze the structure of attention models.\\nThe attention mechanism is typically relatively simple\\nto understand and implement and can lead to signicant\\nimprovements in performance. As such, it is no surprise that\\nthis is a highly active eld of research with new attention\\nmechanisms and models being developed constantly. Not\\nonly are new mechanisms consistently being developed,\\nbut there is also still ample opportunity for the exploration\\nof existing mechanisms for new tasks. For example, multi-\\ndimensional attention [64] is a technique that shows promis-\\ning results and is general enough to be implemented in\\nalmost any attention model. However, it has not seen much\\napplication in current works. Similarly, multi-head atten-\\ntion [13] is a technique that can be efciently parallelized\\nand implemented in practically any attention model. Yet,\\nit is mostly seen only in Transformer-based architectures.\\nLastly, similarly to how [43] combines rotatory attention\\nwith multi-hop attention, combining multi-dimensional at-\\ntention, multi-head attention, capsule-based attention, or\\nany of the other mechanisms presented in this survey may\\nproduce new state-of-the-art results for the various elds of\\nresearch mentioned in this survey.\\nThis survey has mainly focused on attention mech-\\nanisms for supervised models, since these comprise the\\nlargest proportion of the attention models in the literature.\\nIn comparison to the total amount of research that has been\\ndone on attention models, research on attention models\\nfor semi-supervised learning [123], [124] or unsupervised\\nlearning [125], [126] has received limited attention and has\\nonly become active recently. Attention may play a more\\nsignicant role for such tasks in the future as obtaining\\nlarge amounts of labeled data is a difcult task. Yet, as\\nlarger and more detailed data sets become available, the\\nresearch on attention models can advance even further. For\\nexample, we mentioned the fact that attention weights can\\nbe trained directly based on hand-annotated data [116] or\\nactual human attention behaviour [117], [119]. As new data\\nsets are released, future research may focus on developing\\nattention models that can incorporate those types of data.\\nWhile attention is intuitively easy to understand, there\\nstill is a substantial lack of theoretical support for attention.\\nAs such, we expect more theoretical studies to additionally\\ncontribute to the understanding of the attention mecha-\\nnisms in complex deep learning systems. Nevertheless, the\\npractical advantages of attention models are clear. Since\\nattention models provide signicant performance improve-\\nments in a variety of elds, and as there are ample opportu-\\nnities for more advancements, we foresee that these models\\nwill still receive signicant attention in the time to come.\\nREFERENCES\\n[1]\\nH. Larochelle and G. E. Hinton, Learning to combine foveal\\nglimpses with a third-order Boltzmann machine, in 24th Annual\\nConference in Neural Information Processing Systems (NIPS 2010).\\nCurran Associates, Inc., 2010, pp. 12431251.\\n[2]\\nV. Mnih, N. Heess, A. Graves, and k. kavukcuoglu, Recurrent\\nmodels of visual attention, in 27th Annual Conference on Neural\\nInformation Processing Systems (NIPS 2014).\\nCurran Associates,\\nInc., 2014, pp. 22042212.\\n[3]\\nD. Bahdanau, K. Cho, and Y. Bengio, Neural machine translation\\nby jointly learning to align and translate, in 3rd International\\nConference on Learning Representation (ICLR 2015), 2015.\\n[4]\\nT. Luong, H. Pham, and C. D. Manning, Effective approaches to\\nattention-based neural machine translation, in 2015 Conference\\non Empirical Methods in Natural Language Processing (EMNLP\\n2015).\\nACL, 2015, pp. 14121421.\\n[5]\\nZ. Yang, D. Yang, C. Dyer, X. He, A. Smola, and E. Hovy,\\nHierarchical attention networks for document classication, in\\n2016 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies (NAACL-\\nHLT 2016).\\nACL, 2016, pp. 14801489.\\n[6]\\nY. Wang, M. Huang, X. Zhu, and L. Zhao, Attention-based LSTM\\nfor aspect-level sentiment classication, in 2016 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP 2016).\\nACL, 2016, pp. 606615.\\n[7]\\nP. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould,\\nand L. Zhang, Bottom-up and top-down attention for image\\ncaptioning and visual question answering, in 2018 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR\\n2018), 2018, pp. 60776086.\\n[8]\\nK. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\\nR. Zemel, and Y. Bengio, Show, attend and tell: Neural image\\ncaption generation with visual attention, in 32nd International\\nConference on Machine Learning (ICML 2015), vol. 37.\\nPMLR,\\n2015, pp. 20482057.\\n[9]\\nY. Ma, H. Peng, and E. Cambria, Targeted aspect-based senti-\\nment analysis via embedding commonsense knowledge into an\\nattentive LSTM, in 32nd AAAI Conference on Articial Intelligence\\n(AAAI 2018).\\nAAAI Press, 2018, pp. 58765883.\\n[10]\\nJ. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio,\\nAttention-based models for speech recognition, in 28th Annual\\nConference on Neural Information Processing Systems (NIPS 2015).\\nCurran Associates, Inc., 2015, pp. 577585.\\n[11]\\nD. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio,\\nEnd-to-end attention-based large vocabulary speech recogni-\\ntion, in 2016 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP 2016).\\nIEEE Signal Processing Society,\\n2016, pp. 49454949.\\n[12]\\nS. Kim, T. Hori, and S. Watanabe, Joint CTC-attention based\\nend-to-end speech recognition using multi-task learning, in\\n2017 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP 2017).\\nIEEE Signal Processing Society, 2017,\\npp. 48354839.\\n[13]\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\\nGomez, L. u. Kaiser, and I. Polosukhin, Attention is all you\\nneed, in 31st Annual Conference on Neural Information Processing\\nSystems (NIPS 2017).\\nCurran Associates, Inc., 2017, pp. 5998\\n6008.\\n[14]\\nK. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, On\\nthe properties of neural machine translation: Encoderdecoder\\napproaches, in 8th Workshop on Syntax, Semantics and Structure\\nin Statistical Translation (SSST 2014).\\nACL, 2014, pp. 103111.\\n18\\n[15]\\nN. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku,\\nand D. Tran, Image Transformer, in 35th International Conference\\non Machine Learning (ICML 2018), vol. 80. PMLR, 2018, pp. 4055\\n4064.\\n[16]\\nL. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong, End-to-\\nend dense video captioning with masked transformer, in 2018\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\\n(CVPR 2018).\\nIEEE Computer Society, 2018, pp. 87398748.\\n[17]\\nF. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang,\\nBERT4Rec: Sequential recommendation with bidirectional en-\\ncoder representations from transformer, in 28th ACM Interna-\\ntional Conference on Information and Knowledge Management (CIKM\\n2019).\\nACM, 2019, p. 14411450.\\n[18]\\nF. Wang and D. M. J. Tax, Survey on the attention based RNN\\nmodel and its applications in computer vision, arXiv:1601.06823,\\n2016.\\n[19]\\nJ. B. Lee, R. A. Rossi, S. Kim, N. K. Ahmed, and E. Koh, Attention\\nmodels in graphs: A survey, ACM Transitions on Knowledge\\nDiscovery from Data, vol. 13, pp. 62:162:25, 2019.\\n[20]\\nS. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath, An\\nattentive survey of attention models, ACM Transactions on In-\\ntelligent Systems and Technology, vol. 12, no. 5, pp. 132, 2021.\\n[21]\\nD. Hu, An introductory survey on attention mechanisms in NLP\\nproblems, in Proceedings of the 2019 Intelligent Systems Conference\\n(IntelliSys 2019), ser. AISC, vol. 1038. Springer, 2020, pp. 432448.\\n[22]\\nA. Galassi, M. Lippi, and P. Torroni, Attention, please! a critical\\nreview of neural attention models in natural language process-\\ning, arXiv:1902.02181, 2019.\\n[23]\\nM. Daniluk, T. Rocktaschel, J. Welbl, and S. Riedel, Frustratingly\\nshort attention spans in neural language modeling, in 5th Inter-\\nnational Conference on Learning Representations (ICLR 2017), 2017.\\n[24]\\nY. Xu, Q. Kong, Q. Huang, W. Wang, and M. D. Plumbley, At-\\ntention and localization based on a deep convolutional recurrent\\nmodel for weakly supervised audio tagging, in Proceedings of the\\n18th Annual Conference of the International Speech Communication\\nAssociation (Interspeech 2017).\\nISCA, 2017, pp. 30833087.\\n[25]\\nC. Yu, K. S. Barsim, Q. Kong, and B. Yang, Multi-level attention\\nmodel for weakly supervised audio classication, in Proceedings\\nof the Detection and Classication of Acoustic Scenes and Events 2018\\nWorkshop (DCASE 2018), 2018, pp. 188192.\\n[26]\\nS. Sharma, R. Kiros, and R. Salakhutdinov, Action recognition\\nusing visual attention, in Proceedings of the 4th International\\nConference on Learning Representations Workshop (ICLR 2016), 2016.\\n[27]\\nL. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, Video caption-\\ning with attention-based LSTM and semantic consistency, IEEE\\nTransactions on Multimedia, vol. 19, no. 9, pp. 20452055, 2017.\\n[28]\\nH. Ying, F. Zhuang, F. Zhang, Y. Liu, G. Xu, X. Xie, H. Xiong,\\nand J. Wu, Sequential recommender system based on hierarchi-\\ncal attention networks, in 27th International Joint Conference on\\nArticial Intelligence (IJCAI 2018).\\nIJCAI, 2018, pp. 39263932.\\n[29]\\nH. Song, D. Rajan, J. Thiagarajan, and A. Spanias, Attend and\\ndiagnose: Clinical time series analysis using attention models, in\\n32nd AAAI Conference on Articial Intelligence (AAAI 2018). AAAI\\nPress, 2018, pp. 40914098.\\n[30]\\nD. T. Tran, A. Iosidis, J. Kanniainen, and M. Gabbouj, Temporal\\nattention-augmented bilinear network for nancial time-series\\ndata analysis, IEEE Transactions on Neural Networks and Learning\\nSystems, vol. 30, no. 5, pp. 14071418, 2019.\\n[31]\\nP. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio,\\nand Y. Bengio, Graph attention networks, in 6th International\\nConference on Learning Representations (ICLR 2018), 2018.\\n[32]\\nJ. Lu, J. Yang, D. Batra, and D. Parikh, Hierarchical question-\\nimage co-attention for visual question answering, in 30th Annual\\nConference on Neural Information Processing Systems (NIPS 2016).\\nCurran Associates, Inc., 2016, pp. 289297.\\n[33]\\nF. Fan, Y. Feng, and D. Zhao, Multi-grained attention network\\nfor aspect-level sentiment classication, in 2018 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP 2018).\\nACL, 2018, pp. 34333442.\\n[34]\\nD. Ma, S. Li, X. Zhang, and H. Wang, Interactive attention\\nnetworks for aspect-level sentiment classication, in 26th In-\\nternational Joint Conference on Articial Intelligence (IJCAI 2017).\\nIJCAI, 2017, pp. 40684074.\\n[35]\\nM. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi, Bidirec-\\ntional attention ow for machine comprehension, in 4th Interna-\\ntional Conference on Learning Representations (ICLR 2016), 2016.\\n[36]\\nS. Zheng and R. Xia, Left-center-right separated neural network\\nfor aspect-based sentiment analysis with rotatory attention,\\narXiv:1802.00892, 2018.\\n[37]\\nB. Jing, P. Xie, and E. Xing, On the automatic generation of med-\\nical imaging reports, in 56th Annual Meeting of the Association for\\nComputational Linguistics (ACL 2018).\\nACL, 2018, pp. 25772586.\\n[38]\\nJ. Gao, X. Wang, Y. Wang, Z. Yang, J. Gao, J. Wang, W. Tang,\\nand X. Xie, CAMP: Co-attention memory networks for diagnosis\\nprediction in healthcare, in 2019 IEEE International Conference on\\nData Mining (ICDM 2019).\\nIEEE, 2019, pp. 10361041.\\n[39]\\nY. Tay, A. T. Luu, and S. C. Hui, Multi-pointer co-attention net-\\nworks for recommendation, in 24th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining (KDD 2018).\\nACM, 2018, pp. 23092318.\\n[40]\\nS. Liu, Z. Chen, H. Liu, and X. Hu, User-video co-attention\\nnetwork for personalized micro-video recommendation, in 2019\\nWorld Wide Web Conference (WWW 2019).\\nACM, 2019, pp. 3020\\n3026.\\n[41]\\nM. Tu, G. Wang, J. Huang, Y. Tang, X. He, and B. Zhou, Multi-\\nhop reading comprehension across multiple documents by rea-\\nsoning over heterogeneous graphs, in 57th Annual Meeting of the\\nAssociation for Computational Linguistics (ACL 2019).\\nAssociation\\nfor Computational Linguistics, 2019, pp. 27042713.\\n[42]\\nY.-J. Lu and C.-T. Li, GCAN: Graph-aware co-attention networks\\nfor explainable fake news detection on social media, in 58th\\nAnnual Meeting of the Association for Computational Linguistics\\n(ACL 2020).\\nACL, 2020, pp. 505514.\\n[43]\\nO. Wallaart and F. Frasincar, A hybrid approach for aspect-\\nbased sentiment analysis using a lexicalized domain ontology\\nand attentional neural models, in 16th Extended Semantic Web\\nConference (ESWC 2019), ser. LNCS, vol. 11503.\\nSpringer, 2019,\\npp. 363378.\\n[44]\\nS. Zhao and Z. Zhang, Attention-via-attention neural machine\\ntranslation, in 32nd AAAI Conference on Articial Intelligence\\n(AAAI 2018).\\nAAAI Press, 2018, pp. 563570.\\n[45]\\nL. Wu, L. Chen, R. Hong, Y. Fu, X. Xie, and M. Wang, A\\nhierarchical attention model for social contextual image recom-\\nmendation, IEEE Transactions on Knowledge and Data Engineering,\\n2019.\\n[46]\\nY. Wang, S. Wang, J. Tang, N. OHare, Y. Chang, and B. Li,\\nHierarchical attention network for action recognition in videos,\\narXiv:1607.06416, 2016.\\n[47]\\nZ. Li, Y. Wei, Y. Zhang, and Q. Yang, Hierarchical attention\\ntransfer network for cross-domain sentiment classication, in\\n32nd AAAI Conference on Articial Intelligence (AAAI 2018). AAAI\\nPress, 2018, pp. 58525859.\\n[48]\\nC. Xing, Y. Wu, W. Wu, Y. Huang, and M. Zhou, Hierarchical\\nrecurrent attention network for response generation, in 32nd\\nAAAI Conference on Articial Intelligence (AAAI 2018).\\nAAAI\\nPress, 2018, pp. 56105617.\\n[49]\\nV. A. Sindagi and V. M. Patel, HA-CCN: Hierarchical attention-\\nbased crowd counting network, IEEE Transactions on Image Pro-\\ncessing, vol. 29, pp. 323335, 2019.\\n[50]\\nD. Kiela, C. Wang, and K. Cho, Dynamic meta-embeddings\\nfor improved sentence representations, in 2018 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP 2018).\\nACL, 2018, pp. 14661477.\\n[51]\\nS. Maharjan, M. Montes, F. A. Gonzalez, and T. Solorio, A\\ngenre-aware attention model to improve the likability prediction\\nof books, in 2018 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP 2018).\\nACL, 2018, pp. 33813391.\\n[52]\\nG. I. Winata, Z. Lin, and P. Fung, Learning multilingual meta-\\nembeddings for code-switching named entity recognition, in 4th\\nWorkshop on Representation Learning for NLP (RepL4NLP 2019).\\nACL, 2019, pp. 181186.\\n[53]\\nR. Jin, L. Lu, J. Lee, and A. Usman, Multi-representational con-\\nvolutional neural networks for text classication, Computational\\nIntelligence, vol. 35, no. 3, pp. 599609, 2019.\\n[54]\\nA. Sordoni, P. Bachman, A. Trischler, and Y. Bengio, It-\\nerative\\nalternating\\nneural\\nattention\\nfor\\nmachine\\nreading,\\narXiv:1606.02245, 2016.\\n[55]\\nA. Graves, G. Wayne, and I. Danihelka, Neural Turing ma-\\nchines, arXiv:1410.5401, 2014.\\n[56]\\nD. Britz, A. Goldie, M.-T. Luong, and Q. Le, Massive exploration\\nof neural machine translation architectures, in 2017 Conference on\\nEmpirical Methods in Natural Language Processing (EMNLP 2017).\\nACL, 2017, pp. 14421451.\\n19\\n[57]\\nR. J. Williams, Simple statistical gradient-following algorithms\\nfor connectionist reinforcement learning, Machine Learning,\\nvol. 8, no. 3, pp. 229256, 1992.\\n[58]\\nT. Shen, T. Zhou, G. Long, J. Jiang, S. Wang, and C. Zhang, Rein-\\nforced self-attention network: a hybrid of hard and soft attention\\nfor sequence modeling, in 27th International Joint Conference on\\nArticial Intelligence (IJCAI 2018).\\nIJCAI, 2018, pp. 43454352.\\n[59]\\nM. Malinowski, C. Doersch, A. Santoro, and P. Battaglia, Learn-\\ning visual question answering by bootstrapping hard attention,\\nin 2018 European Conference on Computer Vision (ECCV 2018), 2018.\\n[60]\\nY. Liu, W. Wang, Y. Hu, J. Hao, X. Chen, and Y. Gao, Multi-\\nagent game abstraction via graph attention neural network, in\\n34th AAAI Conference on Articial Intelligence (AAAI 2020), vol. 34,\\nno. 05.\\nAAAI Press, 2020, pp. 72117218.\\n[61]\\nS. Seo, J. Huang, H. Yang, and Y. Liu, Interpretable convolutional\\nneural networks with dual local and global attention for review\\nrating prediction, in 11th ACM Conference on Recommender Sys-\\ntems (RecSys 2017).\\nACM, 2017, pp. 297305.\\n[62]\\nJ. Wang, C. Sun, S. Li, X. Liu, L. Si, M. Zhang, and G. Zhou, As-\\npect sentiment classication towards question-answering with\\nreinforced bidirectional attention network, in 57th Annual Meet-\\ning of the Association for Computational Linguistics (ACL 2019).\\nACL, 2019, pp. 35483557.\\n[63]\\nM. Jiang, C. Li, J. Kong, Z. Teng, and D. Zhuang, Cross-\\nlevel reinforced attention network for person re-identication,\\nJournal of Visual Communication and Image Representation, vol. 69,\\np. 102775, 2020.\\n[64]\\nT. Shen, T. Zhou, G. Long, J. Jiang, S. Pan, and C. Zhang, DiSAN:\\nDirectional self-attention network for RNN/CNN-free language\\nunderstanding, in 32nd AAAI Conference on Articial Intelligence\\n(AAAI 2018).\\nAAAI Press, 2018, pp. 54465455.\\n[65]\\nO. Arshad, I. Gallo, S. Nawaz, and A. Calefati, Aiding intra-text\\nrepresentations with visual context for multimodal named entity\\nrecognition, in 2019 International Conference on Document Analysis\\nand Recognition (ICDAR 2019).\\nIEEE, 2019, pp. 337342.\\n[66]\\nW. Wu, X. Sun, and H. Wang, Question condensing networks for\\nanswer selection in community question answering, in Proceed-\\nings of the 56th Annual Meeting of the Association for Computational\\nLinguistics (ACL 2018).\\nACL, 2018, pp. 17461755.\\n[67]\\nO. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich,\\nK. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz,\\nB. Glocker, and D. Rueckert, Attention U-Net: Learning where\\nto look for the pancreas, in 1st Medical Imaging with Deep Learning\\nConference (MIDL 2018), 2018.\\n[68]\\nR. Tan, J. Sun, B. Su, and G. Liu, Extending the transformer\\nwith context and multi-dimensional mechanism for dialogue\\nresponse generation, in 8th International Conference on Natural\\nLanguage Processing and Chinese Computing (NLPCC 2019), ser.\\nLNCS, J. Tang, M.-Y. Kan, D. Zhao, S. Li, and H. Zan, Eds., vol.\\n11839.\\nSpringer, 2019, pp. 189199.\\n[69]\\nL. Chen, B. Lv, C. Wang, S. Zhu, B. Tan, and K. Yu, Schema-\\nguided multi-domain dialogue state tracking with graph at-\\ntention neural networks, in 34th AAAI Conference on Articial\\nIntelligence (AAAI 2020), vol. 34, no. 05.\\nAAAI Press, 2020, pp.\\n75217528.\\n[70]\\nH. Wang, G. Liu, A. Liu, Z. Li, and K. Zheng, Dmran: A hier-\\narchical ne-grained attention-based network for recommenda-\\ntion, in 28th International Joint Conference on Articial Intelligence\\n(IJCAI 2019).\\n[71]\\nZ. Lin, M. Feng, C. N. d. Santos, M. Yu, B. Xiang, B. Zhou,\\nand Y. Bengio, A structured self-attentive sentence embedding,\\nin 5th International Conference on Learning Representations (ICLR\\n2017), 2017.\\n[72]\\nJ. L. Ba, J. R. Kiros, and G. E. Hinton, Layer normalization,\\narXiv:1607.06450, 2016.\\n[73]\\nH. Zhao, J. Jia, and V. Koltun, Exploring self-attention for image\\nrecognition, in 2020 IEEE/CVF Conference on Computer Vision and\\nPattern Recognition (CVPR 2020), 2020, pp. 10 07610 085.\\n[74]\\nH. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, Self-\\nattention generative adversarial networks, in 36th International\\nConference on Machine Learning (ICML 2019), ser. Proceedings of\\nMachine Learning Research, K. Chaudhuri and R. Salakhutdinov,\\nEds., vol. 97.\\nPMLR, 2019, pp. 73547363.\\n[75]\\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\\nFarley, S. Ozair, A. Courville, and Y. Bengio, Generative ad-\\nversarial nets, in 27th Annual Conference on Neural Information\\nProcessing Systems (NIPS 2014).\\nCurran Associates, Inc., 2014,\\npp. 26722680.\\n[76]\\nA. Sinha and J. Dolz, Multi-scale self-guided attention for med-\\nical image segmentation, IEEE Journal of Biomedical and Health\\nInformatics, vol. 25, no. 1, pp. 121130, 2021.\\n[77]\\nJ. Fajtl, H. S. Sokeh, V. Argyriou, D. Monekosso, and P. Re-\\nmagnino, Summarizing videos with attention, in 2018 Asian\\nConference on Computer Vision (ACCV 2018), ser. LNCS, vol. 11367.\\nSpringer, 2018, pp. 3954.\\n[78]\\nJ. Salazar, K. Kirchhoff, and Z. Huang, Self-attention networks\\nfor connectionist temporal classication in speech recognition,\\nin 2019 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP 2019).\\nIEEE, 2019, pp. 71157119.\\n[79]\\nT. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman,\\nDeep audio-visual speech recognition, IEEE Transactions on\\nPattern Analysis and Machine Intelligence, pp. 11, 2018.\\n[80]\\nS. Zhang, Y. Tay, L. Yao, and A. Sun, Next item recommendation\\nwith self-attention, arXiv preprint arXiv:1808.06414, 2018.\\n[81]\\nG. Letarte, F. Paradis, P. Gigu`ere, and F. Laviolette, Impor-\\ntance of self-attention for sentiment analysis, in 2018 Workshop\\nBlackboxNLP: Analyzing and Interpreting Neural Networks for NLP\\n(BlackboxNLP 2018).\\nACL, 2018, pp. 267275.\\n[82]\\nA. Sankar, Y. Wu, L. Gou, W. Zhang, and H. Yang, Dysat:\\nDeep neural representation learning on dynamic graphs via self-\\nattention networks, in 13th International Conference on Web Search\\nand Data Mining (WSDM 2020), 2020, pp. 519527.\\n[83]\\nP. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o,\\nand Y. Bengio, Graph attention networks, in 5th International\\nConference on Learning Representations (ICLR 2017), 2017.\\n[84]\\nS. Iida, R. Kimura, H. Cui, P.-H. Hung, T. Utsuro, and M. Nagata,\\nAttention over heads: A multi-hop attention for neural machine\\ntranslation, in 57th Annual Meeting of the Association for Com-\\nputational Linguistics: Student Research Workshop (ACL-SRW 2019).\\nACL, 2019, pp. 217222.\\n[85]\\nN. K. Tran and C. Niedereee, Multihop attention networks for\\nquestion answer matching, in 41st ACM SIGIR International Con-\\nference on Research & Development in Information Retrieval (SIGIR\\n2018).\\nACM, 2018, pp. 325334.\\n[86]\\nY. Gong and S. R. Bowman, Ruminating reader: Reasoning\\nwith gated multi-hop attention, in 5th International Conference\\non Learning Representation (ICLR 2017), 2017.\\n[87]\\nS. Yoon, S. Byun, S. Dey, and K. Jung, Speech emotion recogni-\\ntion using multi-hop attention mechanism, in 2019 IEEE Interna-\\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP\\n2019).\\nIEEE, 2019, pp. 28222826.\\n[88]\\nZ. Yang, X. He, J. Gao, L. Deng, and A. Smola, Stacked attention\\nnetworks for image question answering, in 2016 IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition (CVPR 2016),\\n2016, pp. 2129.\\n[89]\\nY. Wang, A. Sun, J. Han, Y. Liu, and X. Zhu, Sentiment analysis\\nby capsules, in 2018 World Wide Web Conference (WWW 2018).\\nACM, 2018, p. 11651174.\\n[90]\\nS. Sabour, N. Frosst, and G. E. Hinton, Dynamic routing be-\\ntween capsules, in 31st Annual Conference on Neural Information\\nProcessing Systems (NIPS 2017).\\nCurran Associates, Inc., 2017, p.\\n38593869.\\n[91]\\nM. India, P. Safari, and J. Hernando, Self multi-head attention for\\nspeaker recognition, in Proceedings of the 20th Annual Conference\\nof the International Speech Communication Association (Interspeech\\n2019).\\nISCA, 2019, pp. 28222826.\\n[92]\\nC. Wu, F. Wu, S. Ge, T. Qi, Y. Huang, and X. Xie, Neural\\nnews recommendation with multi-head self-attention, in 2019\\nConference on Empirical Methods in Natural Language Processing and\\nthe 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP 2019).\\nACL, 2019, pp. 63896394.\\n[93]\\nY. Wang, W. Chen, D. Pi, and L. Yue, Adversarially regularized\\nmedication recommendation model with multi-hop memory net-\\nwork, Knowledge and Information Systems, vol. 63, no. 1, pp. 125\\n142, 2021.\\n[94]\\nM. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, Meshed-\\nmemory transformer for image captioning, in 2020 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR\\n2020), 2020, pp. 10 57810 587.\\n[95]\\nJ. Chen, Y. Lu, Q. Yu, X. Luo, E. Adeli, Y. Wang, L. Lu, A. L. Yuille,\\nand Y. Zhou, TransUnet: Transformers make strong encoders\\nfor medical image segmentation, arXiv preprint arXiv:2102.04306,\\n2021.\\n20\\n[96]\\nP. Zhong, D. Wang, and C. Miao, Knowledge-enriched trans-\\nformer for emotion detection in textual conversations, in 2019\\nConference on Empirical Methods in Natural Language Processing and\\nthe 9th International Joint Conference on Natural Language Processing\\n(EMNLP-IJCNLP 2019).\\nACL, 2019, pp. 165176.\\n[97]\\nY. Zhou, R. Ji, J. Su, X. Sun, and W. Chen, Dynamic capsule\\nattention for visual question answering, in 33rd AAAI Conference\\non Articial Intelligence (AAAI 2019), vol. 33, no. 01.\\nAAAI Press,\\n2019, pp. 93249331.\\n[98]\\nY. Wang, A. Sun, M. Huang, and X. Zhu, Aspect-level sentiment\\nanalysis using AS-capsules, in The World Wide Web Conference,\\n2019, pp. 20332044.\\n[99]\\nZ. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov,\\nTransformer-XL: Attentive language models beyond a xed-\\nlength context, in 57th Annual Meeting of the Association for\\nComputational Linguistics (ACL 2019).\\nACL, 2019, pp. 29782988.\\n[100] N. Kitaev, . Kaiser, and A. Levskaya, Reformer: The efcient\\nTransformer, in 8th International Conference on Learning Represen-\\ntations (ICLR 2020), 2020.\\n[101] S. Wang, B. Li, M. Khabsa, H. Fang, and H. Ma, Linformer: Self-\\nattention with linear complexity, arXiv:2006.04768, 2020.\\n[102] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, Lite transformer\\nwith long-short range attention, in 8th International Conference\\non Learning Representations (ICLR 2020), 2020.\\n[103] Y. Tay, D. Bahri, D. Metzler, D.-C. Juan, Z. Zhao, and C. Zheng,\\nSynthesizer: Rethinking self-attention for transformer models,\\nin Proceedings of the 38th International Conference on Machine Learn-\\ning (ICML 2021), vol. 139.\\nPMLR, 2021, pp. 10 18310 192.\\n[104] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, Efcient trans-\\nformers: A survey, arXiv:2009.06732, 2020.\\n[105] X. Li, J. Song, L. Gao, X. Liu, W. Huang, X. He, and C. Gan,\\nBeyond RNNs: Positional self-attention with co-attention for\\nvideo question answering, in 33rd AAAI Conference on Articial\\nIntelligence (AAAI 2019), vol. 33.\\nAAAI Press, 2019, pp. 8658\\n8665.\\n[106] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi,\\nand Q. V. Le, QANet: Combining local convolution with global\\nself-attention for reading comprehension, in 6th International\\nConference on Learning Representations (ICLR 2018), 2018.\\n[107] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, BLEU: a method\\nfor automatic evaluation of machine translation, in 40th Annual\\nMeeting of the Association for Computational Linguistics (ACL 2002).\\nACL, 2002, pp. 311318.\\n[108] S. Banerjee and A. Lavie, METEOR: An automatic metric for MT\\nevaluation with improved correlation with human judgments,\\nin 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for\\nMachine Translation and/or Summarization.\\nACL, 2005, pp. 6572.\\n[109] R. Sennrich, Perplexity minimization for translation model do-\\nmain adaptation in statistical machine translation, in 13th Con-\\nference of the European Chapter of the Association for Computational\\nLinguistics (EACL 2012).\\nACL, 2012, pp. 539549.\\n[110] M. Popovic and H. Ney, Word error rates: Decomposition over\\nPOS classes and applications for error analysis, in 2nd Workshop\\non Statistical Machine Translation (WMT 2007).\\nACL, 2007, pp.\\n4855.\\n[111] P. Schwarz, P. Matejka, and J. Cernock`y, Towards lower error\\nrates in phoneme recognition, in 7th International Conference\\non Text, Speech and Dialogue (TSD 2004), ser. LNCS, vol. 3206.\\nSpringer, 2004, pp. 465472.\\n[112] D. S. Turaga, Y. Chen, and J. Caviedes, No reference PSNR\\nestimation for compressed pictures, Signal Processing: Image\\nCommunication, vol. 19, no. 2, pp. 173184, 2004.\\n[113] P. Ndajah, H. Kikuchi, M. Yukawa, H. Watanabe, and S. Mu-\\nramatsu, SSIM image quality metric for denoised images, in\\n3rd WSEAS International Conference on Visualization, Imaging and\\nSimulation (VIS 2010).\\nWSEAS, 2010, pp. 5358.\\n[114] M. A. Rahman and Y. Wang, Optimizing intersection-over-union\\nin deep neural networks for image segmentation, in 12th Inter-\\nnational Symposium on Visual Computing (ISVC 2016), ser. LNCS,\\nvol. 10072.\\nSpringer, 2016, pp. 234244.\\n[115] X. Chen, L. Yao, and Y. Zhang, Residual attention U-net for auto-\\nmated multi-class segmentation of COVID-19 chest CT images,\\narXiv:2004.05645, 2020.\\n[116] S. Liu, Y. Chen, K. Liu, and J. Zhao, Exploiting argument\\ninformation to improve event detection via supervised attention\\nmechanisms, in 55th Annual Meeting of the Association for Compu-\\ntational Linguistics (ACL 2017).\\nACL, 2017, pp. 17891798.\\n[117] C. Liu, J. Mao, F. Sha, and A. Yuille, Attention correctness in\\nneural image captioning, in 31st AAAI Conference on Articial\\nIntelligence (AAAI 2017).\\nAAAI Press, 2017, pp. 41764182.\\n[118] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, Human\\nattention in visual question answering: Do humans and deep\\nnetworks look at the same regions? Computer Vision and Image\\nUnderstanding, vol. 163, pp. 90  100, 2017.\\n[119] Y. Yu, J. Choi, Y. Kim, K. Yoo, S.-H. Lee, and G. Kim, Supervising\\nneural attention models for video captioning by human gaze\\ndata, in 2017 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR 2017).\\nIEEE Computer Society, 2017.\\n[120] S. Jain and B. C. Wallace, Attention is not explanation, in\\n2019 Conference of the North American Chapter of the Association for\\nComputational Linguistics: Human Language Technologies (NAACL-\\nHLT 2019).\\nACL, 2019, pp. 35433556.\\n[121] S. Wiegreffe and Y. Pinter, Attention is not not explanation, in\\n2019 Conference on Empirical Methods in Natural Language Process-\\ning and the 9th International Joint Conference on Natural Language\\nProcessing (EMNLP-IJCNLP 2019).\\nACL, 2019, pp. 1120.\\n[122] A. K. Mohankumar, P. Nema, S. Narasimhan, M. M. Khapra,\\nB. V. Srinivasan, and B. Ravindran, Towards transparent and\\nexplainable attention models, in 58th Annual Meeting of the\\nAssociation for Computational Linguistics (ACL 2020).\\nACL, 2020,\\npp. 42064216.\\n[123] K. K. Thekumparampil, C. Wang, S. Oh, and L.-J. Li, Attention-\\nbased graph neural network for semi-supervised learning,\\narXiv:1803.03735, 2018.\\n[124] D. Nie, Y. Gao, L. Wang, and D. Shen, ASDNet: Attention based\\nsemi-supervised deep networks for medical image segmenta-\\ntion, in 21st International Conference on Medical Image Computing\\nand Computer-Assisted Intervention (MICCAI 2018), ser. LNCS, vol.\\n11073.\\nSpringer, 2018, pp. 370378.\\n[125] Y. Alami Mejjati, C. Richardt, J. Tompkin, D. Cosker, and K. I.\\nKim, Unsupervised attention-guided image-to-image transla-\\ntion, in 32nd Annual Conference on Neural Information Processing\\nSystems (NIPS 2018).\\nCurran Associates, Inc., 2018, pp. 3693\\n3703.\\n[126] R. He, W. S. Lee, H. T. Ng, and D. Dahlmeier, An unsupervised\\nneural attention model for aspect extraction, in 55th Annual\\nMeeting of the Association for Computational Linguistics (ACL 2017).\\nACL, 2017, pp. 388397.\\nGianni Brauwers was born in Spijkenisse, the\\nNetherlands, in 1998. He received the B.S. de-\\ngree in econometrics and operations research\\nfrom Erasmus University Rotterdam, Rotterdam,\\nthe Netherlands, in 2019, and is currently pursu-\\ning the M.S. degree in econometrics and man-\\nagement science at Erasmus University Rotter-\\ndam.\\nHe is a Research Assistant at Erasmus Uni-\\nversity Rotterdam, focusing his research on neu-\\nral attention models and sentiment analysis.\\nFlavius Frasincar was born in Bucharest, Ro-\\nmania, in 1971. He received the M.S. degree\\nin computer science, in 1996, and the M.Phil.\\ndegree in computer science, in 1997, from Po-\\nlitehnica University of Bucharest, Bucharest, Ro-\\nmania, and the P.D.Eng. degree in computer sci-\\nence, in 2000, and the Ph.D. degree in computer\\nscience, in 2005, from Eindhoven University of\\nTechnology, Eindhoven, the Netherlands.\\nSince 2005, he has been an Assistant Profes-\\nsor in computer science at Erasmus University\\nRotterdam, Rotterdam, the Netherlands. He has published in numerous\\nconferences and journals in the areas of databases, Web information\\nsystems, personalization, machine learning, and the Semantic Web.\\nHe is a member of the editorial boards of Decision Support Systems,\\nInternational Journal of Web Engineering and Technology, and Com-\\nputational Linguistics in the Netherlands Journal, and co-editor-in-chief\\nof the Journal of Web Engineering. Dr. Frasincar is a member of the\\nAssociation for Computing Machinery.\\n'),\n",
       "   Document(metadata={'Published': '2024-10-21', 'Title': 'Generalized Probabilistic Attention Mechanism in Transformers', 'Authors': 'DongNyeong Heo, Heeyoul Choi', 'Summary': 'The Transformer architecture has become widely adopted due to its\\ndemonstrated success, attributed to the attention mechanism at its core.\\nDespite these successes, the attention mechanism of Transformers is associated\\nwith two well-known issues: rank-collapse and gradient vanishing. In this\\npaper, we present a theoretical analysis that it is inherently difficult to\\naddress both issues simultaneously in the conventional attention mechanism. To\\nhandle these issues, we introduce a novel class of attention mechanism,\\nreferred to as generalized probabilistic attention mechanism (GPAM), and its\\ndual-attention implementation within the Transformer architecture. Unlike\\nconventional attention mechanisms, GPAM allows for negative attention scores\\nwhile preserving a fixed total sum. We provide theoretical evidence that the\\nproposed dual-attention GPAM (daGPAM) effectively mitigates both the\\nrank-collapse and gradient vanishing issues which are difficult to resolve\\nsimultaneously with the conventional attention mechanisms. Furthermore, we\\nempirically validate this theoretical evidence, demonstrating the superiority\\nof daGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.'}, page_content='Arxiv preprint version\\nGENERALIZED PROBABILISTIC ATTENTION MECHA-\\nNISM IN TRANSFORMERS\\nDongNyeong Heo & Heeyoul Choi\\nDept. of Computer Science and Electrical Engineering\\nHandong Global University\\nPohang, South Korea\\ndnheo@handong.ac.kr & hchoi@handong.edu\\nABSTRACT\\nThe Transformer architecture has become widely adopted due to its demonstrated\\nsuccess, attributed to the attention mechanism at its core. Despite these successes,\\nthe attention mechanism of Transformers is associated with two well-known is-\\nsues: rank-collapse and gradient vanishing. In this paper, we present a theoret-\\nical analysis that it is inherently difficult to address both issues simultaneously\\nin the conventional attention mechanism. To handle these issues, we introduce a\\nnovel class of attention mechanism, referred to as generalized probabilistic atten-\\ntion mechanism (GPAM), and its dual-attention implementation within the Trans-\\nformer architecture. Unlike conventional attention mechanisms, GPAM allows for\\nnegative attention scores while preserving a fixed total sum. We provide theoret-\\nical evidence that the proposed dual-attention GPAM (daGPAM) effectively mit-\\nigates both the rank-collapse and gradient vanishing issues which are difficult to\\nresolve simultaneously with the conventional attention mechanisms. Furthermore,\\nwe empirically validate this theoretical evidence, demonstrating the superiority of\\ndaGPAM compared to other alternative attention mechanisms that were proposed\\nto address the same issues. Additionally, we demonstrate the practical benefits\\nof GPAM in natural language processing tasks, such as language modeling and\\nneural machine translation.\\n1\\nINTRODUCTION\\nThe Transformer model, as introduced by (Vaswani, 2017), has emerged as a pivotal architecture\\ndriving the advancement of contemporary deep learning models across various domains, including\\nnatural language processing (Brown et al., 2020), audio signal processing (Gulati et al., 2020), and\\nimage processing (Dosovitskiy et al., 2021). Central to the Transformers success is the attention\\nmechanism, which facilitates the contextualization of input token representations. Based on the\\nsimilarities of query and key vectors, the attention mechanism mixes value vectors as follows (a\\nsingle scaled dot-product self-attention head (Vaswani, 2017)):\\nY = PXV ,\\n(1)\\nPij =\\nexp(Aij)\\nPT\\nk=1 exp(Aik)\\n,\\n(2)\\nA =\\n\\x10p\\ndqk\\n\\x111\\nXQX\\nK,\\n(3)\\nwhere XQ = XWQ, XK = XWK, and XV = XWV . X RT d is input representations with\\nT sequence length and d dimensionality, and WQ, WK Rddqk, and WV Rddv are weight\\nmatrices.\\nDespite its empirical success, the conventional attention mechanism, particularly the self-attention\\nmechanism, has been shown to exhibit two significant limitations. The first issue is the phenomenon\\nknown as the rank-collapse problem (Dong et al., 2021), where output token representations become\\nsimilar to others, leading to a loss of rank as they progress through each layer. While a controlled\\n1\\narXiv:2410.15578v1  [cs.LG]  21 Oct 2024\\nArxiv preprint version\\nFigure 1: Examples of convex and\\naffine combinations Yc and Ya,\\ngiven X value representations.\\nFigure 2: Our proposed daGPAM structure in an example\\nof two multi-head self-attention in Transformer.\\ndegree of rank-reduction can be beneficial for eliminating redundant information (Tishby & Za-\\nslavsky, 2015), excessive rank-reduction risks the loss of critical information. Previous studies have\\nextensively documented the self-attention layers tendency for intensive rank-reduction (Dong et al.,\\n2021; Noci et al., 2022; 2024). The second issue is the gradient vanishing problem (Richter & Wat-\\ntenhofer, 2022; Wang et al., 2021). During the softmax-based normalization step, the gradient is\\nconsistently less than 1 and saturates close to 0, thereby impeding upper attention layers sufficient\\nflow of gradients to the lower layers. Moreover, in this paper, we demonstrate that these two issues\\nare inherently challenging to address both simultaneously.\\nPrevious approaches to addressing the rank-collapse problem in Transformers have primarily fo-\\ncused on preserving input token representations. One strategy amplifies the coefficient of the short-\\ncut branch in the residual connection (Noci et al., 2022), while another reduces the contextualization\\neffect by regularizing the attention matrix, P, to be similar to an identity matrix (He et al., 2023;\\nNoci et al., 2024). However, these methods can diminish the attention layers ability to capture\\nmeaningful contextual information (Veit et al., 2016; Zhang et al., 2022). To address the gradient\\nvanishing problem, alternative attention mechanisms have been proposed to replace the conven-\\ntional softmax-based mechanism, Eqs.2 and 3 (Richter & Wattenhofer, 2022; Wang et al., 2021).\\nDespite their theoretical effects for improving gradient flow, these alternative mechanisms have not\\ndemonstrated practical advantages in benchmark experiments.\\nIn contrast to prior approaches, we posit that the underlying issues stem from the convex combi-\\nnation structure in the conventional attention mechanism. The normalized attention scores, Pij, as\\ndefined in Eq.2, are non-negative and sum to 1 along the query axis, making the conventional atten-\\ntion mechanism a valid convex combination of input representations. Consequently, as illustrated\\nin Fig.1, the output representations, Yc, are constrained within the convex hull (yellow plane) of\\nthe input value representations. This fundamental constraint limits the diversity of output represen-\\ntations, causing them to become more similar to one another than the input value representations,\\nthereby intensifying the rank-collapse problem.\\nThis consideration naturally raised the question: how about making it an affine combination? The\\naffine combination is a generalization of the convex combination, allowing the normalized attention\\nscores to take negative values while still maintaining a total sum to be 1. As depicted in Fig.1,\\nthe output representations, Ya, are no longer constrained to lie within the convex hull, but instead\\nwithin the affine hull (green plane), thereby removing the fundamental constraint that can lead to\\nrank-collapse. However, from a probabilistic perspective, the notion of negative normalized atten-\\ntion scores may seem unconventional, since negative probability is unfamiliar concept. Nevertheless,\\nthe discussions by prominent physicists, such as P. Dirac (Dirac, 1942) and R. Feynman (Feynman,\\n1984), have concretized the concept of negative probability by generalizing the Kolmogorovs prob-\\nability conditions as follows:\\nKolmogorovs Probability Conditions.. (1) P(e) 0, and (2) P\\ne\\nP(e) = 1 where P(e) R is\\nthe probability measure of event e and is event space.1\\n1For simplicity, we mention only the two main conditions.\\n2\\nArxiv preprint version\\nGeneralized Probability Conditions (Szekely, 2005). (1) P\\ne\\n|P(e)| < , and (2) P\\ne\\nP(e) = 1\\nwhere P(e) R is the probability measure of event e and is event space.\\nIt is important to note that the first condition of generalized probability allows for the possibility of\\nnegative probabilities. Building on this long-standing line of research, we introduce the attention\\nmechanism based on the affine (or scaled-affine) combination, which we refer to as the generalized\\nprobabilistic attention mechanism (GPAM). As a generalized framework, GPAM encompasses the\\nconventional attention mechanism as a special case where all attention values are non-negative.\\nIn this paper, we explore GPAM in the Transformer architecture, and as a cornerstone design of\\nGPAM, we propose a dual-attention design that facilitates GPAM with adding only a negligible\\nnumber of parameters. Specifically, we add an additional attention matrix computation to the orig-\\ninal scaled dot-product self-(or cross-) attention mechanism with a small additional weight matrix\\nwhich increases less than 1% of total parameters on average. Then, the resulting two attention matri-\\nces are treated as positive and negative parts of the final attention score, respectively. By combining\\nthese two attention scores with pre-defined or trainable scalar weights, we ensure that our proposed\\ndual-attention GPAM (daGPAM) a valid affine (or scaled-affine) combination. Fig.2 illustrates the\\nstructure of this design. When we propose a GPAM method, we show theoretically that our method\\nis advantageous for mitigating not only the rank-collapse problem, but also the gradient vanishing\\nproblem. Our empirical validations give evidences for the above theories in various aspects. In\\naddition, we experimentally demonstrate and explain the superiority of daGPAM compared to other\\nalternative attention mechanisms that was proposed to address the mentioned problems. Finally, we\\ndemonstrate the benefits of daGPAM in benchmark experiments in tasks such as language modeling\\n(LM) and neural machine translation (NMT).\\n2\\nRELATED WORKS\\n2.1\\nRANK-COLLAPSE PROBLEM IN TRANSFORMER\\nRecent studies have analyzed the occurrence and practical risks of the rank-collapse problem in\\nTransformers (Dong et al., 2021; Yan et al., 2022; Noci et al., 2022; He et al., 2023; Noci et al.,\\n2024). The rank-collapse phenomenon refers to the tendency of token representations to become\\nincreasingly similar to one another as they are processed through successive layers. The first theo-\\nretical exploration of this issue demonstrated that the pure attention layer, as described by Eqs.13,\\nreduces the residual at an exponential rate with increasing layer depth. The residual metric quan-\\ntifies how close token representations are to their mean point in terms of Euclidean distance, and is\\nformally defined as follows:\\nres(X) = X 1xRT d, where x = arg min\\nx\\nX 1x.\\n(4)\\nThen, the relationship of input/output residual is derived as follows:\\nLemma 1 (Dong et al. (2021), Simplified). For any single scaled dot-product self-attention layer\\nwith a term  that depends on the attention entries, the composite norm of output residual is bounded\\nby\\nres(Y)1,4\\n\\n2WQK1WV 1,\\np\\ndqk\\nres(X)3\\n1,,\\n(5)\\nwhere WQK = WQW\\nK. In the region that holds 4\\n\\n2WQK1WV 1,<\\np\\ndqk, the output\\nresidual norm is diminished compared to the cubic rate of input residual norm.\\nBuilding on this lemma and extending it to multi-layer cases, prior research has argued for the\\nexistence of the rank-collapse problem and empirically demonstrated that a substantial region of the\\nparameter space that falls into rank-collapse issue (Dong et al., 2021). For a complete description\\nof the lemma, please refer to Appendix A.1.1, and read the original paper for the proof. In addition\\nto the rank-collapse problem, related concepts such as attention collapse, over-smoothing, over-\\ncorrelation, and dimensional collapse have been identified in various domains, including vision\\nTransformers (Zhou et al., 2021; Tang et al., 2021; Gong et al., 2021; Wang et al., 2022), contrastive\\n3\\nArxiv preprint version\\nlearning (Jing et al., 2021; Hua et al., 2021), graph neural networks (Li et al., 2018; Jin et al., 2022;\\nGuo et al., 2023; Roth & Liebig, 2024), and general neural networks (Feng et al., 2022; Jacot, 2023).\\nPrevious studies have attempted to address the rank-collapse problem through various strategies,\\nsuch as strengthening the shortcut branch (Tang et al., 2021; Noci et al., 2022), regularizing the at-\\ntention matrix to be similar to an identity matrix (He et al., 2023; Noci et al., 2024), and regularizing\\nto preserve local information (Yan et al., 2022). However, these approaches may weaken the contex-\\ntualization effect of the attention layer. Alternative methods have proposed various sequence-wise\\nnormalization techniques aimed at explicitly diversifying token representations (Hua et al., 2021;\\nGuo et al., 2023). However, these techniques may be unsuitable for short sentences and autore-\\ngressive architectures due to their reliance on inadequate statistic across sequences. In contrast to\\nthese approaches, our GPAM develops the attention layer using a novel methodology grounded in\\nthe principles of convex and affine combinations.\\n2.2\\nGRADIENT VANISHING PROBLEM IN TRANSFORMER\\nSeveral previous studies have identified the gradient vanishing problem within the Transformer ar-\\nchitecture (Zhang et al., 2019; Richter & Wattenhofer, 2022; Liu et al., 2020; Wang et al., 2021).\\nA primary contributor to this issue is the gradients of the softmax function, as represented in Eq.2,\\nwhich consistently yield gradient values less than 1. The following lemma derives these gradients.\\nLemma 2 (Gradient Vanishing in Attention Mechanism). The gradient that the input unnormalized\\nattention score, Eq.3, receives through the normalized attention score, Eq.2, (without 1/\\np\\ndqk) is\\nderived as follows:\\nPij\\nAij\\n= Pij(1 Pij),\\nPik,k=j\\nAij\\n= PikPij,\\n(6)\\nThe maximum magnitude of each gradient is 0.25 when Pij = 0.5 for the first, and Pij = 0.5 and\\nPik = 0.5 for the last.\\nPrevious research has demonstrated that there exists a significant saturation range of Aij values,\\nleading to the gradients in Eq.6 approaching close to 0 (Richter & Wattenhofer, 2022).\\nPrevious studies have sought to address the gradient vanishing problem by proposing various forms\\nof unnormalized attention scores, including non-exponentiated raw scores (Richter & Wattenhofer,\\n2022) and scores transformed by periodic functions (Wang et al., 2021). Additionally, some ap-\\nproaches have eliminated the normalization step entirely to ensure that the gradient remains equal\\nto 1 (Richter & Wattenhofer, 2022). While these approaches are theoretically effective in mitigat-\\ning the gradient vanishing issue, they have not demonstrated practical advantages in benchmark\\nexperiments.\\n2.3\\nALTERNATIVE ATTENTION MECHANISMS\\nAmong the previous studies that have proposed alternative attention mechanisms comparable to our\\nGPAM (Wang et al., 2021; Richter & Wattenhofer, 2022; He et al., 2023; Noci et al., 2024), some\\nmethods unintentionally permit negative attention scores, similar to our approach. Additionally,\\nthere is prior research that intentionally incorporates negative attention scores within its framework\\n(Tay et al., 2019). However, most of these studies do not adhere to the generalized probability\\nconditions, specifically: (1) a finite range and (2) a total sum equal to 1 (or another fixed value). We\\nwill discuss the significance of adhering to these conditions and demonstrate the practical advantages\\nthrough experiment results.\\n3\\nRELATIONSHIP OF RANK-COLLAPSE AND GRADIENT VANISHING\\nIn this section, we analyze an inherent relationship between the rank-collapse and gradient vanishing\\nproblems within the conventional attention mechanism. We conjecture that the maximum total norm\\nof gradients, defined as G(Pi) = PT\\nj=1 Pik\\nAij 1, is attained when complete rank-collapse occurs.\\nWe substantiate this conjecture through the following lemma.\\n4\\nArxiv preprint version\\nLemma 3 (Maximum Total Norm of Gradients). The total norm of gradients, G(Pi), is maximized\\nwhen Pi is the uniform distribution, that is Pi = [ 1\\nT , 1\\nT ,    , 1\\nT ] which is the case of complete\\nrank-collapse.\\nSee Appendix A.1.2 for the proof. To mitigate the gradient vanishing problem, it is better to input\\nsimilar token representations to make a smooth normalized attention score distribution. However,\\nthat remedy can make the rank-collapse problem severe. Therefore, mitigating both problems to-\\ngether is challenging.\\n4\\nDUAL-ATTENTION STRUCTURE OF GENERALIZED PROBABILISTIC\\nATTENTION MECHANISM\\nIn this section, we explain our proposed dual-attention implementation of GPAM within the Trans-\\nformer architecture, as illustrated in Fig.2. We further elucidate the dynamics of daGPAM with\\nrespect to the output space and representations. Additionally, we present theoretical foundations\\nsupporting the assertion that our daGPAM effectively addresses both the rank-collapse and gradient\\nvanishing problems simultaneously.\\n4.1\\nDUAL-ATTENTION GPAM (DAGPAM) STRUCTURE\\nBased on the original scaled dot-product attention mechanism (Vaswani, 2017), we add another\\nprocess of computing negative attention matrix while the original attention matrix is treated as the\\npositive attention matrix. During the computation of the negative attention matrix, we use different\\nquery vectors, but transformed from the original query vectors. Subsequently, both the positive and\\nnegative attention matrices are integrated to derive the final attention matrix, and it is multiplied to\\nthe value representations to output the final representations. This process is formulated as follows\\n(we use + notation to the parts of the original attention part):\\nY = PGXV ,\\n(7)\\nPG = (1 + +)P+ P,\\n(8)\\nP+\\nij =\\nexp(A+\\nij)\\nPT\\nk=1 exp(A+\\nik)\\n,\\nA+ =\\n\\x10p\\ndqk\\n\\x111\\nX+\\nQ\\n\\x00X+\\nK\\n\\x01,\\n(9)\\nP\\nij =\\nexp(A\\nij)\\nPT\\nk=1 exp(A\\nik)\\n,\\nA=\\n\\x10p\\ndqk\\n\\x111 \\x10\\n(X+\\nQ)W\\nQ\\n\\x11 \\x00X+\\nK\\n\\x01,\\n(10)\\nwhere X+\\nQ = XW+\\nQ, X+\\nK = XW+\\nK, and XV = XWV . For the new, negative parts, we only add\\nthe non-linear activation,  (we used ReLU throughout this work), and linear transformation with\\nweight matrix W\\nQ Rdqkdqk that has small number of parameters (note that d > dqk). + and\\nare pre-defined or trainable scalars that control the effect of positive and negative normalized\\nattention scores, respectively.\\ndaGPAM exhibits several notable properties concerning its final normalized attention scores, de-\\nnoted as PG. Specifically, the total sum of these scores is  = (1 + + )2 and the range of the\\nscores is constrained such that PG\\nij (1 + +). In the special case where + = = 0,\\ndaGPAM becomes the conventional attention mechanism, Eqs.13. In general, when + = ,\\ntherefore  = 1, daGPAM facilitates a valid affine combination. In the next section, we will further\\nelucidate the case where + = , that facilitates scaled-affine combination whose affine hull is\\nsimply translated.\\n4.2\\nDYNAMICS OF DUAL-ATTENTION GPAM\\nIn this section, we explain the dynamic how the output space and representations of daGPAM are\\ninfluenced by the combination of + and . We begin by explaining the dynamic of the output\\nspace. Based on the formulations in the previous section, the final output representations can be\\n2For simplicity, we utilize the  symbol to represent PT\\nj=1 PG\\nij.\\n5\\nArxiv preprint version\\nFigure 3: Different output space and representations according to  combinations. varies while\\n+ is fixed to 1.\\nderived and expressed as follows: Y = (PGXV ) =  Y where PG\\nij = PG\\nij/. It is important\\nto note that Y represents a valid affine combination of XV since the total sum of PG is equal to 1.\\nConsequently, the possible outcomes for Y can be interpreted as -scaled versions of the outcomes\\nfrom Y which reside within the affine hull defined by XV . This concept is visually represented\\nin Fig.3, where each colored plane illustrates the translated affine hull corresponding to different\\ncombinations of s. It is noteworthy that the degree of translation is contingent upon  rather than\\nthe specific values of s, so in this example, only was adjusted. Constraining the output to lie\\nwithin affine hull (or translated) is critical for effective information processing. The translated affine\\nhull represents a lower-dimensional hyperplane, which is a condensed subset of the d-dimensional\\nspace determined by the processed representations from lower layers. This constraint enhances the\\ninfluence of lower layers on the information processing of upper layers, thereby improving overall\\nmodel efficacy.\\nSecondly, we examine the dynamic of output representations in relation to s. Utilizing the def-\\ninitions established in Eqs.7 and 8, we can express Y in an alternative form as follows: Y =\\n(1++)(P+XV )(PXV ) = (1++)Y+ Y. Here, Y+ represents the output of con-\\nventional attention mechanism, and both Y+ and Yare derived from convex combination of XV ,\\nas depicted in the third (orange) hyperplane of Fig.3. We can further reformulate this as follows:\\nY = Y+ + (Y+ Y) = Y+ + . A crucial characteristic of is that it represents a\\ndirect movement on the hyperplane from the original point Y+, and it is manipulated by , while\\nthe original point Y+ is manipulated by . Consequently, when is large to make  less than 1,\\ndaGPAM scales-down the original point while amplifying the movement on the hyperplane, result-\\ning in an increase in the relative diversity of output representations compared to their average norm.\\nThis dynamic is illustrated in Fig.3 with empirically computed cosine similarity, , based on our toy\\nexperiment setting3. It shows that the cosine similarity decreases with only changing to make a\\nsmall .\\n4.3\\nTHEORETIC ADVANTAGES OF DUAL-ATTENTION GPAM\\nIn this section, we discuss the theoretical advantages of daGPAM in addressing the issues of rank-\\ncollapse and gradient vanishing. First, to enhance our theoretical understanding of the rank-collapse\\nproblem, we derive the input/output relationship of the residual, Eq.4, based on daGPAM structure.\\nThe results of this derivation is presented in the following lemma.\\nLemma 4 (Dual-Attention GPAM residual Bound, Simplified). For any single daGPAM self-\\nattention layer with for a term  that depends on the attention entries, the composite norm of output\\nresidual is bounded by\\nres(Y)1,Borg +\\n4\\n\\n2\\n\\x10\\x0c\\x0c\\x0c+W+\\nQK1 W\\nQK1\\n\\x0c\\x0c\\x0c\\n\\x11\\nWV 1,\\np\\ndqk\\nres(X)3\\n1,, (11)\\n3We implemented daGPAM, Eqs.710, with X and the weight matrices which are randomly initialized by\\nstandard Gaussian distribution. Then, we computed the average cosine similarity of each output representations\\nY with each  combination over 100 times repeatedly.\\n6\\nArxiv preprint version\\nwhere W+\\nQK = W+\\nQ(W+\\nK)and W\\nQK = (W+\\nQW\\nQ)(W+\\nK). Borg is the upper bound derived\\nby Lemma 1. Because the second term is positive, this upper bound is always greater than the\\noriginal.\\nThe complete lemma and its proof are provided in Appendix A.1.3. Based on this lemma, we ar-\\ngue that daGPAM can have greater or equal residual bound compared to the conventional attention\\nmechanism, which suggests a higher likelihood that the average distance between output representa-\\ntions is either maintained or not reduced compared to the average distance of input representations.\\nSecondly, we derive the gradients of daGPAM. Due to the complexity of this derivation, throughout\\nthis derivation, we consider the simple case that approximates the negative unnormalized attention\\nscore is simply the positive score with -1 multiplication, that is A+ = Awith approximating \\nto identity activation and W\\nQ I where I is identity matrix with size dqk. . The result of this\\nderivation is as follows (we simply denote A+ as A for comparison between the original lemma 2):\\nLemma 5 (Dual-Attention GPAM Gradients). The gradient that the input unnormalized attention\\nscore, A, receives through the normalized attention score, PG (without 1/\\np\\ndqk) is derived as\\nfollows:\\nPG\\nij\\nAij\\n= gorg\\nj\\n+ +P+\\nij(1 P+\\nij) + P\\nij(1 P\\nij),\\n(12)\\nPG\\nik,k=j\\nAij\\n= gorg\\nk\\n+ +(P+\\nikP+\\nij) + (P\\nikP\\nij),\\n(13)\\nwhere gorg\\nj\\nand gorg\\nk\\nare the derived gradient of the conventional attention mechanism, Eq.6, re-\\nspectively.\\nThe proof of this lemma is provided in Appendix A.1.4. Because the last two additional terms have\\nthe same sign of the original, daGPAM always flow greater gradients than the conventional attention\\nmechanism. Therefore, daGPAM can mitigate the rank-collapse and gradient vanishing problems\\ntogether.\\n5\\nEMPIRICAL VALIDATIONS\\nTo evaluate the effectiveness of daGPAM in addressing the rank-collapse problem relative to the\\nconventional attention mechanism, we conducted rank-collapse analyses at initialization (faithful-\\nness test) (Poole et al., 2016; Schoenholz et al., 2016; Yang & Schoenholz, 2017; Hayou et al.,\\n2019; Noci et al., 2024) and after the training phase. To assess the impact on the gradient vanish-\\ning problem, we monitored the gradient norm history of the query weights, specifically \\nL\\nWQ 2,\\nthroughout the training process. Finally, to argue the importance of preserving generalized proba-\\nbility conditions, as discussed in Section 2.3, we compare daGPAM with other several alternative\\nattention mechanisms in preliminary experiments.\\nAll empirical analyses were conducted within the framework of our preliminary experimental set-\\ntings, specifically utilizing the Penn Treebank dataset (PTB, 1M total number of tokens (Marcus\\net al., 1993)), for a world-level LM task using a 15-layered decoder-only Transformer model. We\\nemployed open-source resource4 for data-related processes, including preprocessing and tokeniza-\\ntion. Detailed configurations regarding the model architecture and optimization processes are pre-\\nsented in Table 4 in Appendix A.2. We replaced only the conventional attention layers, self-attention\\nlayer, with daGPAM. We explored various configurations, including different combinations of con-\\nstant s and trainable  methods.\\n5.1\\nRANK-COLLAPSE ANALYSES\\nFor the two rank-collapse analyses, we measured the amount of diversity based on the output rep-\\nresentations of self-attention layer (output of multi-head attention layer) for all layers (15, in this\\nexperiment). We used two different metrics to measure the diversity.\\n4https://github.com/kimiyoung/transformer-xl/\\n7\\nArxiv preprint version\\nFigure 4: The results of rank-collapse analyses (left two graphs) and gradient histories during train-\\ning (right two graphs). Horizontal axis of rank-collapse analyses indicate layer index, while those\\nof gradient histories indicate training iterations.\\n Average relative norm of residual (Dong et al., 2021) : res(Y) = 1\\nT\\nPT\\ni=1\\nYiy2\\nYi2 .\\n Average cosine similarity (Noci et al., 2022): cos(Y) =\\n1\\nT 2\\nPT\\ni=1\\nPT\\nj=1\\nYiYj\\nYi2Yi2 .\\nHigher or lower values of res(Y) or cos(Y), respectively, mean less collapsed representations Y.\\nThe left two columns of Fig.4 present the results of the two rank-collapse analyses. In the analysis\\nat initialization, as known, the rank-collapse phenomenon becomes more pronounced in the upper\\nlayers. However, except for the case with (+ = 1.0, = 0.5), daGPAM models exhibited a less\\nintensive tendency toward rank-collapse. Notably, this mitigation becomes more effective when  is\\nsmaller, such as in the case of (+ = 1.0, = 2.0), where  = 0 showed the greatest reduction in\\nrank-collapse. In the analysis after training phase, all daGPAM models demonstrated a less intensive\\ntendency for rank-reduction compared to the baseline. These empirical findings are consistent with\\nLemma 4 and the discussions in Section 4.2. Additionally, we provide further analyses addressing\\nother aspects beyond the scope of the above experiments, including the effect of varying +, a rank-\\ncollapse analysis based on the output of the multi-layer perceptron (MLP) layer (i.e., the final output\\nof the Transformer layer), and an examination of the actual influence of the attention layer compared\\nto the shortcut branch. These additional results can be found in Appendix A.3.\\n5.2\\nGRADIENT HISTORY DURING TRAINING\\nIn addition to addressing the rank-collapse issue, we tracked the gradient norm history of the query\\nweight matrix to verify our claim regarding the gradient vanishing problem (Lemma 5). The right\\ntwo columns of Fig.4 illustrate the gradient norms of query weights across layers 5, 8, 11, and 14. In\\neach case, the query weights in daGPAM model exhibited larger gradients compared to the baseline,\\nsupporting the predictions in Lemma 5. Furthermore, the observation that higher layers receive\\nstronger gradients might align with Lemma 3. As described in the lemma, significant rank-collapse\\nleads to the maximum gradient flow through the softmax operation. Combining with the analysis\\nresult of higher layers more collapsed output representations (Section 5.1), we guess the lemma\\ncould explain why upper layers receive greater gradients than lower layers.\\n5.3\\nCOMPARISON BETWEEN OTHER ALTERNATIVE ATTENTION MECHANISMS\\nIn this section, we compare the performance of daGPAM with other alternative attention mecha-\\nnisms discussed in Section 2.3. We specifically focus on analyzing each mechanisms adherence\\nto the generalized probability conditions and its corresponding performance, in order to highlight\\nthe significance of adhering these conditions. For a fair comparison, we re-implemented all alterna-\\n8\\nArxiv preprint version\\nTable 1: PTB experiment results of various attention mechanisms. Symbol * means that the initial\\nnumbers are trainable. X means the model does not follow that condition, that is unbounded\\ninfinite range or non-fixed total sum of normalized attention scores.\\nModel\\n# Param.\\nFinite Range of P\\nFixed Sum \\nPTB (PPL)\\nBaseline Transformer\\n29.2M\\n[0, 1]\\n1\\n108.26\\nNON (Richter & Wattenhofer, 2022)\\n29.2M\\nX\\nX\\n168.97\\nNAP (Richter & Wattenhofer, 2022)\\n29.2M\\nX\\n0*\\n258.85\\nCoDA (Tay et al., 2019)\\n29.2M\\n[-1, 1]\\nX\\n133.11\\nSin-Softmax (Wang et al., 2021)\\n29.2M\\n[0, 1]\\n1\\n123.04\\nValueSkipInit (He et al., 2023)\\n29.2M\\n[0, 2]*\\n2*\\n108.40\\nShaped (Noci et al., 2024)\\n29.2M\\n[- 1\\nT , 2- 1\\nT ]\\n1\\n109.06\\ndaGPAM (+ = 1.0, = 1.0)\\n29.6M\\n[-1, 2]\\n1\\n109.01\\ndaGPAM (Trainable s)\\n29.6M\\n[-1, 2]*\\n1*\\n106.38\\ntive attention mechanisms and applied them to the Transformer baseline, replacing the conventional\\nattention mechanism in the same manner as with daGPAM.\\nTable 1 presents the results of various attention mechanisms trained on the PTB LM task, evaluated\\nusing perplexity (PPL). Our daGPAM models resulted in approximately 1% increases in parame-\\nters. Each model is labeled to indicate whether it adheres to the generalized probability conditions,\\nspecifically finite range and a fixed total sum of normalized attention scores. The models which\\ndo not adhere to the conditions exhibit significant performance degradation. Conversely, with the\\nexception of Sin-Softmax, most models that adhered to both conditions performed similarly to the\\nbaseline Transformer. Although the ValueSkipInit and Shaped models did not degrade perfor-\\nmance, they also failed to provide improvements, because these methods regularize the attention\\nmatrix to be similar to the identity matrix, thereby weakening the contextualization effect. In con-\\ntrast, daGPAM enhances the attention mechanism itself, leading to improved performance in the\\ndaGPAM (Trainable s) model.\\n6\\nPRACTICAL BENEFITS IN BENCHMARK EXPERIMENTS\\nIn this section, we compare the performance of our models on several benchmark tasks. We con-\\nducted LM experiments on the Wikitext103 (word-level LM) and Enwiki8 (character-level LM)\\ndatasets. We followed the same data-related processes based on the same open-source of the PTB\\ndataset used in our preliminary experiment. For the baseline model, we re-implemented Trans-\\nformerXL (Dai, 2019), heavily following the public code5. This model is larger than the standard\\nTransformer and is optimized for long-context sentences. We followed the original congifurations of\\nmodel and optimization, except for modifications in the number of layers and the hyperparameters\\nrelated to daGPAM approach. For further details on the basic configurations, refer to (Dai, 2019).\\nSecond, we performed NMT experiments on the IWSLT14 English-German dataset (160K training\\npairs) and the WMT14 English-German dataset (3.9M training pairs) (Heo et al., 2024). The data\\npreprocessing steps, including tokenization and subword byte-pair encoding, were carried out using\\nthe Fairseq toolkit (Ott et al., 2019). For the baseline models, we re-implemented PreLN (Xiong\\net al., 2020) and Admin (Liu et al., 2020) encoder-decoder Transformer architectures, which are\\nwidely used NMT baselines nowadays. The basic model configurations and optimization settings are\\nprovided in Table 4 of Appendix A.2. Throughout our experiments with daGPAM models (including\\nmodels in LM task) using constant  values, we empirically determined the optimal combination of\\ns, which are reported in Tables 5 and 6 in Appendix A.2.\\n6.1\\nLANGUAGE MODELING\\nFor the evaluation of LM experiments, we utilized PPL for the Wikitext103 (word-level) task and\\nbits per character (BPC) for the Enwiki8 (character-level) task. The experiments were conducted by\\nvarying the number of layers in the TransformerXL model. For the TransformerXL baseline models,\\nthe number of parameters are (130.6M / 151.1M / 171.6M) and (10.3M / 41.1M) for Wikitext103\\n5https://github.com/kimiyoung/transformer-xl/\\n9\\nArxiv preprint version\\nTable 2: Experiments on wikitext103 and Enwiki8 LM tasks. daGPAM (Const) and daGPAM\\n(Train) mean daGPAM models with pre-defined constant s and trainable s, respectively.\\nModel\\nWikitext103\\nEnwiki8\\n8L\\n16L\\n24L\\n6L\\n12L\\nTransformerXL\\n25.96\\n23.58\\n22.90\\n1.1570\\n1.0544\\ndaGPAM (Const)\\n25.39\\n23.09\\n22.52\\n1.1534\\n1.0473\\ndaGPAM (Train)\\n25.33\\n23.20\\n22.57\\n1.1532\\n1.0528\\nTable 3: Experiments on IWSLT14 and WMT14 NMT tasks. daGPAM (Const) and daGPAM\\n(Train) were individually applied to the two architectures, respectively: PreLN (at the first row) and\\nAdmin (at the second row)\\nModel\\nIWSLT14\\nWMT14\\nEn-to-De\\nDe-to-En\\nEn-to-De\\nDe-to-En\\nPreLN\\n28.54\\n33.90\\n26.40\\n31.26\\ndaGPAM (Const)\\n29.25\\n34.43\\n26.73\\n31.43\\ndaGPAM (Train)\\n29.11\\n34.11\\n27.19\\n31.45\\nAdmin\\n28.32\\n33.48\\n26.27\\n30.61\\ndaGPAM (Const)\\n28.60\\n33.62\\n26.76\\n31.20\\ndaGPAM (Train)\\n28.43\\n33.23\\n26.79\\n31.10\\n(8L / 16L / 24L) and Enwiki8 (6L / 12L), respectively. In comparison, the number of parameters for\\ndaGPAM models are (130.7M / 151.3M / 172.0M) and (10.4M / 41.4M) for the tasks, respectively.\\nOn average, our approach increased only 0.43% of total parameters.\\nTable 2 demonstrates the results of LM experiments. It shows that daGPAM models consistently im-\\nprove performance across different layered model architectures. Specifically, our best-performing\\nmodels improve approximately 0.5 PPL on the Wikitext103 task and 0.0055 BPC on the Enwiki8\\ntask in average. Additionally, we conducted an analysis of the impact of different  combinations\\nwithin daGPAM model, with results detailed in Appendix A.3.4. Notably, we found that configu-\\nrations where  is close to 0.5 yielded the optimal performance in the Wikitext103 LM task. Also,\\nthe average  calculated from the trained  values in the daGPAM (Train) (8L) is 0.3948, with\\n+ = 0.3303 and = 0.9355.\\n6.2\\nNEURAL MACHINE TRANSLATION\\nFor the evaluation of NMT experiments, we utilized case-sensitive SacreBLEU (Post, 2018). For the\\nbaseline models, the number of parameters are 64.67M and 153.84M for the IWSLT14 and WMT14\\ntasks, respectively. In comparison, the number of parameters for daGPAM models are 65.26M and\\n155.02M parameters, which is around 0.84% addition in average. Table 3 presents the results of the\\nNMT experiments. Similar to the results of the LM experiments, daGPAM models consistently out-\\nperformed the baseline models. Specifically, our best-performing models demonstrated, in average,\\n0.42 BLEU point improvement for the IWSLT14 task and 0.52 BLEU points for the WMT14 task.\\n7\\nCONCLUSION\\nIn this paper, we proposed a novel class of attention mechanism, generalized probabilistic attention\\nmechanism (GPAM), which allows the negative attention score during the information processing.\\nAlso, we proposed one specific type of GPAM, dual-attention GPAM (daGPAM). While showing\\nthat the rank-collapse and gradient vanishing problems in the conventional attention mechanism is\\nin trade-off relationship, we showed that daGPAM could mitigate both problems. Our empirical\\nvalidations provide strong evidences supporting our theories and understanding of this approach.\\nAdditionally, our benchmark experiments demonstrate meaningful performance improvements with\\nonly a minimal increase in the number of parameters.\\n10\\nArxiv preprint version\\n8\\nFUTURE WORKS\\nAlthough we proposed only one structure of GPAM (daGPAM), GPAM does not have to be lim-\\nited to the structure. Actually, the operation of daGPAM increases computational cost due to the\\nadded scaled dot-product attention process of negative part. In future, we aim to develop a more\\nefficient structure than daGPAM while preserving the feature of GPAM. Also, beyond the standard\\nTransformer architecture discussed here, GPAM can be applied to other architectures, such as graph\\nneural networks and vision Transformer, as they are also known to experience the rank-collapse\\nproblem.\\nACKNOWLEDGMENTS\\nREFERENCES\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. Multi-head attention: Collaborate\\ninstead of concatenate. arXiv preprint arXiv:2006.16362, 2020.\\nZihang Dai.\\nTransformer-xl: Attentive language models beyond a fixed-length context.\\narXiv\\npreprint arXiv:1901.02860, 2019.\\nP. A. M. Dirac. Bakerian lecture - the physical interpretation of quantum mechanics. Proceedings\\nof the Royal Society of London. Series A. Mathematical and Physical Sciences, 180:140, 1942.\\ndoi: 10.1098/rspa.1942.0023.\\nYihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: Pure\\nattention loses rank doubly exponentially with depth. In International Conference on Machine\\nLearning, pp. 27932803. PMLR, 2021.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\\nscale, 2021.\\nRuili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank\\ndiminishing in deep neural networks. Advances in Neural Information Processing Systems, 35:\\n3305433065, 2022.\\nRichard Phillips Feynman. Negative probability. Technical report, PRE-27827, 1984.\\nChengyue Gong, Dilin Wang, Meng Li, Vikas Chandra, and Qiang Liu. Vision transformers with\\npatch diversification. arXiv preprint arXiv:2104.12753, 2021.\\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo\\nWang, Zhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer\\nfor speech recognition. arXiv preprint arXiv:2005.08100, 2020.\\nXiaojun Guo, Yifei Wang, Tianqi Du, and Yisen Wang. Contranorm: A contrastive learning per-\\nspective on oversmoothing and beyond. arXiv preprint arXiv:2303.06562, 2023.\\nSoufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the impact of the activation function on\\ndeep neural networks training. In International conference on machine learning, pp. 26722680.\\nPMLR, 2019.\\nBobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andrew Brock, Samuel L Smith,\\nand Yee Whye Teh. Deep transformers without shortcuts: Modifying self-attention for faithful\\nsignal propagation. arXiv preprint arXiv:2302.10322, 2023.\\n11\\nArxiv preprint version\\nDongNyeong Heo, Daniela Noemi Rim, and Heeyoul Choi. N-gram prediction and word difference\\nrepresentations for language modeling. arXiv preprint arXiv:2409.03295, 2024.\\nTianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On feature\\ndecorrelation in self-supervised learning. In Proceedings of the IEEE/CVF International Confer-\\nence on Computer Vision, pp. 95989608, 2021.\\nArthur Jacot. Implicit bias of large depth networks: a notion of rank for nonlinear functions. In The\\nEleventh International Conference on Learning Representations, 2023.\\nWei Jin, Xiaorui Liu, Yao Ma, Charu Aggarwal, and Jiliang Tang. Feature overcorrelation in deep\\ngraph neural networks: A new perspective. arXiv preprint arXiv:2206.07743, 2022.\\nLi Jing, Pascal Vincent, Yann LeCun, and Yuandong Tian. Understanding dimensional collapse in\\ncontrastive self-supervised learning. arXiv preprint arXiv:2110.09348, 2021.\\nQimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks\\nfor semi-supervised learning. In Proceedings of the AAAI conference on artificial intelligence,\\nvolume 32, 2018.\\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei\\nHan. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,\\n2019.\\nLiyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, and Jiawei Han. Understanding the diffi-\\nculty of training transformers. arXiv preprint arXiv:2004.08249, 2020.\\nMitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated corpus\\nof english: The penn treebank. Computational linguistics, 19(2):313330, 1993.\\nLorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien\\nLucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\\nAdvances in Neural Information Processing Systems, 35:2719827211, 2022.\\nLorenzo Noci, Chuning Li, Mufan Li, Bobby He, Thomas Hofmann, Chris J Maddison, and Dan\\nRoy. The shaped transformer: Attention models in the infinite depth-and-width limit. Advances\\nin Neural Information Processing Systems, 36, 2024.\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint\\narXiv:1904.01038, 2019.\\nBen Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-\\ntial expressivity in deep neural networks through transient chaos. Advances in neural information\\nprocessing systems, 29, 2016.\\nMatt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference\\non Machine Translation: Research Papers, pp. 186191, Belgium, Brussels, October 2018. As-\\nsociation for Computational Linguistics. URL https://www.aclweb.org/anthology/\\nW18-6319.\\nOliver Paul Richter and Roger Wattenhofer. Normalized attention without probability cage, 2022.\\nURL https://openreview.net/forum?id=PeG-8G5ua3W.\\nAndreas Roth and Thomas Liebig. Rank collapse causes over-smoothing and over-correlation in\\ngraph neural networks. In Learning on Graphs Conference, pp. 351. PMLR, 2024.\\nSamuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information\\npropagation. arXiv preprint arXiv:1611.01232, 2016.\\nGabor J Szekely. Half of a coin: negative probabilities. Wilmott Magazine, 50:6668, 2005.\\nYehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented\\nshortcuts for vision transformers. Advances in Neural Information Processing Systems, 34:15316\\n15327, 2021.\\n12\\nArxiv preprint version\\nYi Tay, Anh Tuan Luu, Aston Zhang, Shuohang Wang, and Siu Cheung Hui. Compositional de-\\nattention networks. Advances in Neural Information Processing Systems, 32, 2019.\\nNaftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In\\n2015 ieee information theory workshop (itw), pp. 15. IEEE, 2015.\\nA Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017.\\nAndreas Veit, Michael J Wilber, and Serge Belongie. Residual networks behave like ensembles of\\nrelatively shallow networks. Advances in neural information processing systems, 29, 2016.\\nPeihao Wang, Wenqing Zheng, Tianlong Chen, and Zhangyang Wang. Anti-oversmoothing in deep\\nvision transformers via the fourier domain analysis: From theory to practice. arXiv preprint\\narXiv:2203.05962, 2022.\\nShulun Wang, Feng Liu, and Bin Liu. Escaping the gradient vanishing: Periodic alternatives of\\nsoftmax in attention mechanism. IEEE Access, 9:168749168759, 2021.\\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\\nYanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\\nIn International Conference on Machine Learning, pp. 1052410533. PMLR, 2020.\\nHanqi Yan, Lin Gui, Wenjie Li, and Yulan He. Addressing token uniformity in transformers via\\nsingular value transformation. In Uncertainty in artificial intelligence, pp. 21812191. PMLR,\\n2022.\\nGe Yang and Samuel Schoenholz. Mean field residual networks: On the edge of chaos. Advances\\nin neural information processing systems, 30, 2017.\\nBiao Zhang, Ivan Titov, and Rico Sennrich. Improving deep transformer with depth-scaled initial-\\nization and merged attention. arXiv preprint arXiv:1908.11365, 2019.\\nGuodong Zhang, Aleksandar Botev, and James Martens. Deep learning without shortcuts: Shaping\\nthe kernel with tailored rectifiers. arXiv preprint arXiv:2203.08120, 2022.\\nDaquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang Jiang, Qibin Hou,\\nand Jiashi Feng. Deepvit: Towards deeper vision transformer. arXiv preprint arXiv:2103.11886,\\n2021.\\nA\\nAPPENDIX\\nA.1\\nPROOFS OF LEMMAS\\nA.1.1\\nFULL DESCRIPTION OF LEMMA 1\\nIn this section, we describe the complete version of the simplified lemma 1. For the proof of this\\nlemma, see (Dong et al., 2021).\\nLemma 6 (Dong et al. (2021)). For any single scaled dot-product self-attention layer that holds\\n|Eij Eij| 1.256 for any (i, j, j\\n) where E = res(X) WQK\\n\\ndqk res(X), and with  that satisfies\\nrPT\\ni=1 max\\nj,j |Eij Eij| \\nr\\nmax\\nj,j\\nPT\\ni=1 |Eij Eij|, the composite norm of residual of its\\noutput is bounded by\\nres(Y)1,4\\n\\n2WQK1WV 1,\\np\\ndqk\\nres(X)3\\n1,,\\n(14)\\nwhere WQK = WQW\\nK. In the region that holds 4\\n\\n2WQK1WV 1,<\\np\\ndqk, the output\\nresidual norm is diminished compared to the cubic rate of input residual norm.\\n13\\nArxiv preprint version\\nA.1.2\\nPROOF OF LEMMA 3\\nIn this section, we prove our proposed Lemma 3 which was introduced as follows:\\nLemma 7 (Maximum Total Norm of Gradients). The total norm of gradients, G(Pi), is maximized\\nwhen Pi is uniform distribution, that is Pi = [ 1\\nT , 1\\nT ,    , 1\\nT ] which is the case of complete rank-\\ncollapse.\\nProof. Based on Eq.6, G(Pi) is derived as follows:\\nG(Pi) =\\nT\\nX\\nj=1\\nT\\nX\\nk=1\\n\\x0c\\x0c\\x0c\\x0c\\nPik\\nAij\\n\\x0c\\x0c\\x0c\\x0c =\\nT\\nX\\nj=1\\n\\uf8eb\\n\\uf8ed\\nT\\nX\\nk=1,k=j\\n| PikPij| + Pij(1 Pij)\\n\\uf8f6\\n\\uf8f8,\\n=\\nT\\nX\\nj=1\\n T\\nX\\nk=1\\nPikPij + Pij(1 2Pij)\\n!\\n,\\n=\\nT\\nX\\nj=1\\nPij\\nT\\nX\\nk=1\\nPik +\\nT\\nX\\nj=1\\nPij 2\\nT\\nX\\nj=1\\n(Pij)2,\\n= 2 2\\nT\\nX\\nj=1\\n(Pij)2.\\n(15)\\nThen, our goal is to find the input probability distribution, Pi, that maximizes Eq.15, with constraint\\nPT\\nk=1 Pik = 1. We use Lagranges multiplier method to optimize this constrained maximization\\nproblem. With Lagrange coefficient, , we derive the new objective to maximize: L(Pi, ) = 2 \\n2 PT\\nj=1(Pij)2 (PT\\nk=1 Pik 1). The Jacobian of L(Pi, ) with respect to (Pi, ) is formulated\\nas follows:\\nJL =\\nh\\nL\\nPi1\\n  \\nL\\nPiT\\nL\\n\\ni\\n=\\n\\x02\\n(4Pi1 )\\n  \\n(4PiT )\\n(PT\\nk=1 Pik + 1)\\n\\x03\\n.\\nWith setting the Jacobian to zero, the solutions are Pij = \\n4 and  = 4\\nT . Thus, an optimal value\\nof G(Pi) is achieved when P\\ni = [ 1\\nT , 1\\nT ,    , 1\\nT ].\\nTo verify this point is the maximum, we compare G(P\\ni ) with the total magnitude given slightly\\nnoised probabilities, Px,y\\ni\\nwhose x-th probability is 1\\nT + , and y-th probability is 1\\nT , where\\nx, y < T and 0 <  < 1\\nT . The two values are obtained as follows:\\nG(P\\ni ) = 2 2\\nT\\nX\\nj=1\\n\\x12 1\\nT\\n\\x132\\n= 2 2 1\\nT ,\\nG(Px,y\\ni\\n) = 2 2\\n\\uf8eb\\n\\uf8ed\\nT\\nX\\nj=1,j=x,y\\n\\x12 1\\nT\\n\\x132\\n+\\n\\x12 1\\nT + \\n\\x132\\n+\\n\\x12 1\\nT \\n\\x132\\n\\uf8f6\\n\\uf8f8,\\n= 2 2\\n\\uf8eb\\n\\uf8ed\\nT\\nX\\nj=1\\n\\x12 1\\nT\\n\\x132\\n+ 22\\n\\uf8f6\\n\\uf8f8,\\n= 2 2 1\\nT 42 < G(P\\ni ).\\nWe can get the same result with any different combination of x and y. Analogously, adding (and\\nsubtracting) multiple i outputs lower than the optimal value. Therefore, G(P\\ni ) is the maximum.\\nA.1.3\\nFULL DESCRIPTION AND PROOF OF LEMMA 4\\nLemma 8 (Dual-Attention GPAM residual Bound, Completed). Based on the constraints\\nof Lemma 6 that are similarly applied to both positive/negative attention parts and jointly\\n14\\nArxiv preprint version\\nrPT\\ni=1 max\\nj,j |E+\\nij E+\\nij| max\\nj,j |E\\nij E\\nij| \\n\\n2\\nr\\nmax\\nj1,j\\n1,j2,j\\n2\\nPT\\ni=1 |E+\\nij1 E+\\nij\\n1||E\\nij2 E\\nij\\n2|,\\nthe output residuals composite norm of any single daGPAM self-attention layer is bounded by\\nres(Y)1,\\n4\\n\\n2\\n\\x10\\nW+\\nQK1 +\\n\\x0c\\x0c\\x0c+W+\\nQK1 W\\nQK1\\n\\x0c\\x0c\\x0c\\n\\x11\\nWV 1,\\np\\ndqk\\nres(X)3\\n1,,\\n= Borg +\\n4\\n\\n2\\n\\x10\\x0c\\x0c\\x0c+W+\\nQK1 W\\nQK1\\n\\x0c\\x0c\\x0c\\n\\x11\\nWV 1,\\np\\ndqk\\nres(X)3\\n1,, (16)\\nwhere W+\\nQK = W+\\nQ(W+\\nK)and W\\nQK = (W+\\nQW\\nQ)(W+\\nK)with assumption that the non-\\nlinear ReLU activation  is identity. Borg is the upper bound derived by Lemma 6. Since the second\\nterm is positive, this upper bound is always greater than the original.\\nProof. In this proof, we follow the derivations in (Dong et al., 2021), except the triangle inequality\\nfor the Frobenius norm formulation in the middle.\\nBy the technique described in Sec.A.4.2, the positive and negative unnormalized attention matrices,\\nEqs.9 and 10, are approximated as follows:\\nA+ \\n1\\np\\ndqk\\n\\x10\\nRW+\\nQKR+ 1xW+\\nQKR\\x11\\n,\\nA\\n1\\np\\ndqk\\n\\x10\\nRW\\nQKR+ 1xW\\nQKR\\x11\\n,\\nwhere W+\\nQK = W+\\nQ(W+\\nK)and W\\nQK = (W+\\nQW\\nQ)(W+\\nK)with assumption that the non-\\nlinear ReLU activation  is identity, and R = res(X) = X 1x. Then, the positive and negative\\nnormalized attention score matrices are formulated as follows:\\nP+ = softmax\\n\\x00E+ + 1(r+)\\x01\\n,\\nP= softmax\\n\\x00E+ 1(r)\\x01\\n,\\nwhere E+ =\\n1\\n\\ndqk RW+\\nQKR, E=\\n1\\n\\ndqk RW\\nQKR, r+ =\\n1\\n\\ndqk R(W+\\nQK)x, and r=\\n1\\n\\ndqk R(W\\nQK)x.\\nFollowing the technique described in Sec.A.4.3, each normalized attention matrix is lower and upper\\nbounded as follows:\\n\\x00I 2D+\\x01\\n1softmax(r+)P+ \\n\\x00I + 2D+\\x01\\n1softmax(r+),\\n\\x00I 2D\\x01\\n1softmax(r)P\\n\\x00I + 2D\\x01\\n1softmax(r),\\nwhere D is a diagonal matrix whose i-th diagonal element is Dii = max\\nj,j |Eij Eij|. Based on the\\ndesign of daGPAM, Eq.8, PG = (1 + +)P+ P, its lower and upper bound is formulated as\\nfollows:\\n(1 + +)\\n\\x00I 2D+\\x01\\n1softmax(r+)\\x00I 2D\\x01\\n1softmax(r)\\nPG \\n(1 + +)\\n\\x00I + 2D+\\x01\\n1softmax(r+)\\x00I + 2D\\x01\\n1softmax(r).\\n15\\nArxiv preprint version\\nNow, the output of the single daGPAM self-attention layer, Y = PGXV = PGXWV , is derived\\nas follows:\\nY = PGXWV ,\\n= PG(1x+ R)WV ,\\n= PG1xWV + PGRWV ,\\n= 1xWV + PGRWV ,\\n1xWV + [(1 + +)\\n\\x00I + 2D+\\x01\\n1softmax(r+)\\n\\x00I + 2D\\x01\\n1softmax(r)]RWV ,\\n=\\n\\x001x+ (1 + +)1softmax(r+)R 1softmax(r)R\\n\\x01\\nWV\\n+ 2\\n\\x00(1 + +)D+1softmax(r+)D1softmax(r)\\x01\\nRWV ,\\nY 1(r)2\\n\\x00(1 + +)D+1softmax(r+)D1softmax(r)\\x01\\nRWV ,\\nwhere r = W\\nV\\n\\x00x + (1 + +)Rsoftmax(r+) Rsoftmax(r)\\n\\x01\\n.\\nAnalogously, the\\nlower bound of (Y 1(r)) is given by\\nY 1(r)2\\n\\x00(1 + +)D+1softmax(r+)D1softmax(r)\\x01\\nRWV .\\nFollowing the definition of (Dong et al., 2021), we can see (Y 1(r)) is an appropriate approxi-\\nmation of res(Y) = R\\n with assuming r arg minx Y 1x.\\nBased on the triangle inequality, the upper bound of the Frobenius norm of R\\n, that is R\\nF , is\\nobtained by\\nR\\nF 2\\n\\r\\r\\x00(1 + +)D+1softmax(r+)D1softmax(r)\\x01\\r\\r\\nF RF WV F ,\\n= 2\\nq\\n(1 + +)2D+1softmax(r+)2\\nF + ()2D1softmax(r)2\\nF\\n+2(1 + +)D+1softmax(r+), D1softmax(r)F\\nq\\nR2\\nF\\nq\\nWV 2\\nF\\n2\\np\\n(1 + +)2D+11D+1+ ()2D11D1\\n2(1 + +)D+1softmax(r+), D1softmax(r)F R1,WV 1,,\\n(17)\\nwhere  1,=\\np\\n 1 \\np\\n 2\\nF by the Schatten norm inequality. The last inequality is\\nobtained by the upper bounds described in Sec. A.4.4. , F is the Frobenius inner product.\\nBased on the derived upper bound described in Sec.A.4.5, we derive following inequalities:\\nD+11D+182E+2\\n1,\\nD11D182E2\\n1,\\nwhere\\n\\njointly\\nholds\\nthe\\ntwo\\ninitial\\nconditions\\nrPT\\ni=1 max\\nj,j |E+\\nij E+\\nij|\\n\\n\\nr\\nmax\\nj,j\\nPT\\ni=1 |E+\\nij E+\\nij| and\\nrPT\\ni=1 max\\nj,j |E\\nij E\\nij| \\nr\\nmax\\nj,j\\nPT\\ni=1 |E\\nij E\\nij|.\\n16\\nArxiv preprint version\\nThe Frobenius inner product term in the inequality is further derived as follows:\\nD+1softmax(r+),D1softmax(r)F =\\nT\\nX\\ni\\nT\\nX\\nj\\nD+\\niisoftmax(r+)jD\\niisoftmax(r)j,\\n=\\n T\\nX\\ni\\nD+\\niiD\\nii\\n! \\uf8eb\\n\\uf8ed\\nT\\nX\\nj\\nsoftmax(r+)jsoftmax(r)j\\n\\uf8f6\\n\\uf8f8,\\n\\n T\\nX\\ni\\nD+\\niiD\\nii\\n! \\uf8eb\\n\\uf8ed1\\nT\\nX\\nj\\nsoftmax(r)j\\n\\uf8f6\\n\\uf8f8,\\n=\\nT\\nX\\ni\\nD+\\niiD\\nii,\\n=\\nT\\nX\\ni\\nmax\\nj,j |E+\\nij E+\\nij| max\\nj,j |E\\nij E\\nij|,\\n22\\nmax\\nj1,j\\n1,j2,j\\n2\\nT\\nX\\ni\\n|E+\\nij1 E+\\nij\\n1||E\\nij2 E\\nij\\n2|,\\n82 max\\nj1,j2\\nT\\nX\\ni\\n|E+\\nij1||E\\nij2|,\\n82\\n \\nmax\\nj\\nT\\nX\\ni\\n|E+\\nij|\\n!  \\nmax\\nj\\nT\\nX\\ni\\n|E\\nij|\\n!\\n,\\n= 82E+1E1.\\nThe inequality from 5-th to 6-th line is based on the initial condition of this lemma. The techniques\\nused for the latter inequalities are similar to the techniques used in Sec. A.4.5.\\nIncorporating above derivations to Eq.17, R\\nF is obtained by\\nR\\nF 4\\n\\n2\\n\\x12q\\n(1 + +)2E+2\\n1 2(1 + +)E+1E1 + ()2E2\\n1\\n\\x13\\nR1,WV 1,,\\n= 4\\n\\n2\\n\\x00\\x0c\\x0c((1 + +)E+1 E1)\\n\\x0c\\x0c\\x01\\nR1,WV 1,,\\n(18)\\nBy substituting E+ and Ewith their original form, respectively, we achieve the result of Lemma\\nas follows:\\nR\\nF 4\\n\\n2\\n \\x0c\\x0c\\x0c\\x0c\\x0c(1 + +)\\n1\\np\\ndqk\\nRW+\\nQKR1 \\n1\\np\\ndqk\\nRW\\nQKR1\\n\\x0c\\x0c\\x0c\\x0c\\x0c\\n!\\nR1,WV 1,,\\n4\\n\\n2\\np\\ndqk\\n\\x10\\x0c\\x0c\\x0c(1 + +)R1W+\\nQK1R1 R1W\\nQK1R1\\n\\x0c\\x0c\\x0c\\n\\x11\\nR1,WV 1,,\\n= 4\\n\\n2\\np\\ndqk\\n\\x10\\x0c\\x0c\\x0c(1 + +)R1W+\\nQK1RR1W\\nQK1R\\n\\x0c\\x0c\\x0c\\n\\x11\\nR1,WV 1,,\\n= 4\\n\\n2\\np\\ndqk\\n\\x10\\x0c\\x0c\\x0c(1 + +)W+\\nQK1 W\\nQK1\\n\\x0c\\x0c\\x0c\\n\\x11\\nR3\\n1,WV 1,,\\n4\\n\\n2\\np\\ndqk\\n\\x10\\nW+\\nQK1 +\\n\\x0c\\x0c\\x0c+W+\\nQK1 W\\nQK1\\n\\x0c\\x0c\\x0c\\n\\x11\\nR3\\n1,WV 1,.\\n17\\nArxiv preprint version\\nA.1.4\\nPROOF OF LEMMA 5\\nIn this section, we prove our proposed Lemma 5 which was introduced as follows:\\nLemma 9 (Dual-Attention GPAM Gradients). The gradient that the input unnormalized attention\\nscore, A, receives through the normalized attention score, PG (without 1/\\np\\ndqk) is derived as\\nfollows:\\nPG\\nij\\nAij\\n= (1 + +)P+\\nij(1 P+\\nij) + P\\nij(1 P\\nij),\\n= gorg\\nj\\n+ +P+\\nij(1 P+\\nij) + P\\nij(1 P\\nij),\\n(19)\\nPG\\nik,k=j\\nAij\\n= (1 + +)(P+\\nikP+\\nij) + (P\\nikP\\nij),\\n= gorg\\nk\\n+ +(P+\\nikP+\\nij) + (P\\nikP\\nij),\\n(20)\\nwhere gorg\\nj\\nand gorg\\nk\\nare the derived gradient of the conventional attention mechanism, Eq.6, re-\\nspectively.\\nProof. Basically, the final gradients are simply formulated as follows:\\nPG\\nij\\nAij\\n= (1 + +)\\nP+\\nij\\nAij\\nP\\nij\\nAij\\n,\\nPG\\nik,k=j\\nAij\\n= (1 + +)\\nP+\\nik,k=j\\nAij\\nP\\nik,k=j\\nAij\\n.\\nWe use the results of the derived gradients in Lemma 2 for the partial gradients of positive parts,\\nP+\\nij\\nAij and\\nP+\\nik,k=j\\nAij\\n. The partial gradients of negative parts are derived as follows (W is the same as\\nW\\nQ that we simplified as negative identity matrix):\\nP\\nij\\nAij\\n= Wjj\\neWjjA+\\nij\\nPn\\nt=1 eWttA+\\nit \\neWjjA+\\nij\\n(Pn\\nt=1 eWttA+\\nit)2\\n n\\nX\\nz=1\\nWzjeWzzA+\\niz\\n!\\n,\\n= WjjP\\nij P\\nij\\n n\\nX\\nz=1\\nWzjP\\niz\\n!\\n,\\n= WjjP\\nij(1 P\\nij),\\n= P\\nij(1 P\\nij),\\nP\\nik,k=j\\nAij\\n= Wkj\\neWkjA+\\nij\\nPn\\nt=1 eWttA+\\nit \\neWkjA+\\nij\\n(Pn\\nt=1 eWttA+\\nit)2\\n n\\nX\\nz=1\\nWzjeWzzA+\\niz\\n!\\n,\\n= WkjP\\nik P\\nik\\n n\\nX\\nz=1\\nWzjP\\niz\\n!\\n,\\n= Wjj(P\\nikP\\nij),\\n= (P\\nikP\\nij),\\nDuring derivations, we used the simplification that Wij = 0 if i = j and Wii = 1. Finally, the\\nlemma is proved by substituting\\nP+\\nij\\nAij ,\\nP+\\nik,k=j\\nAij\\n,\\nP\\nij\\nAij , and\\nP\\nik,k=j\\nAij\\n, from the above formulations\\nwith the newly derived formulations.\\nA.2\\nEXPERIMENTAL SETTINGS\\nIn this section, we describe the experimental settings of our experiments. Except the benchmark LM\\nexperiments (Section 6.1), we used the described model and optimization configurations in Table 4\\n18\\nArxiv preprint version\\nTable 4: Model and optimizer configurations used for our experiments. We tried to use the same\\nnotations with (Vaswani, 2017), except the number of layers (# of Layers) and multi-head attentions\\nheads (# of Heads) for clarity. RAdam means rectified Adam (Liu et al., 2019). ISRS indicates\\ninverse square root learning rate schedule (Ott et al., 2019) and # of Tokens indicates the number\\nof tokens in a mini-batch at every iteration.\\nConfig.\\nPTB\\nIWSLT14\\nWMT14\\nTransformer\\nPreLN\\nAdmin\\nPreLN\\nAdmin\\ndmodel\\n256\\n512\\n512\\n512\\n512\\ndff\\n2100\\n1024\\n1024\\n2048\\n2048\\ndqk\\n64\\n64\\n64\\n64\\n64\\nPdrop\\n0.3\\n0.3\\n0.3\\n0.1\\n0.1\\nls\\n0.1\\n0.1\\n0.1\\n0.1\\n0.1\\n# of Layers\\n15\\n6\\n6\\n6\\n6\\n# of Head\\n4\\n4\\n4\\n8\\n8\\nOptimizer\\nRAdam\\nRAdam\\nRAdam\\nRAdam\\nRAdam\\nLearning Rate\\n0.00025\\n0.0005\\n0.0005\\n0.001\\n0.001\\nScheduler\\nISRS\\nNone\\nISRS\\nNone\\nISRS\\n# of Tokens\\n4K\\n4K\\n4K\\n25K\\n25K\\nPatience\\n50\\n50\\n50\\n50\\n50\\nTable 5: Empirically found optimal  combination for our daGPAM model with constant s in LM\\nexperiments.\\nModel\\nWikitext103\\nEnwiki8\\n8L\\n16L\\n24L\\n6L\\n12L\\n(+, )\\n(1.0, 1.5)\\n(1.5, 2.0)\\n(1.0, 1.0)\\n(1.0, 1.5)\\n(1.0, 1.0)\\nfor preliminary experiments (Section 5) and the benchmark NMT experiments (Section 6.2). For the\\nexperiments of benchmark LM experiments (Section 6.1), we followed the same experiment settings\\nas the original work (Dai, 2019), except the different number of layers and other things related to\\nour proposed daGPAM. For the settings of the optimal  combinations that we used for daGPAM\\n(constant  setting) models, we demonstrated them in Tables5 and 6.\\nDuring training of all experiments, we saved the best checkpoint based on validations. Especially,\\nfor NMT experiments, IWSLT14 and WMT14, we used checkpoint ensemble technique (Ott et al.,\\n2019) with 10 latest checkpoints at each validation. Except the benchmark LM experiments, we\\nearly stopped the training whenever the model does not over the previous best performance Pa-\\ntience times. For the experiments of benchmark LM experiments, we ran experiments with the\\noriginal works default training iteration settings.\\nWe utilized single GTX1080Ti GPU for all of the preliminary experiments (Section 5). The PTB\\nLM experiments took 12 hours in average. For all of our benchmark experiments (Section 6), we\\nutilized single RTX3090 GPU. The TransformerXL-based LM experiments took averagely 20 and\\n240 hours for Wikitext-103 and Enwiki8 tasks, respectively. The NMT experiments took 75 and 120\\nhours in average for IWSLT14 and WMT14 tasks, respectively.\\nTable 6: Empirically found optimal  combination for our daGPAM model with constant s in NMT\\nexperiments.\\nModel\\nIWSLT14 En-to-De\\nIWSLT14 De-to-En\\nWMT14 En-to-De\\nWMT14 De-to-En\\nPreLN\\nAdmin\\nPreLN\\nAdmin\\nPreLN\\nAdmin\\nPreLN\\nAdmin\\n(+, )\\n(1.5, 1.5)\\n(2.0, 1.5)\\n(1.0, 1.5)\\n(2.0, 1.5)\\n(1.0, 1.0)\\n(1.0, 1.0)\\n(1.5, 1.5)\\n(1.0, 1.0)\\n19\\nArxiv preprint version\\nFigure 5: Results of faithfulness test (rank-collapse analysis at initialization) varying + with fixing\\nto 1.\\nFigure 6: Results of rank-collapse analysis after training with MLP layers output representations.\\nA.3\\nADDITIONAL EXPERIMENT RESULTS\\nA.3.1\\nFAITHFULNESS TEST WITH VARYING +\\nIn addition to the rank-collapse analysis at initialization that is conducted with varying while\\nfixing + to 1 (Fig.4 in Section 5.1), we conducted the same analysis with varying + while fixing\\nto 1. As demonstrated in Fig.5, daGPAM models achieved less intensive rank-reduciton tendency\\nthan the baseline. Similar to the phenomenon explained in Section 5.1, daGPAM model that has \\nsmaller than 1 shows much less intensive tendency. For example, the (+ = 0.5, = 1.0)\\nconfiguration shows the least intensive tendency, and this tendency is quite similar to the tendency\\nof (+ = 1.0, = 1.5) configuration in Fig.4.\\nA.3.2\\nRANK-COLLAPSE ANALYSIS OF MLP LAYERS OUTPUT REPRESENTATIONS\\nIn addition to the rank-collapse analysis after training based on attention layers output represen-\\ntations (Fig.4 in Section 5.1), we conducted the same analysis based on the MLP layers output\\nrepresentations which are the final outputs of each Transformer block. As demonstrated in Fig.6,\\ndaGPAM models show mitigated rank-collapse phenomenon compared to the baseline, similar to\\nthe resulting phenomenon of attention layers outputs.\\n20\\nArxiv preprint version\\nFigure 7:\\nCosine similarity results between the representations produced by the attention layer\\nand the subsequent residual connection, relative to the initial representations. This cosine similarity\\nserves as an indicator of the impact of the attention layer in comparison to the shortcut branch within\\nthe residual connection.\\nFigure 8: PPL results of daGPAM models (based on 8-layered TransformerXL) with various con-\\nstant s were trained on Wikitext103 LM task. The darker the color is, the better PPL is.\\nA.3.3\\nINFLUENCE OF ATTENTION LAYERS\\nTo assure that our proposed model develops the attention layer rather than enhancing the shortcut\\nbranch like some previous works (Noci et al., 2022), we measured the cosine similarity between\\nresidual connections output representations and the initial representations (word embedding vec-\\ntors). This cosine similarity means the influence of attention layer within the residual connection.\\nAs demonstrated in Fig.7, we found that daGPAM models usually achieve less cosine similarity\\nwhich means the attention layers output representations are more diverse compared to the base-\\nlines. Based on this analysis, we believe that daGPAM actually affects the attention layer rather\\nthan enhancing shortcut branch.\\nA.3.4\\nWIKITEXT103 LM EXPERIMENTS WITH VARIOUS S\\nTo provide profound understanding on the effects of s in practice, we conducted more Wikitext103\\nLM experiments of daGPAM models based on 8-layered TransformerXL architecture with varying\\neach  from 0.5 to 3.0 with 0.5 interval. Note that the total sum of normalized attention score is  =\\n(1++). We discovered that, at each value of +, increasing improves performance until it\\nmakes  lower than -2.0. Usually, the best performance was achieved when  is 0.5. Interestingly,\\nwe found that, when  is 0.0, performance instantly drops. We understand the 0.0  can overly\\n21\\nArxiv preprint version\\namplify the direct movement on the hyperplane, , while eliminate the effect original point Y+ as\\nwe explained in Section 4.2. This may cause too diverse output representations. Empirically, we\\nfound that the cases when  is close to 0.5, usually be the optimal setting for daGPAM models.\\nA.4\\nTECHNICAL LEMMAS FROM (DONG ET AL., 2021)\\nA.4.1\\nPROPERTIES OF MATRIX NORM\\nIn this section, we describe the formulation and properties of matrix norm which are frequently\\nused in our proof derivations. We note that L-1 and L-norms of a T  d matrix, M, which are\\nformulated as follow:\\nM1 = max\\n1id\\n\\uf8eb\\n\\uf8ed\\nT\\nX\\nj=1\\n|Mji|\\n\\uf8f6\\n\\uf8f8,\\nM= max\\n1iT\\n\\uf8eb\\n\\uf8ed\\nd\\nX\\nj=1\\n|Mij|\\n\\uf8f6\\n\\uf8f8.\\nNoticeably, M1 = M.\\nIf there is a matrix N which is multiplicative with M, then\\nMNMN. This property holds for L-1 and L-norms.\\nA.4.2\\nSIMPLIFICATION OF UNNORMALIZED ATTENTION MATRIX\\nGiven the definition of the unnormalized attention matrix A, Eq.3, A is related to the residual, Eq.4,\\nas follows:\\nA =\\n1\\np\\ndqk\\n(R + 1x)WQK(R + 1x)\\n=\\n1\\np\\ndqk\\n\\x00RWQKR+ RWQK x1+ 1xWQKR+ 1xWQK x1\\x01\\n,\\nwhere WQK = WQW\\nK and R = res(X) = X 1x. By following the shift invariance\\nproperty of the softmax function (Cordonnier et al., 2020) which means a constant term added to\\nevery element of a row is negligible, we approximate A as follows:\\neA \\n1\\np\\ndqk\\n\\x00RWQKR+ 1xWQKR\\x01\\n.\\nNote that every eliminated (by the approximation) terms has the form, c1, so that we can ex-\\npress A = eA + c\\n1where the vector, c\\n, is the summation of every eliminated terms without the\\ncommon factor, 1. Then, the output of the softmax function with A as input is softmax(A) =\\nsoftmax( eA + c\\n1) = softmax( eA). Therefore, we can safely use the approximated unnormal-\\nized attention matrix instead of the original during the attention mechanism.\\nA.4.3\\nBOUNDS OF NORMALIZED ATTENTION MATRIX\\nWith incorporating the approximation of the unnormalized attention matrix to the definition of the\\nnormalized attention matrix, Eq.2 (without considerations of positive and negative signs), we can\\nformulate the normalized attention matrix as follows:\\nP = softmax(\\n1\\np\\ndqk\\nA) = softmax\\n\\x001r+ E\\n\\x01\\n,\\nwhere E =\\n1\\n\\ndqk RWQKRand r =\\n1\\n\\ndqk R(WQK)x.\\nThe (i, j)-th element of P is derived as follow:\\nPij =\\nexp\\n\\x00(1r)ij + Eij\\n\\x01\\nPT\\nt=1 exp ((1r)it + Eit)\\n=\\nexp\\n\\x00(1r)ij\\n\\x01\\nexp (Eij)\\nPT\\nt=1 exp ((1r)it) exp (Eit)\\n.\\n22\\nArxiv preprint version\\nThis value is upper bounded when we replace exp(Eit) in the denominator with min\\nj exp(Eij).\\nLikewise, it is lower bounded when we replace the same term with max\\nj\\nexp(Eij). Based on these\\nideas, we can bound the (i, j)-th element of P as follows:\\nexp(Eij)\\nmax\\nj\\nexp(Eij)\\nePij Pij \\nexp(Eij)\\nmin\\nj exp(Eij)\\nePij,\\n\\x12\\nmin\\nj exp(Eij Eij)\\n\\x13\\nePij Pij \\n\\x12\\nmax\\nj\\nexp(Eij Eij)\\n\\x13\\nePij,\\nwhere ePij =\\nexp((1r)ij)\\nPT\\nt=1 exp((1r)it) = softmax(1r)ij = (1softmax(r))ij. Given the fact that\\nexp(x) is approximated by 1+x+ 1\\n2!x2 + 1\\n3!x3 +. . . with Taylor series near to x = 0 and it is upper\\nbounded exp(x) 1 + 2x where the condition |x| 1.256 holds, we know that max\\nj\\nexp(Eij \\nEij) is upper bounded by 1 + 2 max\\nj (Eij Eij) with the condition that |Eij Eij| 1.256.\\nAnalogously, min\\nj exp(Eij Eij) is lower bounded by 12 min\\nj (Eij Eij). Therefore, the lower\\nand upper bounds of (i, j)-th element of P are derived as follows:\\n\\x12\\n1 2 min\\nj (Eij Eij)\\n\\x13\\nePij Pij \\n\\x12\\n1 + 2 max\\nj (Eij Eij)\\n\\x13\\nePij,\\n\\x12\\n1 + 2 max\\nj (Eij Eij)\\n\\x13\\nePij Pij \\n\\x12\\n1 + 2 max\\nj (Eij Eij)\\n\\x13\\nePij.\\n(21)\\nNote that the negative sign of lower bound of the first inequality goes inside of the min operation\\nwhile changing it to max operation. Finally, the matrix level inequality is formulated as follows:\\n(I 2D) eP P (I + 2D) eP,\\nwhere I is identity matrix. D is a diagonal matrix whose i-th diagonal element is Dii = max\\nj,j |Eij \\nEij|. We note that the last inequality is even larger/smaller bounds than those of Eq.21 because it\\ntakes the maximum value across j and j\\n simultaneously.\\nA.4.4\\nUPPER BOUNDS OF L-1 AND L-NORMS OF THE MATRIX FORMED\\n(Z1softmax(r))\\nBy following the property of matrix norm (Sec.A.4.1), the L1 norm of the matrix can be upper\\nbounded as follows:\\nZ1softmax(r)1 Z11softmax(r)1,\\n= Z11,\\nbased on the facts that softmax(r)1 = PT\\ni=1 |softmax(r)i| = 1. Similarly, the upper bound\\nof L-norm is as follows:\\nZ1softmax(r)Z1softmax(r),\\nZ1,\\nbased on the fact that softmax(r)= max\\ni\\n|softmax(r)i| 1.\\nA.4.5\\nUPPER BOUND OF D11D1\\nAbout the L-1 and L-norms of D1, we follow the same definition of Sec.A.4.3 for the matrix D\\nwhose diagonal element is Dii = max\\nj,j |EijEij|. Based on the initial condition of Lemma 6 which\\nis related to :\\nrPT\\ni=1 max\\nj,j |Eij Eij| \\nr\\nmax\\nj,j\\nPT\\ni=1 |Eij Eij|. Then, the multiplication\\n23\\nArxiv preprint version\\nof L-1 and L-norms is derived and upper bounded as follows:\\nD11D1= max\\ni,j,j |Eij Eij|\\nT\\nX\\ni=1\\nmax\\nj,j |Eij Eij|,\\n2 max\\ni,j |Eij|\\nT\\nX\\ni=1\\nmax\\nj,j |Eij Eij|,\\n2E1\\nT\\nX\\ni=1\\nmax\\nj,j |Eij Eij|,\\n22E1\\n \\nmax\\nj,j\\nT\\nX\\ni=1\\n|Eij Eij|\\n!\\n,\\n22E1\\n \\n2 max\\nj\\nT\\nX\\ni=1\\n|Eij|\\n!\\n,\\n= 82E1E1 = 82E2\\n1,\\n24\\n'),\n",
       "   Document(metadata={'Published': '2022-11-05', 'Title': 'Tri-Attention: Explicit Context-Aware Attention Mechanism for Natural Language Processing', 'Authors': 'Rui Yu, Yifeng Li, Wenpeng Lu, Longbing Cao', 'Summary': 'In natural language processing (NLP), the context of a word or sentence plays\\nan essential role. Contextual information such as the semantic representation\\nof a passage or historical dialogue forms an essential part of a conversation\\nand a precise understanding of the present phrase or sentence. However, the\\nstandard attention mechanisms typically generate weights using query and key\\nbut ignore context, forming a Bi-Attention framework, despite their great\\nsuccess in modeling sequence alignment. This Bi-Attention mechanism does not\\nexplicitly model the interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and resulting in\\npoor attention performance. Accordingly, a novel and general triple-attention\\n(Tri-Attention) framework expands the standard Bi-Attention mechanism and\\nexplicitly interacts query, key, and context by incorporating context as the\\nthird dimension in calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based additive,\\ndot-product, scaled dot-product, and bilinear operations in Bi-Attention to the\\ntensor operations for Tri-Attention. Extensive experiments on three NLP tasks\\ndemonstrate that Tri-Attention outperforms about 30 state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.'}, page_content='JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n1\\nTri-Attention: Explicit Context-Aware Attention\\nMechanism for Natural Language Processing\\nRui Yu, Yifeng Li, Member, IEEE, Wenpeng Lu, Member, IEEE, and Longbing Cao, Senior Member, IEEE\\nAbstractIn natural language processing (NLP), the context of\\na word or sentence plays an essential role. Contextual information\\nsuch as the semantic representation of a passage or historical\\ndialogue forms an essential part of a conversation and a precise\\nunderstanding of the present phrase or sentence. However,\\nthe standard attention mechanisms typically generate weights\\nusing query and key but ignore context, forming a Bi-Attention\\nframework, despite their great success in modeling sequence\\nalignment. This Bi-Attention mechanism does not explicitly model\\nthe interactions between the contexts, queries and keys of\\ntarget sequences, missing important contextual information and\\nresulting in poor attention performance. Accordingly, a novel and\\ngeneral triple-attention (Tri-Attention) framework expands the\\nstandard Bi-Attention mechanism and explicitly interacts query,\\nkey, and context by incorporating context as the third dimension\\nin calculating relevance scores. Four variants of Tri-Attention\\nare generated by expanding the two-dimensional vector-based\\nadditive, dot-product, scaled dot-product, and bilinear opera-\\ntions in Bi-Attention to the tensor operations for Tri-Attention.\\nExtensive experiments on three NLP tasks demonstrate that Tri-\\nAttention outperforms about 30 state-of-the-art non-attention,\\nstandard Bi-Attention, contextual Bi-Attention approaches and\\npretrained neural language models1.\\nIndex TermsAttention mechanism, Context, Interaction,\\nTriple attention, Natural language understanding\\nI. INTRODUCTION\\nA\\nTTENTION is the cognitive process of selectively con-\\ncentrating on one task while ignoring others. It efciently\\nallocates the nite brain processing resources to more impor-\\ntant tasks and distributes the resources to prioritized tasks [1].\\nIn neural networks, an attention mechanism is an adaptive\\nsparse method for encoding and modelling sequence inter-\\nactions by the association scores between different elements\\n[2]. It has been widely applied in various tasks of natural\\nlanguage processing (NLP), including machine translation [3],\\nThe research work is partly supported by National Key R&D Program\\nof China under Grant No.2018YFC0831700, Australian Research Council\\nDiscovery under Grant No.DP190101079 and Future Fellowship under Grant\\nNo.FT190100734, Natural Science Foundation of Shandong under Grant\\nNo.ZR2022MF243 and Key Program of Science and Technology of Shan-\\ndong Province under Grant No.2020CXGC010901. (Corresponding author:\\nWenpeng Lu.)\\nRui Yu and Wenpeng Lu are with the Department of Computer Sci-\\nence and Technology, Qilu University of Technology (Shandong Academy\\nof Sciences), Jinan 250353, China (e-mail: rui.yu1996@foxmail.com; wen-\\npeng.lu@qlu.edu.cn).\\nYifeng Li is with Department of Computer Science, Brock University,\\nNiagara Region, ON L2S 3A1, Canada (e-mail: yli2@brocku.ca).\\nLongbing Cao is with the Data Science Institute, University of Technology\\nSydney, Sydney, NSW 2007, Australia (e-mail: longbing.cao@uts.edu.au).\\nManuscript received October 15, 2022.\\n1The\\nsource\\ncodes\\nand\\ndata\\nare\\navailable\\nat\\nhttps://github.com/yurui12138/Tri-Attention.\\na. Bi-Attention\\nb. Tri-Attention\\na1\\na2\\na4\\na3\\nb2\\nb3\\nb1\\nc3\\nc2\\nc1\\nc4\\nattention score \\nof a3, b2 and c2\\na1\\na2\\na4\\na3\\nb1\\nb2\\nb3\\nattention score \\nof a3 and b2\\nsequence b\\nsequence a\\nsequence b\\nsequence a\\ncontext c\\nFig. 1: Comparison of the standard Bi-Attention mechanism\\nwith our proposed Tri-Attention mechanism. ai and bj rep-\\nresent two words from two sequences respectively, and ck\\ndenotes a contextual feature to them.\\ntext matching [4], automatic question answering [5], and\\nreading comprehension [6].\\nThe attention mechanism learns an inner-representation of a\\nsequence and the relationships between sequences, achieving\\ngreat success in various tasks [7], [8], [9]. However, it does not\\nexplicitly involve context as human cognition works, where\\ncontext plays a critical role in human visual and language\\ncomprehension. Human brain has neural networks dedicated\\nto reading and providing context from the environment in\\norder to improve perceptual learning and visual interpretation\\n[10]. In natural language understanding, context is the key\\nto derive and predict the meaning of a sentence [11]. It can\\nimprove language comprehensibility and how efciently the\\ncomprehension is cognitively processed [12]. Different from\\nthis human attention mechanism, the existing attention mech-\\nanisms in deep neural networks usually concentrate on word-\\nlevel feature interactions but fail to fully account for the overall\\ncontext of the word or sentence [13]. Accordingly, the attention\\nmatrix is calculated just by two individual tokens query and\\nkey extracted from sequences without a full consideration\\nof their context, forming a standard query-key-based two-\\ndimensional attention (we call it Bi-Attention for simplicity)\\nframework to capture sequence interactions, as shown in Fig.\\n1(a). This standard Bi-Attention framework requires a deeper\\nunderstanding of the data to obtain the inner representation\\nof sequences and capture the intrinsic relationships between\\nsequences [14]. Consequently, without involving context, the\\nexisting attention models may not effectively capture really\\nimportant context-aware information [13] and may produce\\nunsatised to inaccurate attention. Some existing work has\\nidentied this design aw in the Bi-Attention mechanism and\\narXiv:2211.02899v1  [cs.CL]  5 Nov 2022\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n2\\nincorporated a contextual feature into the attention matrix. For\\nexample, the COIN and RE2 models align contextual features\\nwith attention to improve sentence representation learning,\\nyielding better results than those without context [13], [15].\\nAlthough the importance of context has been recognized in\\nthe existing work, context has only been employed as extra\\nsupporting information to sequence representation learning.\\nNo general frameworks are available to simulate human at-\\ntention with context and explicitly capture the interactions\\nbetween target and contextual sequences or information. To\\nmanage this framework gap, we propose a triple-attention\\n(Tri-Attention) framework to simulate human attention and\\nexplicitly capture interactions between sequences and between\\nsequences and context. Different from existing Bi-Attention\\nand contextual attention like COIN and RE2 models by\\nintegrating contextual features into queries and keys, our Tri-\\nAttention mechanism directly involves context as the third\\ndimension in quantifying sequence interactions. A query-key-\\ncontext three-dimensional attention framework is formed for\\nTri-Attention, as shown in Fig. 1(b). The standard Bi-Attention\\nmechanism adopts query and key to calculate two-dimensional\\nrelevance (e.g., dot product), forming an attention matrix. Then\\nmatrix operations are applied to the attention matrix and value\\nto obtain the attention embedding. Differently, the attention\\nmatrix in Tri-Attention captures three-dimensional interactions\\nbetween query, key, and context. To make value consistent\\nwith the semantic space of the triple attention matrix, the rel-\\nevance between context and value is also calculated, producing\\ncontextual value. Finally, the attention embedding is obtained\\nas a weighted linear combination of contextual value vectors.\\nWe test Tri-Attention-enabled networks for three independent\\nNLP tasks, including retrieval-based English dialogue, Chinese\\nsentence matching, and English reading comprehension. The\\nresults show that Tri-Attention outperforms most state-of-the-\\nart approaches without attention, with Bi-Attention, contextual\\nBi-Attention, and pretrained language models with context.\\nFurthermore, we evaluate the different performance of Bi-\\nAttention, contextual Bi-Attention versus Tri-Attention and the\\neffectiveness of Tri-Attention through a case study.\\nThe main contributions of this work include:\\n The proposed Tri-Attention mechanism expands the stan-\\ndard two-dimensional attention framework to explicitly\\ninvolve and couple contextual information with query and\\nkey, hence the attention weights more sufciently capture\\ncontext-aware sequence interactions. To the best of our\\nknowledge, this is the rst work on explicitly involving\\ncontext (contextual features) and learning query-key-\\ncontext interaction-based attention between sequences.\\n Tri-attention takes a general three-dimensional tensor\\nframework, which can be instantiated into different imple-\\nmentations and applied to various tasks. We illustrate four\\nvariants by expanding the additive, dot-product, scaled\\ndot-product and trilinear operations on query, key and\\ncontext using tensor algebra for calculating Tri-Attention.\\n Extensive experiments on three different NLP tasks\\nand their real-world public datasets demonstrate the ef-\\nfectiveness of Tri-Attention. The Tri-Attention-enabled\\nnetworks produce substantially better performance than\\nabout 30 state-of-the-art methods.\\nThe rest of this paper is organized as follows. Section II\\nintroduces the related work. Section III discusses the back-\\nground and preliminaries. Section IV proposes Tri-Attention\\nmechanism and its variant implementations. Section V intro-\\nduces the multi-task-shared Tri-Attention network. Section VI\\ndemonstrates the performance of Tri-Attention by comparing\\nit with state-of-the-art methods in terms of a variety of aspects.\\nLastly, Section VII concludes this work.\\nII. RELATED WORK\\nIn this section, we briey review the related work on\\nattention mechanisms and involving contextual information in\\nattention, respectively.\\nHuman perception and visual processing may selective cer-\\ntain relevant parts of an image while ignoring other irrelevant\\ninformation [1]. This human attention mechanism has inspired\\nthe neural attention models widely applied in computer vision\\n[16]. Attention has also shown success in neural NLP tasks,\\nsuch as machine translation, automatic question answering,\\nsentence matching, and word sense disambiguation, where\\nattention usually captures the interactions between sequences.\\nFor example, in machine translation, a recurrent attention\\nmeasures the weights of all heads in each Transformer layer\\nto build more efcient neural machine translation models\\n[17]. In [18], syntax-enhanced attention mechanisms SEAs\\nimprove syntactic-enhanced machine translation by involving\\na dependency mask bias and a relative local-phrasal position\\nbias. In [19], a source-target bilingual syntactic alignment\\nSyntAligner aligns the syntactic structures of source and\\ntarget sentences with border-sensitive span attention and then\\nmaximizes their mutual dependency for machine translation. In\\nautomatic question answering, a Block-Skim attention mech-\\nanism measures the importance of tokens and blocks to skim\\nunnecessary context, improving the Transformer performance\\n[20]. In sentence semantic matching (SSM), A 3D CNN-based\\nSSM model rst constructs multi-dimensional representations\\nfor each sentence, then utilizes a 3D CNN and an attention\\nmechanism to learn the interactive matching features [21].\\nIn word sense disambiguation (WSD), an extractive sense\\ncomprehension mechanism employs local self-attention to\\nconstraint attentions to be local, which allows to handle longer\\ncontext without heavy computing burdens [22].\\nIn NLP tasks, the existing Bi-Attention mechanisms calcu-\\nlate attention scores by focusing on local relevance matching\\nat the token level without explicitly taking their overall context\\ninto account. Each element of the attention matrix is only\\ncomputed based on two individual tokens query and key.\\nThis two-dimensional Bi-Attention mechanism ignores the\\ninteractions with and inuence of their context, e.g., contextual\\nwords or sentences surrounding target words. On one hand,\\ncontextual information may involve important information and\\ninuence on the target, as demonstrated in other areas such\\nas contextual and sequential recommender systems [23] and\\ncontextual matrix factorization [24]. On the other, the inter-\\nactions and couplings between a target and its context form\\nessential constituents of the target, as shown in attribute/feature\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n3\\ninteraction analysis [25], [26], and multi-party interaction\\nlearning [27], [28]. These prompt the potential and need for\\nincluding context and target-context interactions into neural\\nattention mechanisms.\\nIn fact, various efforts aim to involve contextual information\\nin Bi-Attention mechanisms for NLP [13], [29], [30]. For\\nexample, Yang et al. [29] directly incorporate contextual\\ninformation into the representations of query and key with\\naddition operations. Ding et al. [30] argue that the existing\\ncross-attention was confused by the localness perception prob-\\nlem, which fails to adequately capture the whole context. A\\ncontext-aware cross-attention method models both local and\\nglobal contexts, which uses an interpolation gating mechanism\\nto combine the original and local cross-attention. Hu et al.\\n[13] propose a context-aware attention network (COIN) for\\nsentence matching. Its core component is a context-aware\\ninteraction block consisting of a context-aware cross-attention\\nlayer and a gate fusion layer, which consults contextual\\ninformation to enable better alignments and blends the original\\nand aligned representations with a gate connection.\\nIn addition, contextual information or contextual attention\\nhas also been applied in various non-NLP tasks, such as image\\ninpainting [31], [32], video captioning [33], recommendation\\n[23], [34], and decision making [35], [36]. Regarding contex-\\ntual attention for various vision and image-related tasks, for\\ninstance, in [31], contextual attention in CNN synthesizes both\\nimage structures and surrounding image features as references\\nfor image inpainting. The selective contextual attention in\\n[37] further learns pixel- and patch-level attentions from back-\\nground regions and selectively utilizes contextual attention to\\nenhance the original features. The contextual attention network\\nin [33] rst extracts visual and textual features at each time\\nstep, then utilizes a contextual attention mechanism to capture\\nmore information for captioning. In contextual attention-based\\nrecommendation, for example, attention-based inuential con-\\ntexts are modeled by involving heterogeneous relations for\\nrecommendation [34]. In [23], the attention of context is\\nmeasured and incorporated into sequential recommendation.\\nThese studies show the great potential of integrating context\\nwith attention for non-NLP tasks. Furthermore, in contextual\\nreinforcement learning (cRL), a contextual MDP (cMDP) ex-\\ntended the standard Markov Decision Process (MDP) to learn\\ncontext-conditioned policies, which increases the robustness\\nand generalization of RL models [35], [36].\\nAlthough the above-mentioned work strives to implement\\ncontextual attention to model the inuence from context, they\\ndo not directly capture target-context interactions, i.e., the\\ninteractions between context, query and key in calculating\\nattention scores. They still rely on the framework of standard\\nquery-key-based Bi-Attention mechanism, merely integrate\\ncontextual information and the original representation using\\naddition operations or gating mechanisms, there are no-to-\\nweak interactions between contextual information and the\\nrepresentations of query and key.\\nTo address the shortage of two-dimensional query-key at-\\ntention and the limitation of existing studies, we propose a\\nquery-key-context triple attention (Tri-Attention) framework.\\nTri-Attention uses tensor algebra techniques to explicitly in-\\nvolve contextual information and capture query-key-context\\ninteractions. In calculating the attention matrix, Tri-Attention\\nincorporates contextual information and treats it equally with\\nthe Bi-Attention factors query, key, and value. In this way,\\nTri-Attention expands Bi-Attention mechanisms by allowing\\ncontextual information to explicitly participate in calculating\\nattention weights.\\nConsequently, the contextual information in Tri-Attention\\nplays an essential role in capturing the interactions between\\nsequences under their contexts. This also expands the con-\\ntextual Bi-Attention mechanisms where context only plays a\\ncomplementary role.\\nIII. BACKGROUND AND PRELIMINARIES\\nHere, we introduce tensor algebra and the design of the\\nstandard Bi-Attention mechanism. These lay the foundation\\nof our proposed Tri-Attention.\\nA. Tensor Algebra\\nOur Tri-Attention builds on tensor algebra to incorporate\\nand interact context with query, key, and value and calculate\\nattention weights on the query-key-context tensor. Therefore,\\nhere, we introduce the essential algebraic notations and con-\\ncepts. We use a bold uppercase symbol to represent a matrix,\\ne.g., X; a bold lowercase for a vector (column), e.g., x;\\nand a lowercase or uppercase for a scalar, e.g., x or X. A\\ntensor is denoted by a bold calligraphic symbol, e.g., a three-\\ndimensional real-valued tensor: X RIJK.\\nThe tensor or vector matrix multiplication [38] is applied\\nto calculate Tri-Attention weights. Given a tensor X\\n\\nRI1I2IN , and a matrix Y\\n\\nRJIn, the n-mode\\nproduct of X and Y\\nis dened as X n Y\\n= Z\\n\\nRI1I2In1JIn+1IN , where\\nZi1i2in1jin+1iN =\\nIn\\nX\\nin=1\\nxi1i2in1inin+1iN yjin.\\n(1)\\nSimilarly, the n-mode product between X\\nand column\\nvector y\\n\\nRIn\\nis written as X n y\\n=\\nZ\\n\\nRI1I2In11In+1IN with element-wise entry as:\\nZi1i2in11in+1iN =\\nIn\\nX\\nin=1\\nxi1i2iniN yin.\\n(2)\\nOftentimes, the trivial n-th dimension in Z is squeezed out\\nsuch that Z RI1I2In1In+1IN is (n 1)-way.\\nIn this work, we keep this dimension for the convenience in\\nformulating the Trilinear attention in Section IV.\\nTo formulate our idea, we use C=[c1, c2,    , cJ] RDJ\\nto represent the contextual matrix which contains J context\\nvectors. Each contextual vector is a column vector of length D,\\nthat is cj RD, j = 1, 2,    , J. In this paper, a vector refers\\nto a column vector by default. The key information is repre-\\nsented by matrix K=[k1, k2,    , kI] RDI which includes\\nI key vectors of D dimensions. Similarly, the corresponding\\nvalue information is represented by V =[v1, v2,    , vI] \\nRDI. In general, a query vector is denoted by q RD.\\nFor N query vectors, we use Q RDN.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n4\\nB. Standard Bi-Attention Mechanism\\nThe standard Bi-Attention mechanism customarily obtains\\ninteractive features by calculating the relevance score between\\nsequences. It consists of three major steps.\\nFirst, the relevance F(q, ki) between query q and key ki\\nis calculated:\\nF(q, ki) = Similarity(q, ki),\\ni = 1, 2,    , I,\\n(3)\\nSecond, F(q, ki) is normalized by the softmax function:\\ni =\\nexp\\n\\x00F(q, ki)\\n\\x01\\nIP\\ni=1\\nexp\\n\\x00F(q, ki)\\n\\x01,\\ni = 1, 2,    , I,\\n(4)\\nwhere i is the normalized attention weight.\\nLastly, the attention embedding is obtained by a weighted\\nlinear combination with value vi:\\nqnew =\\nI\\nX\\ni=1\\nivi = V .\\n(5)\\nIn practice, the similarity measure is not unique, resulting\\nin different attention mechanisms and designs. The four most\\ncommonly used similarity measure methods are additive (Add)\\nsimilarity, dot-product (DP) similarity, scaled dot-product\\n(SDP) similarity, and bilinear (Bili) similarity.\\nAdditive (Add) similarity [39]:\\nF(q, ki) = ptanh(W q + Uki),\\ni = 1, 2,    , I,\\n(6)\\nDot-product (DP) similarity [40]:\\nF(q, ki) = qki,\\ni = 1, 2,    , I,\\n(7)\\nScaled dot-product (SDP) similarity [41]:\\nF(q, ki) = qki\\n\\nD\\n,\\ni = 1, 2,    , I,\\n(8)\\nBilinear (Bili) similarity [40]:\\nF(q, ki) = qW ki,\\ni = 1, 2,    , I,\\n(9)\\nW , U and p are learnable parameters.\\nThe main drawback of the standard Bi-Attention mecha-\\nnisms lies in that the relevance scores between sequences only\\nbuild on the token representations of q and ki. No contextual\\ninformation and the interaction between context and query and\\nkey are involved, which may miss important environmental\\ninformation and cause inaccurate relevance.\\nIV. TRI-ATTENTION MECHANISM\\nIn this section, we introduce the proposed Tri-Attention\\nmechanism. It extends the Bi-Attention mechanisms and builds\\non tensor operations for variant implementations of the query-\\nkey-context similarity.\\nContext\\nKey\\nQuery\\nValue\\nValue\\nCon_Value\\nF(Query,Key)\\nSoftMax\\nMatMul\\na. Bi-Attention\\nb. Tri-Attention\\nRelevance Score\\nAttention Weight\\nAttention Value\\nF(Query,Key,Context)\\nSoftMax\\nMatMul\\nQuery\\nKey\\nRelevance Score\\nAttention Weight\\nAttention Value\\nMerge\\nFig. 2: Comparison of the standard Bi-Attention versus Tri-\\nAttention mechanisms. The elements in red refer to the new\\nmodules introduced by Tri-Attention, which adjust standard\\nquery-key-value to be context dependent. As a result, the\\ncalculation of contextual relevance scores is semantically\\nconsistent with the computation of contextual values.\\nA. Framework of Tri-Attention Mechanism\\nThe Tri-Attention mechanism expands the standard query-\\nkey similarity-based Bi-Attention mechanism in Section III-B\\nto query-key-context tensor similarity, which thus explicitly\\ninvolves contextual information and captures the interactions\\nbetween target sequences and their contexts. Tri-Attention\\nengages contextual features in calculating the relevance scores\\nbetween sequences and then adjusts values to be context\\ndependent. Bi-Attention can be viewed as a special case\\nof Tri-Attention without the explicit dimension of context.\\nOnce context is removed, Tri-Attention is degraded to Bi-\\nAttention. Fig. 2 illustrates the main ideas and processes of\\nBi-Attention versus Tri-Attention mechanism. The extension\\nof Bi-Attention to Tri-Attention is marked in red for differen-\\ntiating the two mechanisms.\\nIn contrast to the standard three-step attention learning in\\nBi-Attention, Tri-Attention generalizes their rst and third\\nsteps. In the rst step, Tri-Attention rst expands the similar-\\nity calculation by involving contextual information to obtain\\ncontext-dependent query-key relevance scores, forming con-\\ntextual relevance score. In the third step, Tri-Attention explic-\\nitly integrates context and value to produce context-dependent\\nvalue, resulting in contextual value. The resultant contextual\\nvalue resides in the same semantic space as contextual rele-\\nvance scores. Both capture extra contextual information and\\nquery-key-value-context interactions for a more informative\\nbut semantically consistent attention representation.\\nB. Contextual Relevance Score\\nTo obtain relevance scores informative to the context of\\ntarget sequences, we expand the relevance calculation meth-\\nods including additive, dot-product, scaled dot-product, and\\nbilinear similarity in Section III-B by explicitly incorporating\\ncontextual information as a third dimension. This results in\\nfour contextual relevance similarity calculators: T-additive\\n(TAdd), T-dot-product (TDP), T-scaled-dot-product (TSDP),\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n5\\nand Trilinear (Trili) operations on the query-key-context tensor\\nby applying the tensor-matrix or vector product operations\\ndened in Eqs. (1) and (2). Here, prex T- indicates its roles\\nfor Tri-Attention by involving the third dimension context into\\nattention mechanisms.\\nT-additive (TAdd) similarity: The T-additive similarity ex-\\npands the additive similarity in Eq. (6) by adding a context\\ndimension cj to the usual terms q and ki, formulated below:\\nF(q, ki, cj) = ptanh(W q + Uki + Hcj),\\ni = 1, 2,    , I; j = 1, 2,    , J\\n(10)\\nwhere W , U, H, and p are learnable parameters.\\nT-dot-product (TDP) similarity: The T-dot-product similar-\\nity expands the standard query-key dot-product attention in\\nEq. (7) for Bi-Attention to the query-value-context T-dot-\\nproduct attention. T-dot-product attention replaces the inner\\nproduct of query and value vectors by the contextual inner\\nproduct between the query, value, and context vectors, which\\nis formulated below:\\nF(q, ki, cj) =\\nD\\nX\\nd=1\\nqdkidcjd = q, ki, cj\\n= I 1 q2 k\\ni 3 c\\nj\\ni = 1, 2,    , I; j = 1, 2,    , J\\n,\\n(11)\\nwhere q, ki, cjis the contextual inner product of three\\nvectors; q, k\\ni , and c\\nj , which are treated as row vectors,\\ni.e. of size 1  D. 1, 2, and 3 are 1-mode, 2-mode,\\nand 3-mode tensor-matrix or vector multiplication operators,\\nrespectively. I is a 3-way identity tensor of size D  D  D,\\nwhere only the (d, d, d)-th element is 1 (d = 1, 2,    , D), and\\nothers are 0. The resultant F(q, ki, cj) is a scalar (i.e., of size\\n1  1  1).\\nT-scaled-dot-product\\n(TSDP)\\nsimilarity:\\nSimilarly,\\nthe\\nscaled-dot-product attention in Eq. (8) for Bi-Attention is gen-\\neralized to T-scaled-dot-product. T-scaled-dot-product divides\\nthe contextual inner product by the squared root of the number\\nof dimensions:\\nF(q, ki, cj) =\\nPD\\nd=1 qdkidcjd\\n\\nD\\n= q, ki, cj\\n\\nD\\n=\\nI 1 q2 k\\ni 3 c\\nj\\n\\nD\\ni = 1, 2,    , I; j = 1, 2,    , J\\n.\\n(12)\\nTrilinear (Trili) similarity: With context involved, the bi-\\nlinear form in Eq. (9) is naturally extended to a multilinear,\\nprecisely trilinear, form using tensor-vector products, labeled\\nas Trilinear:\\nF(q, ki, cj) =\\nD\\nX\\nd=1\\nD\\nX\\nd=1\\nD\\nX\\nd=1\\nwdddqdkidcjd\\n= W 1 q2 k\\ni 3 c\\nj\\ni = 1, 2,    , I; j = 1, 2,    , J\\n,\\n(13)\\nThe learnable weight tensor W RDDD governs the\\ninteractions between any dimensions of the three vectors. T-\\ndot-product and T-scaled-dot-product are special cases of this\\ngeneral form. A nice property of this contextual generalization\\nis that the contextual relevance scores can be computed and\\nstored in a tensor using query, key, and context matrices:\\nF(Q, K, C) = W 1 Q2 K3 CRNIJ where\\nN, I, and J are the number of query vectors, key vectors, and\\ncontext vectors, respectively. However, the size of W grows\\nin a cubic way. Hence, F(Q, K, C) may not scale well. In\\npractice, we can use an economic version as follows:\\nF(q, ki, cj) = W q, Uki, Hcj\\n= I 1 (W q)2 (Uki)3 (Hcj)\\ni = 1, 2,    , I; j = 1, 2,    , J\\n,\\n(14)\\nwhere W , U, and H are learnable matrices. Concisely, for\\nmatrix inputs, we have F(Q, K, C) = I 1 (W Q)2\\n(UK)3 (HC)RNIJ.\\nC. Normalized Contextual Relevance Score\\nConsistent with the Bi-Attention mechanism, the contextual\\nrelevance score F(q, ki, cj) calculated by Eq. (14) for Tri-\\nAttention is also normalized by the softmax function:\\nc\\nij =\\nexp\\n\\x00F(q, ki, cj)\\n\\x01\\nIP\\ni=1\\nJP\\nj=1\\nexp\\n\\x00F(q, ki, cj)\\n\\x01\\ni = 1, 2,    , I; j = 1, 2,    , J\\n.\\n(15)\\nConsequently, the attention weights for query q is represented\\nby matrix Ac RIJ.\\nD. Contextual Value\\nThe above contextual relevance scores contain contextual\\ninformation and interactions between context, query, and key.\\nTo semantically match value to this contextual relevance score,\\nwe further integrate value with context using one of the\\nfollowing methods to obtain contextual value.\\nAdditive context-value integration:\\nvc\\n(i,j) = vi + cj\\ni = 1, 2,    , I; j = 1, 2,    , J.\\n(16)\\nMultiplicative context-value integration:\\nvc\\n(i,j) = vi cj\\ni = 1, 2,    , I; j = 1, 2,    , J\\n(17)\\nwhere is the Hadamard product operator.\\nBilinear context-value integration:\\nvc\\n(i,j) = (U vi) (Hcj)\\ni = 1, 2,    , I; j = 1, 2,    , J.\\n(18)\\nIn Eqs. (16)-(18), vc\\n(i,j) RD. These methods generate the\\ncontextual value tensor Vc RIJD.\\nE. Contextual Attention Embedding\\nSince there are different approaches to obtain contextual\\nattention weight matrix Ac by Eq. (15) and value tensor\\nVc per Eqs. (16)-(18), for the semantic consistency, their\\noperation may be consistent. For example, if T-additive in\\nEq. (10) is applied to obtain c\\nij, then additive context-value\\nintegration in Eq. (16) should be correspondingly applied to\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n6\\nBERTt\\nBERTt\\nBERTn\\nAvg_Pool\\nAvg_Pool\\nAvg_Pool\\nTri-Attention\\nTri-Attention\\nDense\\nInteraction \\nModule\\nEncoding \\nModule\\n...\\nX N\\nFiltering \\nModule\\nTri-Attention\\nTri-Attention\\nAvg_Pool\\nAvg_Pool\\nBERTn \\nPooled output\\nConcat\\nAggregation \\nModule\\n-\\nSoftmax\\nClassifier\\nPrediction \\nModule\\nQuestion\\nResponse\\nContext\\nS4\\nS3\\nS6\\nS5\\nR\\nS1\\nS2\\nDialogue History\\nResponse\\nQuestion\\nAnswer\\nPassage\\nInput Module\\nS2\\nConcat\\nS1\\n[S1;S2]\\ni. Retrieval-based Dialogue\\nii. Sentence Semantic Matching\\niii. Machine Reading Comprehension\\na. Input Modules of Different Tasks\\nb. Model Architecture\\nFig. 3: Shared architecture of Tri-Attention network (TAN) for three NLP tasks: left panel - input modules, right panel - model\\narchitecture.\\nobtain contextual value vc\\n(i,j). Further, the new contextual\\nattention embedding qc\\nnew corresponding to query q is:\\nqc\\nnew =\\nI\\nX\\ni=1\\nJ\\nX\\nj=1\\nc\\nijvc\\n(i,j) = V cc,\\n(19)\\nwhere qc\\nnew\\nRD, V c is mode-3 matricized from Vc:\\nV c=[vc\\n1, vc\\n2,    , vc\\nIJ] RD(IJ), and c is vectorized\\nfrom Ac. For m=(i1)I +j, vc\\nm in V c corresponds to vc\\n(i,j)\\nin Vc and c\\nm in c is the same as c\\nij in Ac.\\nF. Choice of Contexts\\nSince contextual information varies over tasks and deni-\\ntions, the acquisition of context is task-specic [13], [15], [30],\\n[36], [42]. As an example by following the practice in [42] for\\nNLP tasks, we rst choose the encoding results of BERT for\\ndifferent sequences as the preliminary contextual information.\\nThen an average pooling operation is applied to obtain the\\nnal contextual features.\\nMore specically, assume a task consists of several input\\nsequences S1, S2,    , and SK. First, we segment each se-\\nquence according to the minimum granularity of the language\\nunderlying the inputs. For example, if the sequences are in\\nChinese, Chinese can be segmented in terms of Chinese char-\\nacters. In contrast, if English is involved, the inputs should be\\nsegmented on the granularity of words. Then, we concatenate\\nthe segmented sequences with segments {SEP} to a new\\nsequence as follows:\\nS = {[CLS], S1, [SEP],    , [SEP], SK, [SEP]}\\n= {[CLS], w1\\n1,    , w\\nlS1\\n1 , [SEP],    , [SEP],\\nw1\\nK,    , w\\nlSK\\nK , [SEP]},\\n(20)\\nwhere wl\\nk is the l-th token in the k-th sequence. Finally, we\\nfeed this new sequence S into BERT to obtain the contextual\\ninformation, which consists of a list of vectors corresponding\\nto all tokens respectively. It is worth noting that the output of\\nBERT consists of two parts, and we use the rst part as the\\ncontextual feature.\\nV. A MULTI-TASK-SHARED TRI-ATTENTION NETWORK\\nHere, we apply Tri-Attention in diverse NLP tasks by con-\\nstructing a multi-task-shared Tri-Attention-enabled network\\n(TAN). We test Tri-Attention for retrieval-based dialogue,\\nsentence semantic matching, and machine reading comprehen-\\nsion. Accordingly, the shared architecture of our Tri-Attention\\nnetwork for these tasks is shown in Fig. 3.\\nTAN consists of two main panels. The left consists of three\\ninput modules corresponding to three NLP tasks, respectively.\\nFor each task, its input sequences and contexts specied ac-\\ncording to the task-specic requirement, which are marked by\\ndifferent colors. The right panel implements the corresponding\\nattention mechanism and learning tasks. It consists of ve\\ncore modules: encoding module, ltering module, interaction\\nmodule, aggregation module, and prediction module. First, the\\nencoding module employs BERTt to encode the two input\\nsequences to obtain their feature representations, and utilizes\\nBERTn to encode the context to obtain the contextual feature\\nrepresentation. Second, the ltering module trims the sequence\\nlength of the above representations to facilitate the following\\nprocessing. Third, the interaction module applies Tri-Attention\\nto engage three representations of input and context sequences\\nand then couple them by the Tri-Attention mechanism. The\\nstack number (i.e., N in Fig. 3) of interaction modules is\\nadjustable per learning task requirements and based on the\\nrelevance score calculation methods, as shown in Table IV.\\nFurthermore, the feature aggregation module performs the\\naverage-pooling of the representations of two input sequences\\nto lter them and then concatenates their representations with\\nthe pooling representation of the output of BERTn, and the\\ndifference between representations of two input sequences.\\nLastly, a prediction module feeds the above four represen-\\ntations to a softmax function to predict the nal response\\nmatching score. Below, in discussing the experimental settings\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n7\\nfor each task, we will further explain the task-specic settings\\nto customize TAN for each task.\\nVI. EXPERIMENTS\\nTo verify the effectiveness of Tri-Attention mechanism in\\ncomparison with Bi-Attention, we conduct extensive exper-\\niments on three NLP tasks: retrieval-based dialogue, sen-\\ntence semantic matching, and machine reading comprehen-\\nsion. Three public datasets are used to evaluate the results by\\ncomparing with different baseline methods for each task. In\\naddition, we report the results of ablation study and case study.\\nA. Evaluation Tasks and Their Datasets\\nWe evaluate Tri-Attention in three diverse NLP tasks:\\nretrieval-based dialogue, sentence semantic matching, and\\nmachine reading comprehension. These tasks and their cor-\\nresponding datasets are introduced below.\\n Retrieval-based dialogue. Given historical dialogue utter-\\nances, this task selects the correct response from multiple\\ncandidate responses. A commonly used data Ubuntu\\nCorpus V1 [43] is tested here. It is a public dialogue\\ndataset containing some 1 million multi-turn dialogues,\\nwith a total of over 7 million utterances and 100 million\\nwords. The ratio of positive versus negative instances\\nin the training set is 1:1. The ratio of positive versus\\nnegative instances in the validation and test sets is 1:9.\\nTable I illustrates the samples from the Ubuntu Corpus\\nV1 corpus. S1-S6 are historical dialogue utterances, and\\nthe candidate response utterances consist of a positive\\ncase and a negative case.\\nS1\\nhey guys i am trying to compile an\\napplication\\npath\\nwent well and\\nit created a\\npath\\nwhich takes\\nsome arguments but i dont know\\nhow to use this le ... can somebody\\ngive me a hint\\nHistorical\\nS2\\ndid nt\\npath\\ncreated a makele\\ndialogue\\nS3\\nikonia take a look at this\\nurl\\nutterances\\nS4\\nit did but is does nothing\\nS5\\nit just mentions make *** no targets\\nspecied and no makele found stop .\\nS6\\nseems like\\npath\\nuses this .. you\\ndo nt have to run this manually\\nPositive\\nlook at the install le i think i just\\nCandidate\\n(label:1)\\nhave to pass the right arguments\\nresponse\\nthanks a lot i think i have more to go\\nutterances\\nNegative\\non for my little project the syntax there\\n(label:0)\\nthere is pretty easy to parse and all you\\nneed is to make wget download that url\\nTABLE I: Instances of retrieval-based dialogues [43].\\n Sentence semantic matching. This task decides whether\\ntwo sentences share similar meaning. We use a large-\\nscale Chinese corpus LCQMC [44] for sentence semantic\\nmatching. It consists of 260,068 question pairs collected\\nby Baidu Knows. There are three subsets: 238,766 ques-\\ntion pairs for training, 8,802 question pairs for validation,\\nand 12,500 question pairs for test. Table II illustrates two\\nof its instances in LCQMC.\\nS1\\n\\nPositive\\n(En: Which song has this lyrics)\\n(label:1)\\nS2\\n\\n(En: Which song does this lyric come from?)\\nS3\\n\\nNegative\\n(En: Which browser can play movies)\\n(label:0)\\nS4\\n\\n(En: Which browser can download movies)\\nTABLE II: Instances of sentence semantic matching data [44].\\n Machine reading comprehension. Given a passage and\\nits corresponding question, this task identies the correct\\nanswer from multiple candidate choices. RACE [45]\\nis recognized as one of the largest and most difcult\\nEnglish datasets for multi-choice reading comprehension.\\nIt consists of two subsets: RACE-M and RACE-H, cor-\\nresponding to the difculty level for middle school and\\nhigh school, respectively. Table III illustrates this data.\\nAre you carrying too much on your back at\\nschool? Youre not alone. Back experts in\\nthe USA are worried about that young students\\nare having back and neck problems because\\nthey are carrying too much in their backpacks\\nPassage\\n...\\n(3) The heaviest things should be packed\\nclosest to the back.\\n(4) Bend both knees when you pick up the\\npack, dont just bend over the waist\\nQuestion\\nThe main idea of the passage is about\\n.\\nA. the problems made by rolling backpacks\\nCandidate\\nB. the advantage of backpacks\\nchoices\\nC. the best backpacks for students\\nD. how to lighten students backpacks\\nCorrect answer\\nD\\nTABLE III: Instances of machine reading comprehension data\\n[45]\\nB. TAN Settings\\nAs shown in Fig. 3, the contextual information in Tri-\\nAttention the concatenation of the outputs of BERT applied on\\ninput sequences, consistent with [42]. It is shown in [46] that\\nthe outputs of different layers of BERT can capture different\\nsentence features. Inspired by this, our experiment shows that\\nbetter performance can be achieved using the output of layer 1\\nas the representation of a single sequence. When the nal rep-\\nresentations of different sequences are obtained, we lter the\\nrepresentations by average pooling. Finally, inspired by [47],\\nwe concatenate the representations of different sequences, the\\ndifferential representations between sequences, and the pooled\\nrepresentation of BERT. The concatenate results are fed to\\nsoftmax for classication. We also add the dropout strategy\\nwith dropout rate 0.1 following [48]. The above settings are\\napplied to all three tasks: retrieval-based dialogue, sentence\\nsemantic matching, and machine reading comprehension.\\nIn addition, some hyperparameters are set for different tasks.\\nThe learning rate 1e-5 is used for retrieval-based dialogue, and\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n8\\nTask\\nDataset\\nTri-AttentionTAdd\\nTri-AttentionTDP\\nTri-AttentionTSDP\\nTri-AttentionTrili\\nRetrieval-based dialogue\\nUbuntu Corpus V1\\n3\\n2\\n3\\n4\\nSentence semantic matching\\nLCQMC\\n2\\n3\\n3\\n1\\nMachine reading comprehension\\nRACE\\n4\\n3\\n4\\n1\\nTABLE IV: Number of Tri-Attention layers with different similarity operations in TAN for different NLP tasks and datasets.\\n2e-5 for both sentence semantic matching and machine reading\\ncomprehension. The batch size for retrieval-based dialogue\\nis 32, with 64 for sentence semantic matching and 16 for\\nmachine reading comprehension. The cross-entropy loss is\\nused for the objective function of all tasks. AdamW [49]\\nis used for the optimization of retrieval-based dialogue and\\nsentence semantic matching, and BertAdam [50] for machine\\nreading comprehension. In TAN, Tri-Attention is stackable,\\nthus the number of its layers is adjusted for different tasks.\\nTable IV describes the number of layers for different tasks and\\ndatasets under different Tri-Attention similarity operations.\\nC. Task 1: Retrieval-based Dialogue\\nHere, we evaluate TAN with Tri-Attention mechanism\\nagainst three categories of baselines for retrieval-based dia-\\nlogue. The baselines consist of non-attention classic models,\\nstandard Bi-Attention-based neural networks with context or\\ninteraction, and pretrained neural language networks.\\n1) Baseline Methods: First, several non-attention classic\\nmethods are compared with TAN:\\n TF-IDF: A statistical method obtains the relevance fea-\\ntures of sentences per the frequency of characters [43].\\n RNN: A simple RNN encodes utterances to obtain a\\nfeature representation and calculates the relevance score\\nbased on the representation [43].\\n CNN: A CNN extracts utterance features and calculates\\nthe relevance representation of the features [51].\\n LSTM: LSTM serves as the context encoder to extract\\nutterance features and then calculates the relevance rep-\\nresentation [51].\\nSecond, several advanced neural networks with standard Bi-\\nAttention mechanisms with context or interaction are com-\\npared with TAN:\\n SMN: A sequential matching network selects responses\\nto multi-turn conversations, which matches a response to\\neach utterance in the context at multi-level granularities\\nand utilizes RNN to accumulate the matching vectors to\\nmodel the relations between utterances [52].\\n DUA: A self-matching attention routes the vital informa-\\ntion in each utterance then matches a response to each\\nrened utterance, followed by attention mechanism to\\naggregate matching vectors to obtain the nal matching\\nscore [53].\\n DAM: It constructs the representations of utterances at\\ndifferent granularities solely with stacked self-attention\\nthen calculates the matching score between the context\\nand response by cross-attention [54].\\n IoI: An interaction-over-interaction network performs\\nmatching by stacking multiple interaction blocks to\\nachieve deep interaction between responses and utter-\\nances [55].\\n ESIM: A sequential matching model based only on\\nchain sequence to select multi-turn responses, which\\ninvolves an enhanced sequential inference model and\\nsoft-alignment attention [56].\\n MSN: A multi-hop selector network selects the relevant\\nutterances as the context then calculates the matching\\nscore between the context and response [57].\\nLastly, we compare TAN with several pretrained neural\\nlanguage models:\\n BERT: A classical pretrained language model, which is\\nne-tuned for retrieval-based dialogue [50].\\n RoBERTa-SS-DA: A RoBERTa-based approach, where a\\nspeaker segmentation scheme and dialogue augmentation\\nimprove the performance [58].\\n BERT-DPT: It enhances the BERT ability by post-training\\non domain-specic corpus to train contextualized repre-\\nsentations that are missing in general corpus [59].\\n BERT-VFT: A ne-tuned model based on BERT, which\\ninvolves parameter-efcient transfer learning for various\\ntasks [60].\\n SA-BERT: A speaker-aware BERT-based model, which\\nperceives the change of speaker information and incor-\\nporates domain knowledge into pretrained BERT [61].\\n UMSBERT+: Utterance manipulation strategies are used to\\nselect multi-turn responses, including utterance insertion,\\ndeletion, and search strategies, to address some decien-\\ncies in existing BERT models [62].\\n BERT-SL: A context-response matching model based on\\npretrained language models, which introduces four self-\\nsupervised tasks to jointly train the response selection\\nmodel in a multi-task manner [63].\\n BERT-UMS+FGC: A ne-grained contrastive learning\\nmethod for response selection, which generates better\\nmatching representation at ner granularity to select\\npositive responses [64].\\n2) TAN Settings: The TAN with Tri-Attention in Fig. 3 is\\nspecied as follows for retrieval-based dialogue. The inputs\\nshown on the left panel consist of question, response, and\\ncontext. A question is the last sentence in the dialogue history\\nand a response is the original response in the dataset. The\\ncontext is the concatenation of dialogue and response [42]. The\\nmodel on the right panel has ve core modules, as described\\nin Section V. The performance is evaluated by Rn@k, which\\ndenotes whether the top-k retrieved responses from the n\\ncandidate responses contain the right responses.\\n3) Performance Evaluation:\\nThe performance of TAN\\nagainst all baseline models is shown in Table V. We obtain\\nthe following observations.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n9\\nFirst, compared with the non-attention classic methods, the\\nadvantage of TAN is very signicant. The classic methods\\nmodel sentences only using observable word features and\\nsimple interaction patterns to evaluate each candidate response.\\nThey are unable to effectively utilize latent semantic infor-\\nmation and learn complex interaction patterns. Thanks to the\\npowerful pretrained language model and the Tri-Attention\\nmechanism, TAN learns more sophisticated semantic features\\nthan these baselines. The learned contextual features are bene-\\ncial for representing utterances and responses, thus measuring\\ntheir relevance more accurately.\\nSecond, compared with Bi-Attention and interaction-based\\nnetworks, TAN also performs much better. This is probably\\nbecause TAN employs the pretrained BERTbase model trained\\non large corpus, which enhances our model with great prior\\nknowledge. In contrast, the benchmarks do not involve the vast\\namount of prior knowledge.\\nThird, TAN also outperforms the pretrained BERT, which is\\nthe state-of-the-art language model with self-attention mecha-\\nnism. Although BERT gains rich prior knowledge pretrained\\non a vast amount of textual data, it may not capture interactions\\nand contexts between utterances and responses for this task.\\nIn addition, BERT pretrained on a general corpus may not be\\nadaptive to this domain-specic dialogue dataset. In contrast,\\nTAN applies Tri-Attention to explicitly capture contexts and\\nadopts the post-training strategy in [59] on the task dataset to\\noptimize its specicity and adaptability.\\nLastly, TAN with Tri-Attention consistently outperforms\\nall pretrained BERT variants. Although most baselines also\\nperform post-training, they may fail to incorporate contextual\\ninformation and accurately capture interactive features be-\\ntween utterances and responses. In contrast, Tri-Attention fully\\nconsiders the context in its attention mechanism to determine\\nthe relevance between utterances and responses, contributing\\nto the improved performance of TAN over the BERT variants.\\nD. Task 2: Sentence Semantic Matching\\nWe further test TAN with Tri-Attention mechanism against\\nthree categories of baselines for sentence semantic matching.\\nThe baselines consist of non-attention classic networks, more\\nadvanced networks with relations or standard Bi-Attention-\\nbased context, and pretrained neural language networks.\\n1) Baseline Methods: First, several non-attention classic\\nnetworks are compared to TAN with Tri-Attention:\\n WMDchar and\\nWMDword: They apply the Wasserstein\\ndistance to the matching degree between two sentences\\nin terms of character and word, respectively [44].\\n CNNchar and\\nCNNword: They utilize a CNN to encode\\ntwo sentences for their corresponding sentence repre-\\nsentations, which are then concatenated to predict the\\nmatching degree with a softmax classier [44].\\n BiLSTMchar and BiLSTMword: They are similar with the\\nformer CNN method replacing CNN by a BiLSTM\\ncomponent [44].\\nSecond, more advanced networks with relations or standard\\nBi-Attention-based context are compared with TAN embedded\\nwith Tri-Attention mechanism:\\nMethods\\nR10@1\\nR10@2\\nR10@5\\nTF-IDF\\n41.0\\n54.5\\n70.8\\nRNN\\n40.3\\n54.7\\n81.9\\nCNN\\n54.9\\n68.4\\n89.6\\nLSTM\\n63.8\\n78.4\\n94.9\\nSMN\\n72.6\\n84.7\\n96.1\\nDUA\\n75.2\\n86.8\\n96.2\\nDAM\\n76.7\\n87.4\\n96.9\\nIoI\\n79.6\\n89.4\\n97.4\\nESIM\\n79.6\\n89.4\\n97.5\\nMSN\\n80.0\\n89.9\\n97.8\\nBERT\\n80.8\\n89.7\\n97.5\\nRoBERTa-SS-DA\\n82.6\\n90.9\\n97.8\\nBERT-DPT\\n85.1\\n92.4\\n98.4\\nBERT-VFT\\n85.5\\n92.8\\n98.5\\nSA-BERT\\n85.5\\n92.8\\n98.3\\nUMSBERT+\\n87.5\\n94.2\\n98.8\\nBERT-SL\\n88.4\\n94.6\\n99.0\\nBERT-UMS+FGC\\n88.6\\n94.8\\n99.0\\nTANTAdd\\n90.5\\n95.8\\n99.2\\n(0.3)\\n(0.06)\\n(0.05)\\nTANTDP\\n90.3\\n95.9\\n99.3\\n(0.1)\\n(0.3)\\n(0.06)\\nTANTSDP\\n90.6\\n95.7\\n99.2\\n(0.3)\\n(0.05)\\n(0.04)\\nTANTrili\\n90.1\\n95.7\\n99.3\\n(0.08)\\n(0.09)\\n(0.04)\\nTABLE V: Retrieval-based dialogue: Experimental results on\\nthe Ubuntu Corpus V1 corpus. The values associated with  in\\nbrackets are standard deviation. Mean and standard deviation\\nresults are averaged over ve runs of each model. Four\\nTAN variants of different tensor operation-based Tri-Attention\\nmechanisms: TAdd - T-Additive, TDP - T-Dot-Product, TSDP\\n- T-Scaled-Dot-Product, and Trili - Trilinear.\\n BiMPMchar and BiMPMword: They employ bilateral multi-\\nperspective matching to determine the semantic consis-\\ntency between sentences; BiLSTM learns sentence repre-\\nsentation, matches two sentences from multi-perspectives,\\naggregates the matching results, and makes prediction by\\na dense layer [65].\\n MSEM: A connected graph describes the relations be-\\ntween sentences and realizes a neural architecture for\\nmulti-task learning including both sentence matching and\\nclassication [66].\\n GMN: A neural graph matching network is fed with all\\npossible segmentation paths to form word lattice graphs\\nand learns graph-based representations of sentences [42].\\n COIN: A cross-attention mechanism combines contextual\\ninformation aligning sequences, with aligned representa-\\ntions interpolated by a gate fusion layer [13].\\n 3DSSM: A 3D CNN captures temporal and multi-granular\\nfeatures of sentences and generates matching representa-\\ntion [21].\\nLastly, we compare TAN to several pretrained neural lan-\\nguage networks:\\n BERT-Chinese: A Chinese BERT model [50].\\n BERT-wwm: A Chinese BERT with an entire word mask-\\ning mechanism during pretraining [67].\\n BERT-wwm-ext: A variant of BERT-wwm with more\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n10\\ntraining data and steps [67].\\n ERNIE: It learns language representation enhanced by\\nknowledge masking strategies and entity- and phrase-\\nlevel masking [68].\\n K-BERT: A model enhances BERT with HowNet by\\nintroducing soft position and visible matrix during ne-\\ntuning and inference phases [69].\\n GMN-BERT: A neural graph matching network with\\nmulti-granular input information [42].\\n2) TAN Settings: The TAN with Tri-Attention mechanism\\nis customized for sentence semantic matching. Its architecture\\nis composed of four core modules as shown in Fig. 3.\\nCompared to the TAN variant for retrieval-based dialogue, the\\nltering module is not necessary. This is because the ltering\\nmodule trims too long sequences, while the sequence length\\nin this task is acceptable. Since the input of sentence semantic\\nmatching consists of only two sentences, the contextual feature\\nrepresentation is learned by feeding connected results of\\nquestion and answer into BERTn [42].\\n3) Performance Evaluation: The results of TAN for sen-\\ntence semantic matching compared with all baseline models\\nare shown in Table VI. We obtain the following observations.\\nFirst, TAN signicantly outperforms all non-attention clas-\\nsic methods, similar to that for retrieval-based dialogue. The\\nclassic networks are unable to effectively capture latent seman-\\ntic information and complex interaction patterns. In contrast,\\nour Tri-Attention-based TAN combines the advantages of\\npretrained language model with contextual attention.\\nSecond, TAN also beats advanced networks with interac-\\ntion or Bi-Attention mechanism. Similar to retrieval-based\\ndialogue, TAN gains advantages from the prior knowledge\\nlearned by the pretrained model on large-scale corpus and\\ncontextual attention captured by Tri-Attention.\\nFurther, TAN performs better than pretrained BERT vari-\\nants. The BERT variants including BERT-wwm, BERT-wwm-\\next and ERNIE were trained on large data with more subtle\\ntraining techniques. In contrast, TAN outperforms these base-\\nlines without the involvement of large data or specic training\\ntechniques. This means that TAN not only outperforms these\\npowerful BERT variants in terms of accuracy and F1-score but\\nalso involves less training costs. Additionally, both K-BERT\\nand GMN-BERT are the latest BERT variants for sentence\\nsemantic matching. Their performance is still inferior to the\\nbest results of TANTAdd and TANTSDP.\\nLastly, TANTAdd, TANTDP, and TANTSDP outperform all\\nbaselines in terms of accuracy, and TANTAdd and TANTSDP\\nachieve better performance than baselines in terms of F1-score.\\nThese results verify the effectiveness of our model. TAN builds\\non BERT by replacing its self-attention mechanism with Tri-\\nAttention mechanism, whose four similarity implementations\\nmake 1-2% improvement in terms of accuracy compared to\\nthe original BERT. This indicates the Tri-Attention mechanism\\nplays a core role in making TAN better than BERT, i.e., beating\\nthe standard Bi-Attention mechanism.\\nE. Task 3: Machine Reading Comprehension\\nThe last NLP task for evaluating TAN with Tri-Attention\\nagainst the baselines is machine reading comprehension. We\\nMethods\\nAccuracy\\nF1-score\\nWMDchar\\n70.6\\n73.4\\nWMDword\\n60.0\\n70.8\\nCNNchar\\n71.8\\n75.2\\nCNNword\\n72.8\\n75.7\\nBiLSTMchar\\n73.5\\n77.5\\nBiLSTMword\\n76.1\\n78.9\\nBiMPMchar\\n83.4\\n85.0\\nBiMPMword\\n83.3\\n84.9\\nMSEM\\n85.7\\n-\\nGMN\\n84.6\\n86.0\\nCOIN\\n85.6\\n86.5\\n3DSSM\\n85.7\\n86.4\\nBERT\\n85.73\\n86.86\\nBERT-wwm\\n86.80\\n87.78\\nBERT-wwm-ext\\n86.68\\n87.71\\nERNIE\\n87.04\\n88.06\\nK-BERT\\n87.10\\n-\\nGMN-BERT\\n87.30\\n88.0\\nTANTAdd\\n87.49 (0.47)\\n87.95 (0.27)\\nTANTDP\\n87.25 (0.11)\\n87.83 (0.06)\\nTANTSDP\\n87.23 (0.58)\\n87.80 (0.29)\\nTANTrili\\n86.72 (0.05)\\n87.38 (0.02)\\nTABLE VI: Sentence semantic matching: Experimental results\\non the LCQMC data. Mean and standard deviation results are\\naveraged over ve runs of each model.\\nevaluate this in terms of three sets of baselines: single models,\\nensemble models, and pretrained lanaguage neural networks.\\n1) Baseline Methods: First, we compare TAN with single\\nmodel-based methods:\\n Stanford AR: The standard Bi-Attention mechanism ob-\\ntains the question-related passage representation, which\\nis then coupled with a candidate option to obtain the\\nrelevance score [45].\\n GA Reader: A gated attention mechanism captures the\\npassage information related to a problem, which is cou-\\npled with candidate options [45].\\n ElimiNet: An elimination gate discards irrelevant options,\\nrenes passage representations, and selects the most\\nsuitable option by a selection module [70].\\n HAF: A hierarchical attention mechanism captures the\\ninteractions between passages, questions and candidate\\noptions [71].\\n MUSIC: It dynamically applies different matching strate-\\ngies to different questions and applies a multi-hop rea-\\nsoning method to reach the right answer [72].\\n Hier-Co-Matching: A co-matching strategy matches a\\npassage to a question and a passage to a candidate answer,\\nand then leverages a hierarchical LSTM to encode co-\\nmatching states for the relevance representation [73].\\nSecond, we compare TAN with ensemble models:\\n GA Reader (6-ensemble): An ensemble method based on\\nmulti-hop gated attention mechanism between passages\\nand questions [45].\\n ElimiNet (6-ensemble): An ensemble method with a\\nmulti-hop mechanism eliminates candidate answers [70].\\n GA + ElimiNet (12-ensemble): It integrates GA Reader\\n(6-ensemble) and ElimiNet (6-ensemble) [74].\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n11\\n Dynamic Fusion Network (9-ensemble): An ensemble\\nmodel based on multi-strategy inference for a compre-\\nhension architecture [72].\\n CSA Model + ELMo (9-ensemble): It integrates spatial\\nconvolution attention mechanism with pretrained lan-\\nguage model ELMo [74].\\nLastly, we compare TAN with pretrained language models:\\n BERTbase: It is the most commonly used pretrained lan-\\nguage model structure [50].\\n BERTbase + DCMN: A reading comprehension model\\nbased on BERTbase and a dual co-matching network\\nmechanism [6].\\n2) TAN Settings:\\nWe here customize TAN with Tri-\\nAttention for machine reading comprehension. As shown in\\nFig. 3, its model architecture is similar to that of retrieval-\\nbased dialogue. However, this task involves three input se-\\nquences. Similar to sentence semantic matching, contextual\\nfeature representation is obtained by feeding the concatenation\\nof question, answer, and passage representations into BERTn.\\n3) Performance Evaluation: The results of TAN with Tri-\\nAttention for machine reading comprehension in comparison\\nwith all baselines are reported in Table VII. Again, TAN\\nwith Tri-Attention outperforms most baselines, verifying their\\neffectiveness.\\nFirst, TAN signicantly outperforms all single-model-based\\nmethods. This is probably because these single models have a\\nsingle structure which cannot model the semantic information\\nwithin a text and the semantic relations between texts. TAN\\nembedded with a pretrained language model and contextual\\nattention shows their empower.\\nSecond, our approach is clearly superior to all ensem-\\nble ones. The rst four ensemble models: GA Reader (6-\\nensemble), ElimiNet (6-ensemble), GA + ElimiNet (12-\\nensemble), and Dynamic Fusion Network (9-ensemble), do\\nnot apply pre-training, which may be the main reason for\\ntheir low performance. The last ensemble model CSA Model\\n+ ELMo (9-ensemble) utilizes the pretrained language model\\nELMo [75] to enhance its performance. Although this model\\nhas been greatly improved compared with the previous four\\nmodels, its performance is still much lower than TAN. This\\nmay be due to two reasons: the advantage of pretrained BERT\\nin our model and the contextual enhancement by Tri-Attention.\\nLastly, similar to the above conclusions in other tasks,\\nTAN with pretrained BERT beats the original BERT with\\nself-attention by explicitly capturing contextual interactions\\nwith queries and keys. Specically, the TAN with additive\\nTri-Attention similarity slightly outperform BERTbase+DCMN,\\nwhich is specic for machine reading comprehension and\\nrelies on strategies and skills used by humans to complete\\nreading comprehension tasks. In contrast, as a general attention\\nmechanism, TAN and Tri-Attention are applicable to machine\\nreading comprehension and other NLP tasks.\\nF. Bi-Attention vs Tri-Attention Comparison\\nHere, we further evaluate the effectiveness of Tri-Attention.\\nTwo sets of comparisons are undertaken. The rst compares\\nBi-Attention with Tri-Attention to show the effectiveness\\nMethods\\nAccuracy\\nStanford AR\\n43.3\\nGA Reader\\n44.1\\nElimiNet\\n44.7\\nHAF\\n47.2\\nMUSIC\\n47.4\\nHier-Co-Matching\\n50.4\\nGA Reader (6-ensemble)\\n45.9\\nElimiNet (6-ensemble)\\n46.5\\nGA + ElimiNet (12-ensemble)\\n47.2\\nDynamic Fusion Network (9-ensemble)\\n51.2\\nCSA Model + ELMo (9-ensemble)\\n55.0\\nBERTbase\\n65.0\\nBERTbase+DCMN\\n67.0\\nTANTAdd\\n67.5 (0.04)\\nTANTDP\\n66.9 (0.13)\\nTANTSDP\\n66.7 (0.07)\\nTANTrili\\n66.1 (0.14)\\nTABLE VII: Machine reading comprehension: Experimental\\nresults on RACE. Means and standard deviations are averaged\\nover ve runs of each model.\\nof Tri-Attention. The second compares Tri-Attention with a\\ncommonly used contextual attention enhancement method in\\nthe literature to show the need for capturing explicit query-\\nkey-context interactions. As the RACE data costs too much\\ntime on running a full set of experiments, we here report the\\nresults on LCQMC and the Ubuntu Corpus V1 corpus.\\n1) Effectiveness: Bi-Attention vs Tri-Attention Mechanisms:\\nWe evaluate the different effect of the Bi-Attention without\\ncontextual information versus the Tri-Attention with context\\nin the same network structure. The experimental results are\\nshown in Table VIII. Bi-Attention mechanism does not involve\\ncontextual information in measuring the interactions. Its other\\nparts are the same as Tri-Attention. We report the results of\\nthese two attention mechanisms under four relevance scoring\\noperations: TAdd, TDP, TSDP, and Trili, respectively.\\nOverall, the results show that the Tri-Attention variants\\nconsistently outperform the counterparts without context. This\\ncorroborates the contribution of Tri-Attention with context,\\neven though the performance varies over different relevance\\nscoring functions.\\nSpecically, on LCQMC, the difference between Bi-\\nAttention and Tri-Attention mechanisms shows the greatest\\nunder trilinear operation. Tri-Attention gains extra 1.03%\\naccuracy and 0.76% F1-score over Bi-Attention, respectively.\\nFor the Ubuntu Corpus V1 corpus, the greatest difference is\\nassociated with SDP, where Tri-Attention makes 0.8%, 0.2%,\\nand 0.1% improvement in terms of evaluation metrics R10@1,\\nR10@2, and R10@5, respectively.\\n2) Necessity: Contextual Query-Key Interactions vs Query-\\nKey-Context Interactions: As discussed in Section II, some\\nexisting methods also involve contextual information, typically\\nby simple addition or concatenation to target representations.\\nHere, we evaluate the difference of this way from our ap-\\nproaches in Tri-Attention which involves context equivalently\\nto other query and key entities in the attention learning.\\nWe generate a Bi-Attention variant to realize a contextual\\nattention enhancement method commonly used in the existing\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n12\\nMethods\\nLCQMC\\nUbuntu Corpus V1 Corpus\\nAccuracy\\nF1-score\\nR10@1\\nR10@2\\nR10@5\\nTri-AttentionTAdd\\n87.49 (0.47)\\n87.95 (0.27)\\n90.5 (0.3)\\n95.8 (0.06)\\n99.2 (0.05)\\nBi-AttentionAdd\\n86.77 (0.28)\\n87.37 (0.16)\\n89.9 (0.4)\\n95.5 (0.2)\\n99.2 (0.09)\\nTri-AttentionTDP\\n87.25 (0.11)\\n87.83 (0.06)\\n90.3 (0.1)\\n95.9 (0.3)\\n99.3 (0.06)\\nBi-AttentionDP\\n86.52 (0.09)\\n87.20 (0.06)\\n89.8 (0.2)\\n95.5 (0.1)\\n99.2 (0.04)\\nTri-AttentionTSDP\\n87.23 (0.58)\\n87.80 (0.29)\\n90.6 (0.3)\\n95.7 (0.05)\\n99.2 (0.04)\\nBi-AttentionSDP\\n86.61 (0.09)\\n87.29 (0.04)\\n89.8 (0.3)\\n95.5 (0.2)\\n99.1 (0.05)\\nTri-AttentionTrili\\n86.72 (0.05)\\n87.38 (0.02)\\n90.1 (0.08)\\n95.7 (0.09)\\n99.3 (0.04)\\nBi-AttentionBili\\n85.84 (0.11)\\n86.72 (0.06)\\n89.7 (0.09)\\n95.5 (0.1)\\n99.2 (0.03)\\nTABLE VIII:\\nResults of effectiveness of Bi-Attention vs Tri-Attention mechanisms on LCQMC and Ubuntu Corpus V1\\nCorpus. Means and standard deviations are averaged over ve runs.\\nMethods\\nLCQMC\\nUbuntu Corpus V1 Corpus\\nAccuracy\\nF1-score\\nR10@1\\nR10@2\\nR10@5\\nTri-AttentionTAdd\\n87.49 (0.47)\\n87.95 (0.27)\\n90.5 (0.3)\\n95.8 (0.06)\\n99.2 (0.05)\\nC-BiAttentionAdd\\n86.57 (0.15)\\n87.23 (0.07)\\n90.1 (0.4)\\n95.7 (0.3)\\n99.2 (0.06)\\nTri-AttentionTDP\\n87.25 (0.11)\\n87.83 (0.06)\\n90.3 (0.1)\\n95.9 (0.3)\\n99.3 (0.06)\\nC-BiAttentionDP\\n85.94 (0.21)\\n86.81 (0.12)\\n90.1 (0.3)\\n95.6 (0.2)\\n99.2 (0.06)\\nTri-AttentionTSDP\\n87.23 (0.58)\\n87.80 (0.29)\\n90.6 (0.3)\\n95.7 (0.05)\\n99.2 (0.04)\\nC-BiAttentionSDP\\n86.64 (0.11)\\n87.32 (0.06)\\n90.2 (0.1)\\n95.7 (0.2)\\n99.3 (0.04)\\nTri-AttentionTrili\\n86.72 (0.05)\\n87.38 (0.02)\\n90.1 (0.08)\\n95.7 (0.09)\\n99.3 (0.04)\\nC-BiAttentionBili\\n85.94 (0.10)\\n86.82 (0.06)\\n90.2 (0.4)\\n95.7 (0.2)\\n99.2 (0.05)\\nTABLE IX: Results of contextual query-key interactions vs query-key-context interactions on LCQMC and Ubuntu Corpus\\nV1 Corpus. Means and standard deviations are averaged over ve runs.\\nwork. The contextual information is added (with addition)\\nto each sequence separately to obtain a contextual sequence\\nrepresentation. Then, the context-enhanced sequence repre-\\nsentations interact using the standard query-key Bi-Attention\\nmechanism, whose other network settings are consistent with\\nTri-Attention. This forms the commonly used contextual at-\\ntention in the literature, abbreviated C-BiAttention.\\nThe experimental results are shown in Table IX. It shows\\nthat the performance of C-BiAttention consistently under-\\nperforms than Tri-Attention with all four relevance calcu-\\nlators. C-BiAttention with the dot-product-based relevance\\ncalculator achieves the lowest performance. On LCQMC, C-\\nBiAttention reduces 1.52% accuracy and 1.17% F1-score over\\nTri-Attention, respectively. Similarly, on the Ubuntu Corpus\\nV1 corpus, the highest performance degradation is with the\\nAdditive-based approach. In comparison with Tri-Attention,\\nC-BiAttention decreases 0.4% R10@1, 0.1% R10@2, respec-\\ntively. This experiment veries that the explicit query-key-\\ncontext interactions in Tri-Attention outperforms the simple\\ncontextual attention by adding or concatenating contextual\\ninformation to underlying sequences. It also conrms the\\nnecessity of learning query-key-context interactions in Tri-\\nAttention.\\nG. Hyperparameter Analysis: Tri-Attention Layers\\nTAN in Fig. 3 is a stackable structure, where the number\\nof Tri-Attention layers can be dynamically adjusted according\\nto learning tasks. In addition, In addition, different relevance\\nscore calculation methods in Eqs. (10)-(13) also affect the\\nnumber of Tri-Attention layers for a learning task. We thus\\ntest the effect of the number of Tri-Attention layers.\\nFig. 4 - 6 show the experimental results of retrieval-based\\ndialogue, sentence semantic matching, and machine reading\\ncomprehension tasks on Ubuntu Corpus V1, LCQMC, and\\nRACE, respectively. The best performance of Tri-Attention\\ncorresponds to different relevance score calculation methods\\nfor different tasks. This indicates that, when applying Tri-\\nAttention to a special task, all four relevance calculation\\nmethods should be tried before achieving the best result. In\\naddition, the number of Tri-Attention layers depends on the\\nrelevance calculation methods and learning tasks. Thus, when\\napplied to a specic task, the best number of Tri-Attention\\nlayers needs to be tuned for each relevance calculation method.\\nH. TAN Case Study\\nLastly, we illustrate the prediction results of TAN with Tri-\\nAttention on the LCQMC data for sentence semantic matching.\\nWe only illustrate the additive-based relevance calculation for\\nTri-Attention as the previous experiments show its better sta-\\nbility. We reapply the three variants used in Section VI-F: Tri-\\nAttentionAdd, Bi-AttentionAdd and C-BiAttentionAdd, respec-\\ntively. To verify the advantages of Tri-Attention, the prediction\\nresults of all three models were statistically analyzed.\\nSpecically, we conduct a statistical analysis when only\\nTri-Attention makes correct prediction while the other two\\nmodels predict incorrectly. In the test set of LCQMC, there are\\n151 pieces of data conforming to the above situation, among\\nwhich 49 pieces are positive and 102 pieces are negative. This\\nsuggests that Tri-Attention is better at identifying negative\\ndata. The instance in Table X illustrates the result.\\nFor both S1 and S2, the gold label is 0, which means that\\ntheir meanings are different. Our Tri-attention can make a\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n13\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n1\\n2\\n3\\n4\\n5\\n6\\nR10@1\\nR10@2\\nR10@5\\n10@1\\nR\\n10@2\\nR\\n10@5\\nR\\n(a) Tri-AttentionTAdd\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n1\\n2\\n3\\n4\\n5\\n6\\nR10@1\\nR10@2\\nR10@5\\n10@1\\nR\\n10@2\\nR\\n10@5\\nR\\n(b) Tri-AttentionTDP\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n1\\n2\\n3\\n4\\n5\\n6\\nR10@1\\nR10@2\\nR10@5\\n10@1\\nR\\n10@2\\nR\\n10@5\\nR\\n(c) Tri-AttentionTSDP\\n89\\n90\\n91\\n92\\n93\\n94\\n95\\n96\\n97\\n98\\n99\\n100\\n1\\n2\\n3\\n4\\n5\\n6\\nR10@1\\nR10@2\\nR10@5\\n10@1\\nR\\n10@2\\nR\\n10@5\\nR\\n(d) Tri-AttentionTrili\\nFig. 4: Performance comparison on the Ubuntu Corpus V1 corpus with different number of Tri-Attention layers.\\n86\\n86.5\\n87\\n87.5\\n88\\n88.5\\n1\\n2\\n3\\n4\\n5\\n6\\nF1-score\\nAccuracy\\nscore\\n-\\nF1\\n(a) Tri-AttentionTAdd\\n86\\n86.5\\n87\\n87.5\\n88\\n1\\n2\\n3\\n4\\n5\\n6\\nF1-score\\nAccuracy\\nscore\\n-\\nF1\\n(b) Tri-AttentionTDP\\n86\\n86.5\\n87\\n87.5\\n88\\n1\\n2\\n3\\n4\\n5\\n6\\nF1-score\\nAccuracy\\nscore\\n-\\nF1\\n(c) Tri-AttentionTSDP\\n86\\n86.5\\n87\\n87.5\\n88\\n1\\n2\\n3\\n4\\n5\\n6\\nF1-score\\nAccuracy\\nscore\\n-\\nF1\\n(d) Tri-AttentionTrili\\nFig. 5: Performance comparison on LCQMC with different number of Tri-Attention layers.\\n65.5\\n66\\n66.5\\n67\\n67.5\\n1\\n2\\n3\\n4\\n5\\n6\\nAccuracy\\n(a) Tri-AttentionTAdd\\n65.5\\n66\\n66.5\\n67\\n67.5\\n1\\n2\\n3\\n4\\n5\\n6\\nAccuracy\\n(b) Tri-AttentionTDP\\n65.5\\n66\\n66.5\\n67\\n67.5\\n1\\n2\\n3\\n4\\n5\\n6\\nAccuracy\\n(c) Tri-AttentionTSDP\\n65\\n65.5\\n66\\n66.5\\n67\\n1\\n2\\n3\\n4\\n5\\n6\\nAccuracy\\n(d) Tri-AttentionTrili\\nFig. 6: Performance comparison on RACE with different number of Tri-Attention layers.\\nData\\nLabel\\nS1\\n\\n0\\n(En: Which browsers can play movies)\\nS2\\n\\n(En: Which browsers can download movies)\\nTABLE X: Case study examples from LCQMC.\\ncorrect judgement, while the others cannot. Since there are\\nmany overlap words in S1 and S2, if a model judges the\\nrelations between these two sentences only on the basis of\\nword-level relevance without the contextual information, it\\nis easy to be misled and draw a wrong conclusion. This\\nprobably explains why the Bi-AttentionAdd method fails to\\ncorrectly predict the relation between S1 and S2. As for C-\\nBiAttentionAdd, although it also involves context, its contextual\\ninformation does not participate in the interaction with queries\\nand keys, thus making no direct impact on the relevance\\nscore between sentences. While Tri-Attention directly involves\\ncontext in the interactions between sentences, contributing to\\nbetter prediction.\\nVII. CONCLUSIONS\\nContextual information has shown essential in many learn-\\ning tasks. The great success of attention mechanisms does\\nnot necessarily involves contextual information. In neural\\nNLP, increasing work on contextual attention learning typi-\\ncally concatenates contextual features into underlying targets\\nsuch as sequences, then the standard query-key-based Bi-\\nAttention mechanisms calculate relevance scores on the to-\\nkenized contextual sequence representations. In this paper, a\\nnovel query-key-context-interactive Tri-Attention mechanism\\nexplicitly captures the interactions between query, key and\\ncontext. We derive four query-key-context relevance calcu-\\nlation methods for Tri-Attention using tensor algebraic tech-\\nniques. Intensive experiments on different NLP tasks show\\nthat Tri-Attention-based networks can serve as a general\\nattention framework, which outperforms most state-of-the-art\\nnon-attention, standard Bi-Attention, contextual Bi-Attention,\\nand pretrained language models with attention. Our future\\nwork includes evaluating Tri-Attention in other NLP tasks and\\nTri-Attention without pretrained BERT, and exploring more\\neffective tensor algebraic implementations for for interactions\\nwith n > 3 factors.\\nREFERENCES\\n[1] K. Oberauer, Working memory and attention - A conceptual analysis\\nand review, Journal of Cognition, vol. 2, no. 1, p. 36, 2019.\\n[2] D. Hu, An introductory survey on attention mechanisms in NLP\\nproblems, in Proceedings of SAI Intelligent Systems Conference, 2019,\\npp. 432448.\\n[3] A. Raganato, Y. Scherrer, and J. Tiedemann, Fixed encoder self-\\nattention patterns in transformer-based machine translation, in EMNLP,\\n2020, pp. 556568.\\n[4] B. Lyu, L. Chen, S. Zhu, and K. Yu, LET: Linguistic knowledge\\nenhanced graph transformer for Chinese short text matching, in AAAI,\\n2021, pp. 13 49813 506.\\n[5] Z. Ye, Y. Qin, and W. Xu, Financial risk prediction with multi-round\\nQ&A attention network, in IJCAI, 2020, pp. 45764582.\\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\\n14\\n[6] S. Zhang, H. Zhao, Y. Wu, Z. Zhang, X. Zhou, and X. Zhou, DCMN+:\\nDual co-matching network for multi-choice reading comprehension, in\\nAAAI, 2020, pp. 95639570.\\n[7] T. Wang, Y. Zhu, L. Jin, C. Luo, X. Chen, Y. Wu, Q. Wang, and M. Cai,\\nDecoupled attention network for text recognition, in AAAI, 2020, pp.\\n12 21612 224.\\n[8] M. E. Basiri, S. Nemati, M. Abdar, E. Cambria, and U. R. Acharya,\\nAbcdm: An attention-based bidirectional CNN-RNN deep model for\\nsentiment analysis, Future Generation Computer Systems, vol. 115, no.\\n115, pp. 279294, 2021.\\n[9] L. Ding, L. Wang, and D. Tao, Self-attention with cross-lingual position\\nrepresentation, in ACL, 2020, pp. 16791685.\\n[10] C. Gilbert, M. Ito, M. Kapadia, and G. Westheimer, Interactions\\nbetween attention, context and learning in primary visual cortex, Vision\\nResearch, vol. 40, no. 10, pp. 12171226, 2000.\\n[11] P. Knoeferle, How context inuences language processing and compre-\\nhension, Research Outreach, 2020.\\n[12] R. Willems and M. Peelen, How context changes the neural basis of\\nperception and language, iScience, vol. 24, no. 5, p. 102392, 2021.\\n[13] Z. Hu, Z. Fu, Y. Yin, and G. de Melo, Context-aware interaction\\nnetwork for question matching, in EMNLP, 2021, pp. 38463853.\\n[14] S. Storks, Q. Gao, and J. Y. Chai, Recent advances in natural language\\ninference: A survey of benchmarks, resources, and approaches, arXiv\\npreprint arXiv:1904.01172, 2019.\\n[15] R. Yang, J. Zhang, X. Gao, F. Ji, and H. Chen, Simple and effective text\\nmatching with richer alignment features, in ACL, 2019, pp. 46994709.\\n[16] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, Recurrent models\\nof visual attention, in NIPS, 2014, pp. 22042212.\\n[17] J. Zeng, S. Wu, Y. Yin, Y. Jiang, and M. Li, Recurrent attention for\\nneural machine translation, in EMNLP, 2021, pp. 32163225.\\n[18] T. Zhang, H. Huang, C. Feng, and L. Cao, Enlivening redundant heads\\nin multi-head self-attention for machine translation, in EMNLP, 2021,\\npp. 32383248.\\n[19] T. Zhang, H. Huang, L. Cao, and C. Feng, Self-supervised bilingual\\nsyntactic alignment for neural machine translation, in AAAI, 2021, pp.\\n14 45414 462.\\n[20] Y. Guan, Z. Li, Z. Lin, Y. Zhu, J. Leng, and M. Guo, Block-Skim:\\nEfcient question answering for Transformer, in AAAI, 2022, pp.\\n10 71010 719.\\n[21] W. Lu, R. Yu, S. Wang, C. Wang, P. Jian, and H. Huang, Sentence\\nsemantic matching based on 3D CNN for human-robot language inter-\\naction, ACM Transactions on Internet Technology, vol. 21, no. 4, pp.\\n98:198:24, 2021.\\n[22] G. Zhang, W. Lu, X. Peng, S. Wang, B. Kan, and R. Yu, Word sense\\ndisambiguation with knowledge-enhanced and local self-attention-based\\nextractive sense comprehension, in COLING, 2022.\\n[23] A. Rashed, S. Elsayed, and L. Schmidt-Thieme, Context and attribute-\\naware sequential recommendation via cross-attention, in RecSys, 2022,\\npp. 7180.\\n[24] Y. Zheng and G. Florez Arias, A family of neural contextual matrix\\nfactorization models for context-aware recommendations, in UMAP,\\n2022, pp. 16.\\n[25] Q. Zhang, L. Cao, C. Shi, and Z. Niu, Neural time-aware sequential\\nrecommendation by jointly modeling preference dynamics and explicit\\nfeature couplings, IEEE Transactions on Neural Networks and Learning\\nSystems, 2021.\\n[26] L. Cao, Beyond IID: Non-IID thinking, informatics, and learning,\\nIEEE Intelligent Systems, vol. 37, no. 4, pp. 517, 2022.\\n[27] L. Cao and C. Zhu, Personalized next-best action recommendation with\\nmulti-party interaction learning for automated decision-making, Plos\\none, vol. 17, no. 1, p. e0263010, 2022.\\n[28] C. Zhu, L. Cao, and J. Yin, Unsupervised heterogeneous coupling\\nlearning for categorical representation, IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence, vol. 44, no. 1, pp. 533549, 2020.\\n[29] B. Yang, J. Li, D. F. Wong, L. S. Chao, X. Wang, and Z. Tu, Context-\\naware self-attention networks, in AAAI, 2019, pp. 387394.\\n[30] L. Ding, L. Wang, D. Wu, D. Tao, and Z. Tu, Context-aware cross-\\nattention for non-autoregressive translation, in COLING, 2020, pp.\\n43964402.\\n[31] Y. Zeng, Z. Lin, H. Lu, and V. M. Patel, CR-Fill: Generative image\\ninpainting with auxiliary contextual reconstruction, in ICCV, 2021, pp.\\n14 16414 173.\\n[32] H. Xiang, Q. Zou, M. A. Nawaz, X. Huang, F. Zhang, and H. Yu,\\nDeep learning for image inpainting: A survey, Pattern Recognition, p.\\n109046, 2022.\\n[33] P. Song, D. Guo, J. Cheng, and M. Wang, Contextual attention network\\nfor emotional video captioning, IEEE Transactions on Multimedia,\\n2022.\\n[34] Y. Pang, L. Wu, Q. Shen, Y. Zhang, Z. Wei, F. Xu, E. Chang,\\nB. Long, and J. Pei, Heterogeneous global graph neural networks for\\npersonalized session-based recommendation, in WSDM, 2022, pp. 775\\n783.\\n[35] A. Hallak, D. D. Castro, and S. Mannor, Contextual Markov decision\\nprocesses, ArXiv Preprint, p. arXiv:1502.02259, 2015.\\n[36] C. Benjamins, T. Eimer, F. Schubert, A. Mohan, A. Biedenkapp,\\nB. Rosenhahn, F. Hutter, and M. Lindauer, Contextualize me -\\nThe case for context in reinforcement learning, ArXiv Preprint, p.\\narXiv:2202.04500, 2022.\\n[37] J. Liu, M. Gong, Z. Tang, A. Qin, H. Li, and F. Jiang, Deep image\\ninpainting with enhanced normalization and contextual attention, IEEE\\nTransactions on Circuits and Systems for Video Technology, 2022.\\n[38] T. Kolda and B. Bader, Tensor decompositions and applications, SIAM\\nReview, vol. 51, no. 3, pp. 455500, 2009.\\n[39] D. Bahdanau, K. Cho, and Y. Bengio, Neural machine translation by\\njointly learning to align and translate, in ICLR, 2015, pp. 115.\\n[40] T. Luong, H. Pham, and C. D. Manning, Effective approaches to\\nattention-based neural machine translation, in EMNLP, 2015, pp. 1412\\n1421.\\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\\nL. Kaiser, and I. Polosukhin, Attention is all you need, in NIPS, 2017,\\npp. 59986008.\\n[42] L. Chen, Y. Zhao, B. Lyu, L. Jin, Z. Chen, S. Zhu, and K. Yu, Neural\\ngraph matching networks for Chinese short text matching, in ACL,\\n2020, pp. 61526158.\\n[43] R. Lowe, N. Pow, I. Serban, and J. Pineau, The Ubuntu dialogue corpus:\\nA large dataset for research in unstructured multi-turn dialogue systems,\\nin SIGDIAL, 2015, pp. 285294.\\n[44] X. Liu, Q. Chen, C. Deng, H. Zeng, J. Chen, D. Li, and B. Tang,\\nLCQMC: A large-scale Chinese question matching corpus, in COL-\\nING, 2018, pp. 19521962.\\n[45] G. Lai, Q. Xie, H. Liu, Y. Yang, and E. H. Hovy, RACE: Large-scale\\nreading comprehension dataset from examinations, in EMNLP, 2017,\\npp. 785794.\\n[46] G. Jawahar, B. Sagot, and D. Seddah, What does BERT learn about\\nthe structure of language? in ACL, 2019, pp. 36513657.\\n[47] N. Reimers and I. Gurevych, Sentence-bert: Sentence embeddings using\\nSiamese bert-networks, in EMNLP, 2019, pp. 39803990.\\n[48] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever,\\nand\\nR. Salakhutdinov, Dropout: A simple way to prevent neural networks\\nfrom overtting, Journal of Machine Learning Research, vol. 15, no. 1,\\npp. 19291958, 2014.\\n[49] I. Loshchilov and F. Hutter, Decoupled weight decay regularization,\\nin ICLR, 2019, pp. 118.\\n[50] J. Devlin, M. Chang, K. Lee, and K. Toutanova, BERT: Pre-training of\\ndeep bidirectional transformers for language understanding, in NAACL,\\n2019, pp. 41714186.\\n[51] R. Kadlec, M. Schmid, and J. Kleindienst, Improved deep learning\\nbaselines for Ubuntu corpus dialogs, CoRR, vol. abs/1510.03753, 2015.\\n[52] Y. Wu, W. Wu, C. Xing, M. Zhou, and Z. Li, Sequential matching net-\\nwork: A new architecture for multi-turn response selection in retrieval-\\nbased chatbots, in ACL, 2017, pp. 496505.\\n[53] Z. Zhang, J. Li, P. Zhu, H. Zhao, and G. Liu, Modeling multi-turn\\nconversation with deep utterance aggregation, in COLING, 2018, pp.\\n37403752.\\n[54] X. Zhou, L. Li, D. Dong, Y. Liu, Y. Chen, W. X. Zhao, D. Yu, and\\nH. Wu, Multi-turn response selection for chatbots with deep attention\\nmatching network, in ACL, 2018, pp. 11181127.\\n[55] C. Tao, W. Wu, C. Xu, W. Hu, D. Zhao, and R. Yan, One time\\nof interaction may not be enough: Go deep with an interaction-over-\\ninteraction network for response selection in dialogues, in ACL, 2019,\\npp. 111.\\n[56] Q. Chen and W. Wang, Sequential attention-based network for noetic\\nend-to-end response selection, CoRR, vol. abs/1901.02609, 2019.\\n[57] C. Yuan, W. Zhou, M. Li, S. Lv, F. Zhu, J. Han, and S. Hu, Multi-\\nhop selector network for multi-turn response selection in retrieval-based\\nchatbots, in EMNLP, 2019, pp. 111120.\\n[58] J. Lu, X. Ren, Y. Ren, A. Liu, and Z. Xu, Improving contextual\\nlanguage models for response retrieval in multi-turn conversation, in\\nSIGIR, 2020, pp. 18051808.\\n[59] T. Whang, D. Lee, C. Lee, K. Yang, D. Oh, and H. Lim, An effective\\ndomain adaptive post-training method for BERT in response selection,\\nin INTERSPEECH, 2020, pp. 15851589.\\n15\\n[60] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. de Laroussilhe,\\nA. Gesmundo, M. Attariyan, and S. Gelly, Parameter-efcient transfer\\nlearning for NLP, in ICML, 2019, pp. 27902799.\\n[61] J. Gu, T. Li, Q. Liu, Z. Ling, Z. Su, S. Wei, and X. Zhu, Speaker-aware\\nBERT for multi-turn response selection in retrieval-based chatbots, in\\nCIKM, 2020, pp. 20412044.\\n[62] T. Whang, D. Lee, D. Oh, C. Lee, K. Han, D. Lee, and S. Lee,\\nDo response selection models really know whats next? utterance\\nmanipulation strategies for multi-turn response selection, in AAAI,\\n2021, pp. 14 04114 049.\\n[63] R. Xu, C. Tao, D. Jiang, X. Zhao, D. Zhao, and R. Yan, Learning an\\neffective context-response matching model with self-supervised tasks for\\nretrieval-based dialogues, in AAAI, 2021, pp. 14 15814 166.\\n[64] Y. Li, C. Xu, H. Hu, L. Sha, Y. Zhang, and D. Jiang, Small changes\\nmake big differences: Improving multi-turn response selection in dia-\\nlogue systems via ne-grained contrastive learning, CoRR, 2021.\\n[65] Z. Wang, W. Hamza, and R. Florian, Bilateral multi-perspective match-\\ning for natural language sentences, in IJCAI, 2017, pp. 41444150.\\n[66] Q. Huang, J. Bu, W. Xie, S. Yang, W. Wu, and L. Liu, Multi-task\\nsentence encoding model for semantic retrieval in question answering\\nsystems, in IJCNN, 2019, pp. 18.\\n[67] Y. Cui, W. Che, T. Liu, B. Qin, and Z. Yang, Pre-training with whole\\nword masking for chinese BERT, IEEE/ACM Transactions on Audio,\\nSpeech, and Language Processing, vol. 29, pp. 35043514, 2021.\\n[68] Y. Sun, S. Wang, Y. Li, S. Feng, X. Chen, H. Zhang, X. Tian,\\nD. Zhu, H. Tian, and H. Wu, ERNIE: Enhanced representation through\\nknowledge integration, arXiv preprint arXiv:1904.09223, 2019.\\n[69] W. Liu, P. Zhou, Z. Zhao, Z. Wang, Q. Ju, H. Deng, and P. Wang,\\nK-BERT: Enabling language representation with knowledge graph, in\\nAAAI, 2020, pp. 29012908.\\n[70] S. Parikh, A. Sai, P. Nema, and M. M. Khapra, ElimiNet: A model\\nfor eliminating options for reading comprehension with multiple choice\\nquestions, in IJCAI, 2018, pp. 42724278.\\n[71] H. Zhu, F. Wei, B. Qin, and T. Liu, Hierarchical attention ow for\\nmultiple-choice reading comprehension, in AAAI, 2018, pp. 60776085.\\n[72] Y. Xu, J. Liu, J. Gao, Y. Shen, and X. Liu, Towards human-level\\nmachine reading comprehension: Reasoning and inference with multiple\\nstrategies, CoRR, 2017.\\n[73] S. Wang, M. Yu, J. Jiang, and S. Chang, A co-matching model for\\nmulti-choice reading comprehension, in ACL, 2018, pp. 746751.\\n[74] Z. Chen, Y. Cui, W. Ma, S. Wang, and G. Hu, Convolutional spatial\\nattention model for reading comprehension with multiple-choice ques-\\ntions, in AAAI, 2019, pp. 62766283.\\n[75] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and\\nL. Zettlemoyer, Deep contextualized word representations, in NAACL,\\n2018, pp. 22272237.\\nRui Yu received the masters degree in computer\\napplication technology from the Qilu University\\nof Technology (Shandong Academy of Sciences),\\nJinan, China. He is pursuing the Ph.D. degree at\\nHuazhong University of Science and Technology,\\nWuhan, China. His research interests include text\\nsemantic matching and question answering system.\\nHe may be contacted at rui.yu1996@foxmail.com.\\nYifeng Li received the Ph.D. in Computer Science\\nfrom the University of Windsor, Canada. He is an\\nassistant professor and Canada Research Chair (Tier\\n2) in Machine Learning for Biomedical Data Science\\nat the Department of Computer Science, Department\\nof Biological Sciences, and Centre for Biotechnol-\\nogy, Brock University. His research interests include\\nneural networks, machine learning, data science,\\noptimization, bioinformatics, and chemoinformatics.\\nHe may be contacted at yli2@brocku.ca.\\nWenpeng Lu received the Ph.D. degree in computer\\napplications from the Beijing Institute of Technol-\\nogy, Beijing, China. He is a professor with the\\nDepartment of Computer Science and Technology,\\nQilu University of Technology (Shandong Academy\\nof Sciences), Jinan, China. His research interests in-\\nclude natural language processing, machine learning,\\nand their enterprise applications. He is a member\\nof IEEE, CCF and CIPS. He may be contacted at\\nwenpeng.lu@qlu.edu.cu.\\nLongbing Cao received a Ph.D. degree in pat-\\ntern recognition and intelligent systems and another\\nPh.D. in computing sciences. He is a professor and\\nan ARC Future Fellow (level 3) at the University of\\nTechnology Sydney. His research interests include\\nAI, data science, machine learning, behavior infor-\\nmatics, and enterprise innovation. He is the EICs of\\nIEEE Intelligent Systems and Springers JDSA. He\\nmay be contacted at longbing.cao@uts.edu.au.\\n')]]}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import operator\n",
    "from typing import Annotated, List\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import streamlit as st\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "API_KEYS = ['LANGCHAIN_API_KEY', 'OPENAI_API_KEY', 'LANGCHAIN_TRACING_V2', 'LANGCHAIN_ENDPOINT', \n",
    "            'LANGCHAIN_PROJECT', 'TAVILY_API_KEY', 'GROQ_API_KEY']\n",
    "for api_key in API_KEYS:\n",
    "    os.environ[api_key] = os.getenv(api_key)\n",
    "\n",
    "# st.title(\" Multi-Agent Research Assistant\")\n",
    "# user_question = st.text_input(\"Enter your research topic:\")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "vector_db = Chroma(embedding_function=OpenAIEmbeddings())\n",
    "\n",
    "class ResearchStateInput(TypedDict):\n",
    "    question: str\n",
    "    \n",
    "\n",
    "class ResearchState(TypedDict):\n",
    "    question: str\n",
    "    flattened_docs: list\n",
    "    summary: str\n",
    "    citations: list\n",
    "    fact_check: str\n",
    "    errors: list = []\n",
    "    content: Annotated[list, operator.add]\n",
    "    new_questions: list\n",
    "    vector: List[Document]\n",
    "\n",
    "class ResearchStateOutput(TypedDict):\n",
    "    flattened_docs: str\n",
    "    summary: str\n",
    "    vector: List[Document]\n",
    "    flattened_docs: list\n",
    "\n",
    "def get_user_input(state: ResearchStateInput):\n",
    "    return {'question': state['question']}\n",
    "\n",
    "def wikiloader(state: ResearchStateInput)->ResearchState:\n",
    "    \"\"\"To load the information from Wikipedia based on the user input question.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "    wiki_result = wikipedia.run(question)\n",
    "    doc = Document(page_content=wiki_result, metadata={\"source\": \"Wikipedia\", \"title\": question})\n",
    "    return {'content': [doc]}\n",
    "\n",
    "def arxiv_loader(state: ResearchStateInput)->ResearchState:\n",
    "    \"\"\"To load the information from Arxiv based on the user input question.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    arxiv = ArxivLoader(\n",
    "                        query=question,\n",
    "                        load_max_docs=2)\n",
    "    content = arxiv.load()\n",
    "\n",
    "    return {'content': [content]}\n",
    "\n",
    "def query_translation(state: ResearchStateInput)->ResearchState:\n",
    "    \"\"\"To translate user input queries into better queries/questions that can be used to retrieve information from the web.\"\"\"\n",
    "\n",
    "    question = state['question']\n",
    "    query_translation_prompt = \"\"\"You are expert at translating user input queries into a better queries/questions that \n",
    "                        can be used to retrieve information from the web. Here is your user-input question: {question}\\n\n",
    "                        Output (2 queries):\"\"\"\n",
    "\n",
    "    question_template = query_translation_prompt.format(question=question)\n",
    "    response = model.invoke(question_template)\n",
    "\n",
    "    return {'new_questions': response.content.split('\\n')}\n",
    "\n",
    "def tavily_search_docs(state: ResearchState):\n",
    "    \"\"\"To search the web for the information based on the translated queries/questions and get the url details.\"\"\"\n",
    "    \n",
    "    result_doc = []\n",
    "    new_questions = state['new_questions']\n",
    "    web_search_prompt = \"\"\"You are expert at retrieving information from the web. \n",
    "                        You are asked to find the following information: {question}\"\"\"\n",
    "\n",
    "    tavily = TavilySearchResults(max_results=1)\n",
    "    for question in new_questions:\n",
    "        prompt = web_search_prompt.format(question=question)\n",
    "        tavily_search = tavily.invoke({'query': prompt})\n",
    "        result_doc.extend([Document(page_content=tavily['content'], metadata={\"source\": tavily['url'], \"title\": question}) \n",
    "                            for tavily in (tavily_search)])\n",
    "        \n",
    "    return {'content': result_doc}\n",
    "\n",
    "def retrieve(state: ResearchState):\n",
    "    content = state['content']\n",
    "    question = state['question']\n",
    "\n",
    "    flattened_docs = [item for sublist in content for item in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "    vector = vector_db.add_documents(flattened_docs)\n",
    "\n",
    "    return {'flattened_docs': flattened_docs, 'question': question, 'vector': vector}\n",
    "\n",
    "\n",
    "def summarization(state: ResearchState):\n",
    "    \"\"\"To summarize the retrieved information from the web.\"\"\"\n",
    "    flattened_docs = state['flattened_docs']\n",
    "    question = state['question']\n",
    "\n",
    "    summarization_llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "    summarization_prompt = \"\"\"You are expert at summarizing the retrieved information from the different sources. \n",
    "                            You are asked to summarize the following information and retain the citations: {documents}\"\"\"\n",
    "\n",
    "    summarization_prompt = summarization_prompt.format(documents=flattened_docs)\n",
    "\n",
    "    try:\n",
    "        response = model.invoke(summarization_prompt)\n",
    "    except Exception as _:\n",
    "        response = summarization_llm.invoke(summarization_prompt)\n",
    "\n",
    "    citations = [doc.metadata.get('source', 'Arxiv') for doc in flattened_docs]\n",
    "    return {'summary': response.content, 'citations': list(set(citations))}\n",
    "\n",
    "def fact_check_agent(state: ResearchState):\n",
    "    \"\"\"To fact-check the summarized information.\"\"\"\n",
    "    summary = state['summary']\n",
    "\n",
    "    fact_check_prompt = \"\"\"You are expert at fact-checking the summarized information. \n",
    "                        You are asked to fact-check the following information: {summary}\"\"\"\n",
    "\n",
    "    fact_check_prompt = fact_check_prompt.format(summary=summary)\n",
    "    response = model.invoke(fact_check_prompt)\n",
    "\n",
    "    return {'fact_check': response.content}\n",
    "\n",
    "def error_detection_agent(state: ResearchState):\n",
    "    \"\"\"To detect the errors in the fact-checked information.\"\"\"\n",
    "    fact_check = state['fact_check']\n",
    "    errors = []\n",
    "\n",
    "    if \"conflicting\" in fact_check.lower():\n",
    "        errors.append(\"Conflict identified in the fact-checked information.\")\n",
    "\n",
    "    return {'errors': errors}\n",
    "\n",
    "def error_checker(state: ResearchState)->ResearchStateOutput:\n",
    "    errors = state['errors']\n",
    "    \n",
    "    if errors:\n",
    "        return ['get_user_input']\n",
    "\n",
    "    return END\n",
    "\n",
    "def rag_result(question, document):\n",
    "    prompt = \"\"\"Generate answer of the given question: {question} based on the following summary and document: {summary} \n",
    "                Also adding full document: {document}\"\"\"\n",
    "\n",
    "    retriever = vector.as_retriever(search_kwargs = {'k': 1})\n",
    "\n",
    "    rag_chain = ({'context': retriever, 'question': RunnablePassthrough()}\n",
    "                | prompt\n",
    "                | model\n",
    "                | StrOutputParser())\n",
    "    \n",
    "    return rag_chain.invoke(question)\n",
    "        \n",
    "\n",
    "builder = StateGraph(ResearchStateInput, output=ResearchState)\n",
    "builder.add_node('get_user_input', get_user_input)\n",
    "builder.add_node('wikiloader', wikiloader)\n",
    "builder.add_node('arxiv_loader', arxiv_loader)\n",
    "builder.add_node('query_translation', query_translation)\n",
    "builder.add_node('tavily_search_docs', tavily_search_docs)\n",
    "builder.add_node('retrieve', retrieve)\n",
    "builder.add_node('summarization', summarization)\n",
    "builder.add_node('fact_check_agent', fact_check_agent)\n",
    "builder.add_node('error_detection_agent', error_detection_agent)\n",
    "\n",
    "builder.add_edge(START, 'get_user_input')\n",
    "builder.add_edge('get_user_input', 'wikiloader')\n",
    "builder.add_edge('get_user_input', 'arxiv_loader')\n",
    "builder.add_edge('get_user_input', 'query_translation')\n",
    "builder.add_edge('query_translation', 'tavily_search_docs')\n",
    "\n",
    "builder.add_edge('arxiv_loader', 'retrieve')\n",
    "builder.add_edge('wikiloader', 'retrieve')\n",
    "builder.add_edge('tavily_search_docs', 'retrieve')\n",
    "builder.add_edge('retrieve', 'summarization')\n",
    "builder.add_edge('summarization', 'fact_check_agent')\n",
    "builder.add_edge('fact_check_agent', 'error_detection_agent')\n",
    "\n",
    "builder.add_conditional_edges('error_detection_agent', error_checker, ['get_user_input', END])\n",
    "\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "result = graph.invoke({'question':f'Attention is all you need'})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Attention is all you need',\n",
       " 'flattened_docs': [Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Dont Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i {1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Dont Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {(Attentioni, ) | i {L k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n\\n{1, 2, . . . , L k}} \\n{(, MLPi) | i {L k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {() | i {L k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layers features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Dont Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Dont Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Dont Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and Re, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:87478759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 2213722176. PMLR, 2329 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., ancucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Dont Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:1745617472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 72877301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n1401114023, 2020.\\n6\\n'),\n",
       "  Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'),\n",
       "  Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster  with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\nCorresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe models scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as ) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n The Number of Groups N. In RITA, the number of groups\\nis a crucial factor that balances the speed up and the quality of\\nattention approximation. A small will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate is essential to the performance of group attention.\\nHowever, depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of . In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate \\nalmost impossible.\\n Batch Size. Moreover, as we want to dynamically adjust \\nduring training, a fixed batch size is sub-optimal: as decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate for each group attention layer during the\\ntraining process. It starts with a large and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups and the batch size .\\nThis model is used to predict for a given when training RITA.\\nSpecifically, we first sample some values in a reasonable range.\\nFor each sampled , we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length 2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes hidden\\nembedding vectors Ras input, then projects them to\\nqueries (), keys () and values () and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state , is computed by:\\n= , = ,= \\n= = ( \\n\\n\\n)\\n(1)\\nWhere R,R,Rare projection\\nmatrices for generating , ,. Ris also regarded as the\\npacking of query vectors {1, ...,} with dimension into a\\nmatrix. R,Rare regarded as the packing of key\\nvectors {1, ...,} and value vectors {1, ..., } in the same way.\\nGiven a matrix R, the softmax function normalizes \\nto ensure the sum of each row equals to 1, as shown below.\\n(,) =\\n(,)\\n1\\n=0 (,)\\n(2)\\nNote the attention matrix A is an matrix, where represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length \\nand withvariables as an n  m matrix, RITA usesconvolution\\nkernels to chunkinto n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w  m matrix, where defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of \\nsemantic units 1,2, ...,() as input (e.g. embeddings of\\nwindows for a timeseries), then models the correlations between\\nthe semantic units and outputs 1, ...,() as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a (2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the cloze text pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate . The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, , the attention score of window onto\\nwindow , is determined by the inner product between the query\\nvector of window and the key vector of window , that is,  .\\nGiven another window , if window has the similar key vector\\nto window , that is, , then   . In other words,\\nwhen .\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window , we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping windows into groups, group atten-\\ntion compresses the attention matrix from anmatrix to an\\nmatrix. Because (number of groups) tends to be much smaller\\nthan (number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesnt hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix , canonical self-attention computes the output\\nembedding as O = AV. Because is an  matrix and is an\\nmatrix, the matrix product operation still produces an \\nmatrix . That is, it produces a dimensional feature vector for\\neach window. However, our group attention will produce an  \\nattention matrix e\\n, where corresponds to the number of groups.\\nIn this case the matrix product will produce a matrix e\\n. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix from the group attention matrix e\\n. For example,\\ngiven one group composed of and , we map its group\\nattention vector in e\\ninto two rows that correspond to and\\nin . However, in this case we again get a  attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix .\\nGiven an element,ofcorresponding to the dimension of\\ns feature vector,,= , where vector ai Rn denotes the\\nrow of the attention matrix and vector vj Rn denotes the \\ndimension of all the feature vectors. Given ai =< a1\\ni , a2\\ni ,    , an\\ni >\\nand vj =< v1\\nj , v2\\nj ,    , vn\\nj >, ,= n\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 1 and 2 belong to the same group\\n1. Then 1\\n= 2\\n= e1\\n, where e1\\ne\\ncorresponds to the attention\\nof group 1 onto . Therefore, 1\\n1\\n+ 2\\n2\\n= e1\\n(1\\n+ 2\\n).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector into a -dimensional group fea-\\nture vectorebeforehand, we could directly use the group attention\\nvector eand the group feature vector eto compute ,.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\nthat is identical to the embedding produced\\nby using the full attention matrix and the embedding matrix .\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix is computed as = SoftMax( QKT\\n\\ndk ). To compute ,\\nwe have to first compute (denoted as ) which is an  \\nmatrix. Then normalizing the matrix with softmax produces the\\nattention matrix .\\nGroup attention follows the same procedure. But after grouping\\nkeys into e, eproduces an  matrix e. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e\\nwill result in a group attention matrix e\\nfrom which we are not able\\nto recover a full attention matrix that is identical to first restoring\\neto and then applying softmax on . The matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall  ematrix is not memory efficient, as it will end up with\\na full  matrix .\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n(g\\n,) =\\n(,)\\n1\\n=0 (,)\\n(3)\\nIn Eq. 3, represents the number of windows that Group\\ncontains. Compared to the original softmax, our group softmax\\nconsiders each group as elements and counts it \\ntimes when summing up the exponential of each groups ,. In\\nthis way, the group softmax function operating on the small e\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is () and\\nthe space complexity is(), while the time and space complexity\\nof the original self-attention mechanism are (2) and (2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\nwhich approxi-\\nmates the attention matrixproduced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let be the radius of the ball where all key vectors\\nlive; ebe the representative of the group that contains key . Let \\ndenote the full attention matrix restored from e\\n. Suppose the distance\\nbetween eand (||ekk||) satisfies: ||ekk|| d.\\nThen > 1, if d ln()\\n2R , 1\\nAi,j\\nAi,j \\nLemma 1 shows that the error bound of the group attention is\\ndetermined by the distance . As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups  the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (()).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given centers, in each iteration the time and space\\ncomplexity of K-means is (). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi cj| =\\n\\n(vi cj)2, i [1, n], j [1, N]. The performance bot-\\ntleneck is . We instead use a different formulation: |\\n| = |vi cj| =\\n\\n|vi|2 + |cj|2 2vi  cj, i [1, n], j [1, N]. This is\\nbecause in this formulation, the performance bottleneck is  ,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\nand accordingly the batch size , as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts and based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines . Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate .\\nThe adaptive scheduler of RITA starts with a large and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase .\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard  as difficult as setting an appropriate .\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound , and then uses Lemma 1 to translate to a distance\\nthreshold . RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold .\\nLemma 2. Denote to be the cluster center of . Assume\\nthe existing grouping satisfies k,\\nmax\\nxclusterk\\n|ck x| d , thus satis-\\nfying an error bound by Lemma 1. If there exist clusters, namely,\\n1,2, ...,, satisfying that:\\n\\n\\n|| + || ,, [1,]\\n(4)\\nmerging them into one cluster still meets the error bound .\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if and satisfy:\\n\\n\\n||+|| , and\\n\\n\\n||+|| \\nthere is an undirected edge between and ;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 1,2;\\n(2) If 1 and 2 satisfy:\\n\\n\\n|| + || ,\\n\\n\\n|| + || \\n2\\n(5)\\nis marked.\\n(3) Decrease the number of clusters by counting the masks in 2.\\nIn this solution, clusters in 1 can be regarded as transfer nodes.\\nIf (5) holds for (1,1 2) and (\\n1,2 2), respectively, we have,\\n\\n1\\n|1 2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 | + |2 | \\n(6)\\nThus (4) holds when merging several clusters in 2 with one\\ncluster in 1. As a result, we can greedily merge clusters in 2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by after merging,\\nwe apply a momentum update [42] on the number of clusters , as\\nis commonly used in machine learning to smooth the changing of\\nand avoid sample selection bias. To be specific: = (\\n) + (1 ), where is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batchs GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups , RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries and the average\\ngroup number among all attention module . So RITA samples\\nseveral (, ) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n, we randomly sample integral points (, ) from plane\\n{1 , 1 }. Then we use a binary search based\\nalgorithm to find the maximal batch size that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables and .\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n= (, ) on plane {1 , 1 }.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function , we can estimate a proper\\nbatch size for any (, ) during training, even if it is not seen in\\nthe sampled (, ) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the models robustness.\\n RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In pretraining + few-label finetun-\\ning scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITAs time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate = 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of = 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITAs advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attentions\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITAs advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attentions scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITAs adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number . We\\nvary and the error bound threshold used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed . Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing . More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed . On the ECG dataset,\\nalthough fixed is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when varies, while the results of\\nfixed vary significantly when the value of changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the threshold,\\nwhile it is hard to find an appropriate for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the models ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the cloze test pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a words attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 18771901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T OConnor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241260.\\n[15] Angus Dempster, Franois Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 14541495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n19361962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117128.\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263286.\\n[26] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 11921209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series dataa survey. Pattern recognition\\n38, 11 (2005), 18571874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 13681373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 17621777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 13101318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 11811191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, Franois Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjrgaard, Anind Dey, Tobias Sonne, and Mads Mller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 28282837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 19.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 59986008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD 21). Association for Computing Machinery, New York,\\nNY, USA, 23282337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 2241922430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 55195522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 1728317297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD 21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 21142124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 14. In full-label training scenario, we train the models for\\n100 epochs. In pretraining + few-label finetuning scenario, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPUs capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: ,, ,, \\nEnsure: ,R,R,N,N\\n1: function group_attention(,, )\\n2:\\nfor = 0 1 do\\n3:\\ne1\\n=0 (== )\\n4:\\ne\\n5:\\nfor = 0 1 do\\n6:\\nfor = 0 1 do\\n7:\\n,(e,)\\n8:\\nfor = 0 1 do\\n9:\\n1\\n=0 ,\\n10:\\nfor = 0 1 do\\n11:\\n1\\n=0\\n( e,)\\n\\ne\\n12:\\nreturn \\nIn Alg. 1, we denoteto be the size of the group, to\\nbe the number of groups, rto be the representative key of the \\ngroup and R to be the matrix consisting of all r, to be\\nthe group that kbelongs to. ,are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix for new feature emebddings {1, ...,}, where \\ncorresponds to the feature embedding of . Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: , \\nEnsure: 1 , 1 \\n1: function binary_search(, )\\n2:\\n1\\n3:\\n\\n4:\\n\\n5:\\n\\n6:\\nwhile do\\n7:\\n \\n8:\\n()\\n9:\\n\\n10:\\n\\n\\n11:\\nif 0.9 > then\\n12:\\n+ 1\\n13:\\n\\n14:\\nelse\\n15:\\n1\\n16:\\n+\\n2\\n17:\\nreturn \\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: , , , \\nEnsure: 1 , 1 \\n1: function cost(S)\\n2:\\nif || < then return +\\n3:\\n, , \\n4:\\n(|, )\\nreturn (, , |)\\n5: function dynamic_programming(, , )\\n6:\\nfor 1 = 1 do\\n7:\\nfor 2 = 1 1 do\\n8:\\nfor = 1 1 do\\n9:\\n{2 1, }\\n10:\\n() ()\\n11:\\nfor = 1 do\\n12:\\n{2 1,}\\n13:\\n() ((),() + ())\\n14:\\n2,1 (1)\\n15:\\n16:\\nfor = 1 do\\n17:\\n() (1,)\\n18:\\nfor = 1 do\\n19:\\n() ((),() + (,))\\nreturn ()\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset . When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). () denotes the minimal\\nestimation error for points in sub-plane {2 1, }. In\\nLines 11-13, we enumerate all possible ways of cutting {2 \\n1, } horizontally into two sub-plane {2 1, } and\\n{2 1,} by iterating from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a(1) with\\nminimal estimation error for sub-plane {2 1, 1}, which\\nis recorded as 1,2 in Line 14. () denotes the minimal estimation\\nerror for sub-plane {}. We enumerate all the possible ways\\nof cutting {} vertically into two sub-plane {} and {\\n} by iterating from 1 to (Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as (). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group \\nhave the same key vector, i.e. = (), then the feature\\nembedding produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\nto be the representative vectors of , i.e. e\\n=\\n= (). Algorithm 1 gives that\\ne=\\n1\\n\\n=0\\n(== )v, e,= q r\\n=\\n1\\n\\n=0\\n(e,), e=\\n1\\n\\n=0\\ne,\\n\\ne\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n,= q kj, ,=\\n(,)\\n1\\n=0 (,)\\n, o=\\n1\\n\\n=0\\n,v\\n(8)\\nWith 7 and 8, we have\\n1\\n\\n=0\\n(,) =\\n1\\n\\n=0\\n(q k)\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )(q k)\\n=\\n1\\n\\n=0\\n(q r)\\n1\\n\\n=0\\n(== )\\n=\\n1\\n\\n=0\\n(q r)\\n=\\n1\\n\\n=0\\n(e,)\\n= \\n(9)\\n12\\nFurther,\\no=\\n1\\n\\n=0\\n,vj\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== ),v\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(,)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q k)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q rj)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\n1\\n\\n=0\\n(== )v\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\ne\\n(10)\\nCombining (7), (9) (10), we have oi = N 1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attentions.\\n\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n(,)\\n(,) = (q ek)\\n(q k) = (q (ekk))\\n= (||q||  ||ekk||  (q,ekk))\\n(11)\\nSo\\n() (,)\\n(,) ()\\n(12)\\nThen we have:\\n,\\n,\\n=\\n(,)\\n\\n=1 (,)\\n/\\n(,)\\n\\n=1 (,)\\n= (,)\\n(,)\\n\\n=1 (,)\\n\\n=1 (,)\\n(13)\\nCombining (12) (13), the error is bounded by\\n(2) ,\\n,\\n(2)\\n(14)\\nThus, if d ln()\\n2R , 1\\nAi,j\\nAi,j . This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of to be .After merge-\\ning, the new center will be:\\n =\\n\\n=1 \\n\\n=1 \\nFor [1,], , it holds that:\\n|| || + || ()\\n= || + |\\n\\n=1 \\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n|\\n= || + |\\n\\n=1 ()\\n\\n=1 \\n|\\n= || +\\n| \\n=1 () |\\n\\n=1 \\n|| +\\n\\n=1 ||\\n\\n=1 \\n=\\n\\n=1 (|| + ||)\\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n= \\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where [] Ris\\nthe output representation of [CLS], C is the number of classes, and\\nWcls RCd, Bcls RC are learnable parameters for classification\\ntask. The result vector Rrepresents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nC\\ni=1 y(i)log(y(i)), where is a binary\\nindicator for ground truth label:\\n() =\\n(\\n1\\nis ground truth label\\n0\\n\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries asR, the observed timeseries\\nwith missing values as R, and the set of missing values\\npositions as . We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n(, ) =\\n(\\n1\\n(, ) \\n(, )\\n(, ) \\n(17)\\nis fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +Z2 +... +Zn), where\\nRis the recovered timeseries, and Ris the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n=\\n1\\n||\\n\\n(,)((, ) (, ))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n(, ) =\\n(\\n(, )\\n\\n1\\n\\n(18)\\nWhere is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n'),\n",
       "  Document(metadata={'source': 'Wikipedia', 'title': 'Attention is all you need'}, page_content='Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 140,000 times.\\n\\nPage: All You Need Is Kill\\nSummary: All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Rysuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\nPage: All You Need Is Love\\nSummary: \"All You Need Is Love\" is a song by the English rock band the Beatles that was released as a non-album single in July 1967, with \"Baby, You\\'re a Rich Man\" as its B-side. It was written by John Lennon and credited to the LennonMcCartney partnership. The song was Britain\\'s contribution to Our World, the first live global television link, for which the band were shown performing it at EMI Studios in London on 25 June. The programme was broadcast via satellite and seen by an audience of over 400 million in 25 countries. Lennon\\'s lyrics were deliberately simplistic, to allow for broad appeal to the show\\'s international audience, and captured the utopian ideals associated with the Summer of Love. The single topped sales charts in Britain, the United States and many other countries, and became an anthem for the counterculture\\'s embrace of flower power philosophy.\\nOur World coincide'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/abs/2501.05730v1', 'title': '1. What is the concept of \"Attention is all you need\" and how is it relevant in the field of artificial intelligence?'}, page_content='In just 3 minutes help us improve arXiv: cs arXiv:2501.05730v1 arXiv author ID The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms. Subjects:   Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as:    arXiv:2501.05730 [cs.LG] (or arXiv:2501.05730v1 [cs.LG] for this version) From: Guoxin Feng [view email] Access Paper: cs.LG cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status '),\n",
       "  Document(metadata={'source': 'https://research.google/pubs/attention-is-all-you-need/', 'title': '2. Can you provide a detailed explanation or summary of the \"Attention is all you need\" principle and its significance in neural network models?'}, page_content='The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms')],\n",
       " 'summary': 'Here is a summarized version of the provided documents along with their citations:\\n\\n### 1. \"Attention Is All You Need But You Dont Need All Of It For Inference of Large Language Models\"\\n- **Authors**: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\\n- **Published**: July 22, 2024\\n- **Summary**: This paper addresses the increased demand for inference in large language models (LLMs) and the challenges in maintaining low latency due to the quadratic complexity of attention layers. The authors investigate the effects of dropping certain attention and multi-layer perceptron (MLP) layers in Llama-v2 models during inference. They find that removing deeper attention layers results in only a minor performance drop (1.8% with 33% removal) but significantly improves computing speed, with layer skipping strategies being beneficial. Key benchmarks used include OpenLLM.\\n  \\n### 2. \"All the Attention You Need: Global-local, Spatial-channel Attention for Image Retrieval\" \\n- **Authors**: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\\n- **Published**: July 16, 2021\\n- **Summary**: This research presents the Global-local Attention Module (GLAM), which integrates four types of attention mechanismslocal/global and spatial/channelto improve global image representation for instance-level image retrieval. The authors highlight the significance of attention mechanisms and provide empirical evidence showing improved state-of-the-art results across several benchmarks, focusing on accurately learning embeddings effective for image retrieval.\\n\\n### 3. \"RITA: Group Attention is All You Need for Timeseries Analytics\"\\n- **Authors**: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\\n- **Published**: June 2, 2023\\n- **Summary**: The paper introduces RITA, a tool designed for timeseries analytics that leverages a new group attention mechanism to overcome the scalability issues of traditional transformers when applied to timeseries data. RITA clusters segments of timeseries based on similarity and computes attention efficiently at a coarser granularity. The adaptive nature of RITA\\'s scheduler allows it to adjust dynamically the number of groups and batch size during training. The tool demonstrates significant speed improvements (up to 63X) while maintaining accuracy, outperforming state-of-the-art methods on various tasks.\\n\\n### 4. Background Context on \"Attention Is All You Need\"\\n- **Source**: Wikipedia\\n- **Summary**: The original paper \"Attention Is All You Need,\" published in 2017, introduced the transformer model, revolutionizing deep learning approaches in natural language processing. This architecture relies heavily on attention mechanisms and has shaped the development of large language models, influencing tasks beyond translation to encompass diverse applications like question answering and multimodal generative AI.\\n\\n### 5. Discussion on the \"Attention is All You Need\" Concept\\n- **Source**: arXiv\\n- **Summary**: The architectural advancements from the \"Attention Is All You Need\" paper are evaluated for their potential in creating efficient systems for various applications, particularly in handling long sequences in timeseries data. It highlights approaches including linear attention and state space models that aim to retain performance while reducing computational costs.\\n\\n### 6. Research Overview on Transformer Models\\n- **Source**: Google Research\\n- **Summary**: The paper outlines the limitations of traditional recurrent and convolutional neural networks for sequence transduction and introduces the transformer architecture, which simplifies these models by solely utilizing attention mechanisms. This novel architecture marks a significant shift in the design of neural network models, especially for tasks involving sequential data.\\n\\nEach of these documents contributes to the overarching dialogue surrounding attention mechanisms in machine learning, with particular applications in language models, image retrieval, and timeseries analytics.',\n",
       " 'citations': ['Wikipedia',\n",
       "  'https://arxiv.org/abs/2501.05730v1',\n",
       "  'Arxiv',\n",
       "  'https://research.google/pubs/attention-is-all-you-need/'],\n",
       " 'fact_check': 'Based on the information provided, I cannot fact-check the specific details of the summarized documents as they are not provided in full. However, I can verify the credibility of the sources and authors mentioned in the summaries if needed. Please let me know if you require any specific fact-checking related to the sources or authors themselves.',\n",
       " 'errors': [],\n",
       " 'content': [[Document(metadata={'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}, page_content='Attention Is All You Need But You Dont Need All Of It\\nFor Inference of Large Language Models\\nGeorgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\\nAbstract\\nThe inference demand for LLMs has skyrocketed\\nin recent months, and serving models with low\\nlatencies remains challenging due to the quadratic\\ninput length complexity of the attention layers.\\nIn this work, we investigate the effect of drop-\\nping MLP and attention layers at inference time\\non the performance of Llama-v2 models. We\\nfind that dropping dreeper attention layers only\\nmarginally decreases performance but leads to the\\nbest speedups alongside dropping entire layers.\\nFor example, removing 33% of attention layers\\nin a 13B Llama2 model results in a 1.8% drop in\\naverage performance over the OpenLLM bench-\\nmark. We also observe that skipping layers except\\nthe latter layers reduces performances for more\\nlayers skipped, except for skipping the attention\\nlayers.\\n1. Introduction\\nThe ubiquitous deployment of Large Language Models\\n(LLMs) results in ever-growing amounts of compute spent\\non inference (Patterson et al., 2021; Chen et al., 2023; Kad-\\ndour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\\nther, serving models with low latencies remains challenging\\nbecause contemporary Transformer architectures employ\\nthe self-attention mechanism with quadratic input complex-\\nity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\\nIn this work, we delve deeper into the concept of layer\\nskipping (Fan et al., 2019; Wang et al., 2022a) to reduce\\nthe computation on superfluous LLM components. Our\\nfindings demonstrate that pruning deeper attention layers\\ndoes not significantly affect performance. When applied\\nto Llama-v2 (Touvron et al., 2023b), we maintain good\\nperformance on the OpenLLM (ARC (Clark et al., 2018),\\n*Equal contribution\\n1University College London,\\nUK\\n2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\\n<tyukinegor@gmail.com>.\\nWork presented at TF2M workshop at ICML 2024, Vienna, Austria.\\nPMLR 235, 2024. Copyright 2024 by the author(s).\\nHellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\\n2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\\ning et al., 2023), recording only minimal performance devi-\\nations compared to the full model.\\n2. Method\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\nLayer\\n0.65\\n0.70\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nCosine Similarity\\nCosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\\nLLaMA-v2 7b\\nLLaMA-v2 13b\\nFigure 1. Cosine similarity of Llama-v2 layers with the previous\\nlayer: We observe that the deeper the layer, the more its features\\nare similar to the previous layer except for the very last layer.\\n2.1. Layer skipping\\nConsider a Transformer model M with L layers, each\\nconsisting of an attention sub-layer followed by a multi-\\nlayer perceptron (MLP) sub-layer. We denote each layer as\\nMi = (Attentioni, MLPi) for i {1, 2, . . . , L}.\\nTo compare the performance of Transformer models when\\nskipping specific sub-layers, we create two variants of the\\nmodel:\\n1. Skipping MLP Layers: We construct a model Mskip MLP\\n1\\narXiv:2407.15516v1  [cs.LG]  22 Jul 2024\\nAttention Is All You Need But You Dont Need All Of It\\nby skipping the MLP sub-layer from the last k layers. The\\nresulting model is Mskip MLP = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {(Attentioni, ) | i {L k +\\n1, . . . , L}}.\\n2. Skipping Attention Layers: We construct a model\\nMskip Attention by skipping the attention sub-layer from the\\nlast k layers.\\nThe resulting model is Mskip Attention =\\n{(Attentioni, MLPi)\\n|\\ni\\n\\n{1, 2, . . . , L k}} \\n{(, MLPi) | i {L k + 1, . . . , L}}.\\n3. Skipping Transformer Blocks: We construct a model\\nMskip Attention by skipping the entire last k layers. The re-\\nsulting model is Mskip Block = {(Attentioni, MLPi) | i \\n{1, 2, . . . , L k}} {() | i {L k + 1, . . . , L}}.\\nWe then evaluate the performance of these modified models\\non the OpenLLM benchmark (Beeching et al., 2023), com-\\nparing metrics such as accuracy, computational efficiency,\\nand memory usage. This comparison helps in understand-\\ning the individual contributions of the attention and MLP\\nsub-layers to the overall performance of the Transformer\\nmodel.\\n(a) Skip attention lay-\\ners.\\n(b) Skip attention lay-\\ners,\\nkeep last full\\nblock.\\n(c) Skip ffwd layers.\\n(d) Skip ffwd layers,\\nkeep last full block.\\n(e) Skip full blocks.\\n(f) Skip full blocks,\\nkeep last full block.\\nFigure 2. Skip mechanisms for skipping single layers and entire\\nTransformer blocks (ffwd and attention layers) during inference.\\n2.2. Motivation: Are Deeper Layers More Redundant?\\nIn Transformer models, the last layers have been shown to\\ncontribute less information than earlier layers, making it\\npossible to drop those layers at a minimal performance cost\\n(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\\nSchuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\\n2023).\\nTo verify this, we experiment with removing either the at-\\ntention sublayers or the MLP sublayers. Figure 1 shows the\\ncosine similarities between a layers features and the previ-\\nous layer showing that deeper layers have a lower impact\\non the features than earlier layers. One notable exception\\nto this trend is that the last layer for both Llama-v2 7B and\\n13B has the lowest cosine similarity with the previous layer.\\nPrevious analysis of the attention mechanism has shown\\nthat they can converge to the same value due to attention\\ncollapse (Zhai et al., 2023) and token features that also con-\\nverge to the same value due to over-smoothing (Wang et al.,\\n2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\\n2023), with solutions to these issues typically improving\\nperformance (Ali et al., 2023; Choi et al., 2024).\\n3. Results\\nExperimental Setup\\nFor all experiments, we use either\\nLlama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\\ntwo LLMs trained on trillions of publically available tokens.\\nWe experiment with keeping 66%, 75%, 90% and 100% of\\nthe network and report the corresponding results in Table 1.\\nWe also experiment with removing attention sublayers in\\nTable 2, MLP sublayers in Table 3, and a varying number of\\nlayers similar to Table 1 but keeping the last layer in Table 4.\\n3.1. Chopping Layers\\nTable 1. Llama-v2 skipping full layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.2\\n46.8\\n46.2\\n40.3\\n42.1\\n7B-75%\\n38.3\\n53.0\\n45.1\\n45.9\\n45.6\\n7B-90%\\n47.7\\n69.3\\n39.6\\n46.4\\n50.8\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n37.8\\n46.8\\n45.3\\n51.8\\n45.4\\n13B-75%\\n40.9\\n53.6\\n42.5\\n53.2\\n47.6\\n13B-90%\\n51.3\\n71.3\\n37.1\\n54.8\\n53.6\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nOn all datasets except TruthfulQA, performance drops\\nwhich is expected. It had already been observed that larger\\nlanguage models are less truthful (Lin et al., 2022), but we\\nnow also observe that reducing the size of already trained\\nmodels can also make them more truthful. The observa-\\ntion still holds when the last layer is preserved. Skipping\\n2\\nAttention Is All You Need But You Dont Need All Of It\\nTable 2. Llama-v2 skipping attention sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n51.2\\n77.0\\n42.2\\n39.4\\n52.5\\n7B-75%\\n52.5\\n78.3\\n42.3\\n41.4\\n53.6\\n7B-90%\\n52.8\\n78.9\\n40.0\\n44.0\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n55.6\\n80.1\\n40.1\\n51.3\\n56.8\\n13B-75%\\n55.9\\n79.7\\n39.9\\n52.1\\n56.9\\n13B-90%\\n57.0\\n81.3\\n38.2\\n54.8\\n57.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 3. Llama-v2 skipping ffwd sublayers\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n35.1\\n52.5\\n42.2\\n43.9\\n43.4\\n7B-75%\\n40.4\\n60.3\\n39.2\\n46.3\\n46.6\\n7B-90%\\n48.5\\n71.4\\n38.0\\n46.1\\n51.0\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n41.6\\n56.9\\n40.7\\n53.4\\n48.2\\n13B-75%\\n47.3\\n65.2\\n40.0\\n53.2\\n51.4\\n13B-90%\\n54.2\\n75.8\\n38.3\\n54.7\\n55.8\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nattention layers only leads to better results with only a 1.8%\\ndecrease in performance when keeping 66% of the network\\ncompared to a 13.1% decrease in performance when drop-\\nping dropping the MLP layers only. This seems to indicate\\nthat MLP layers are more important than attention layers, at\\nleast in deeper parts of the network.\\n3.2. Last Layer Inclusion\\nTable 4. Llama-v2 skip full layers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n40.7\\n41.3\\n7B-75%\\n34.5\\n49.4\\n45.9\\n38.3\\n42.0\\n7B-90%\\n46.5\\n73.1\\n41.8\\n41.4\\n50.7\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n19.1\\n37.8\\n13B-75%\\n38.7\\n56.6\\n43.7\\n25.2\\n41.1\\n13B-90%\\n51.2\\n78.1\\n38.0\\n27.1\\n47.9\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nSurprisingly, we notice that skipping layers except the lat-\\nter layers reduces performances for more layers skipped,\\nexcept for skipping the attention layers. This is even more\\nexaggerated compared to just dropping layers, including the\\nlast one. The reason for this could be attributed to the (lack\\nof) robustness of feedforward sublayers, as the last layer\\nnow has to process perturbed information from earlier lay-\\ners. For future work, it would be interesting to see if these\\nperformance drops can be compensated by a small amount\\nTable 5. Llama-v2 skip attention sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n49.3\\n77.1\\n40.5\\n42.5\\n52.4\\n7B-75%\\n51.8\\n78.3\\n41.1\\n44.1\\n53.8\\n7B-90%\\n51.9\\n78.7\\n39.4\\n45.7\\n53.9\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n56.8\\n82.1\\n38.0\\n50.3\\n56.8\\n13B-75%\\n57.5\\n82.1\\n37.0\\n51.4\\n57.0\\n13B-90%\\n58.9\\n82.4\\n36.6\\n54.5\\n58.1\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nTable 6. Llama-v2 skip ffwd sublayers with last layer\\nModel\\nPerformances\\nARC\\nHellaSwag\\nTruthfulQA\\nMMLU\\nAverage\\n7B-66%\\n32.0\\n45.8\\n46.9\\n39.4\\n41.0\\n7B-75%\\n34.5\\n49.4\\n45.9\\n40.2\\n42.5\\n7B-90%\\n46.5\\n73.1\\n41.8\\n40.2\\n50.4\\n7B-100%\\n53.1\\n78.6\\n38.8\\n46.6\\n54.3\\n13B-66%\\n35.1\\n50.0\\n46.9\\n20.4\\n38.1\\n13B-75%\\n38.7\\n56.6\\n43.7\\n33.6\\n43.2\\n13B-90%\\n51.2\\n78.1\\n38.0\\n34.4\\n50.4\\n13B-100%\\n59.6\\n82.1\\n36.9\\n55.4\\n58.5\\nof continued training; since model growing techniques for\\ntraining seem to not suffer from instabilities (Kaddour et al.,\\n2023b).\\n3.3. Compute-matched Comparison\\nTo measure the efficiency of the networks we conducted\\na separate experiment, where we record the time it takes\\nfor the model to output a sequence of length 1, averaging\\nover 1000 sequences. We conducted this experiment for\\nboth 50 and 100 length input sequences. We notice that full\\nlayer droppings do improve time costs the best, followed by\\nattention sublayers, and then feedforward sublayers which\\ndo not impact the speed of processing a lot.\\nWe report the time102 (for clarity) it takes to predict 1\\ntoken for 1000 sequences as well as the percentage improve-\\nment. We show the results of this experiment for Llama 2\\n7B with 0%, 10%, 25%, 33% of layers skipped and we label\\nthese as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\\nTable 7. Llama-v2 time results, 50 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.35\\n32.96\\n36.72\\n21.47\\n43.51\\n6.95\\n7B-75%\\n35.48\\n24.12\\n39.46\\n15.61\\n42.88\\n8.30\\n7B-90%\\n43.31\\n7.38\\n42.93\\n8.19\\n44.17\\n5.53\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\n3\\nAttention Is All You Need But You Dont Need All Of It\\nTable 8. Llama-v2 time results, 50 length sequence, last layer in-\\ncluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n31.78\\n32.04\\n36.92\\n21.04\\n41.31\\n11.66\\n7B-75%\\n34.98\\n25.19\\n40.24\\n13.94\\n42.62\\n8.85\\n7B-90%\\n40.92\\n12.49\\n42.43\\n9.26\\n43.51\\n6.95\\n7B-100%\\n46.76\\n0\\n-\\n-\\n-\\n-\\nTable 9. Llama-v2 time results, 100 length sequence, no last layer\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.36\\n32.58\\n38.97\\n18.18\\n43.08\\n10.25\\n7B-75%\\n36.58\\n23.79\\n41.27\\n14.02\\n44.13\\n8.06\\n7B-90%\\n43.65\\n9.06\\n44.62\\n7.04\\n46.30\\n3.54\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\nTable 10. Llama-v2 time results, 100 length sequence, last layer\\nincluded\\nModel\\nFull\\nAttention\\nffwd\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\nTime(s) 102\\n(%)\\n7B-66%\\n32.05\\n33.23\\n38.52\\n19.75\\n42.66\\n11.13\\n7B-75%\\n36.41\\n24.15\\n41.00\\n14.58\\n43.92\\n8.50\\n7B-90%\\n43.28\\n9.83\\n44.27\\n7.77\\n45.20\\n5.83\\n7B-100%\\n48.00\\n0\\n-\\n-\\n-\\n-\\n4. Related Work\\nEarly Exit during inference\\nEarly exit methods have also\\nbeen proposed in other domains (Graves, 2017; Teerapit-\\ntayanon et al., 2017) before getting adapted to autoregressive\\nmodels (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\\n2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\\n2024). The idea works by dynamically allocating compute\\nbased on the difficulty of the input sequence. Our method\\nprunes the deepest layers and does not involve any level of\\nadaptability. This is beneficial because it does not require\\nthe entire model to be loaded in memory. Dropping layers\\nduring inference has been done on BERT-like models in\\n(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\\nanalysis to more recent LLMs and study the impact of skip-\\nping attention and/or MLP layers in more detail. Concurrent\\nwork to ours by Gromov et al. (2024) yields similar results\\nby pruning deeper layers and applying fine-tuning on the\\npruned model.\\nLayer dropping/growing during training\\nThere are var-\\nious works studying the dropping/growing layers dynami-\\ncally during training (Fan et al., 2019; Gong et al., 2019;\\nKaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\\ncontrast, this work focuses on dropping layers of an already\\npre-trained model in a way similar to Men et al. (2024).\\nOther Inference Speedup Methods\\nOther works to speed\\nup inference include compressing KV caches (Nawrot et al.,\\n2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\\n(Chen et al., 2023), efficient memory management (Kwon\\net al., 2023), or subqudratic attention architectures (Fu et al.,\\n2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\\nbeen provided by Kaddour et al. (2023a).\\n5. Conclusion\\nWe investigated the effect of dropping the last layers from\\nthe 7B and 13B Llama2 models. We observe that dropping\\nattention sublayers lead to much lower drops in performance\\nthan dropping the MLP sublayers, whether the last layer\\nis included or not, while also leading to better inference\\nspeedups. For example, removing 33% of attention layers\\nleads to an 18% speedup in a 13B Llama2 model at the cost\\nof a 1.8% drop in average performance. This shows that\\nmassive improvements can be made over dropping entire\\nlayers from just dropping the attention sublayer.\\nReferences\\nAli, A., Galanti, T., and Wolf, L. Centered self-attention\\nlayers, 2023.\\nBeeching,\\nE.,\\nFourrier,\\nC.,\\nHabib,\\nN.,\\nHan,\\nS.,\\nLambert,\\nN.,\\nRajani,\\nN.,\\nSanseviero,\\nO.,\\nTun-\\nstall,\\nL.,\\nand\\nWolf,\\nT.\\nOpen\\nllm\\nleader-\\nboard.\\nhttps://huggingface.co/spaces/\\nHuggingFaceH4/open_llm_leaderboard,\\n2023.\\nBelrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\\nMcKinney, L., Biderman, S., and Steinhardt, J. Eliciting\\nlatent predictions from transformers with the tuned lens.\\narXiv preprint arXiv:2303.08112, 2023.\\nBi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\\nDing, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\\nScaling open-source language models with longtermism.\\narXiv preprint arXiv:2401.02954, 2024.\\nChen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\\nJumper, J. Accelerating large language model decoding\\nwith speculative sampling. CoRR, abs/2302.01318, 2023.\\ndoi: 10.48550/ARXIV.2302.01318. URL https://\\ndoi.org/10.48550/arXiv.2302.01318.\\nChen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\\nLarge-scale training and inference of early-exit large lan-\\nguage models with 3d parallelism, 2024.\\nChoi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\\nPark, N. Graph convolutions enrich the self-attention in\\ntransformers!, 2024.\\nClark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\\nSchoenick, C., and Tafjord, O. Think you have solved\\n4\\nAttention Is All You Need But You Dont Need All Of It\\nquestion answering? try arc, the ai2 reasoning challenge,\\n2018.\\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\\nto conclusions: Short-cutting transformers with linear\\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\\nDong, Y., Cordonnier, J.-B., and Loukas, A. Attention\\nis not all you need: Pure attention loses rank doubly\\nexponentially with depth, 2023.\\nDovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\\nSetting the record straight on transformer oversmoothing,\\n2024.\\nElbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\\ntransformer. In International Conference on Learning\\nRepresentations, 2020. URL https://openreview.\\nnet/forum?id=SJg7KhVKPH.\\nElhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\\nWasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\\nS., Roman, A., et al. Layer skip: Enabling early exit\\ninference and self-speculative decoding. arXiv preprint\\narXiv:2404.16710, 2024.\\nFan, A., Grave, E., and Joulin, A. Reducing transformer\\ndepth on demand with structured dropout, 2019.\\nFan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\\nA., Wang, Y., and Wang, Z. Not all layers of llms are\\nnecessary during inference, 2024.\\nFu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\\nA., and Re, C. Hungry hungry hippos: Towards lan-\\nguage modeling with state space models. arXiv preprint\\narXiv:2212.14052, 2022.\\nGong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\\nEfficient training of bert by progressively stacking. In\\nInternational conference on machine learning, pp. 2337\\n2346. PMLR, 2019.\\nGraves, A. Adaptive computation time for recurrent neural\\nnetworks, 2017.\\nGromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\\nRoberts, D. A. The unreasonable ineffectiveness of the\\ndeeper layers, 2024.\\nGu, A. and Dao, T. Mamba: Linear-time sequence modeling\\nwith selective state spaces, 2023.\\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\\nSong, D., and Steinhardt, J. Measuring massive multitask\\nlanguage understanding, 2021.\\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\\nChaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\\nLample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\\narXiv:2310.06825, 2023.\\nJiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\\nLearning\\nlayer-skippable inference network. IEEE Transactions on\\nImage Processing, 29:87478759, 2020. doi: 10.1109/\\nTIP.2020.3018269.\\nKaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\\nR., and McHardy, R. Challenges and applications of\\nlarge language models. CoRR, abs/2307.10169, 2023a.\\ndoi: 10.48550/ARXIV.2307.10169. URL https://\\ndoi.org/10.48550/arXiv.2307.10169.\\nKaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\\nM. J.\\nNo train no gain: Revisiting efficient training\\nalgorithms for transformer-based language models. In\\nOh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\\nM., and Levine, S. (eds.), Advances in Neural Information\\nProcessing Systems 36: Annual Conference on Neural\\nInformation Processing Systems 2023, NeurIPS 2023,\\nNew Orleans, LA, USA, December 10 - 16, 2023, 2023b.\\nKwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\\nC. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\\nmemory management for large language model serving\\nwith pagedattention. In Proceedings of the 29th Sym-\\nposium on Operating Systems Principles, pp. 611626,\\n2023.\\nLin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\\nhow models mimic human falsehoods, 2022.\\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\\nShrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\\nB. Deja vu: Contextual sparsity for efficient LLMs at\\ninference time. In Krause, A., Brunskill, E., Cho, K.,\\nEngelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\\nceedings of the 40th International Conference on Ma-\\nchine Learning, volume 202 of Proceedings of Machine\\nLearning Research, pp. 2213722176. PMLR, 2329 Jul\\n2023. URL https://proceedings.mlr.press/\\nv202/liu23am.html.\\nMen, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\\nX., and Chen, W. Shortgpt: Layers in large language\\nmodels are more redundant than you expect, 2024. URL\\nhttps://arxiv.org/abs/2403.03853.\\nNawrot, P., ancucki, A., Chochowski, M., Tarjan, D., and\\nPonti, E. M. Dynamic memory compression: Retrofitting\\nllms for accelerated inference, 2024.\\nPatterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\\nL., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\\nbon emissions and large neural network training. CoRR,\\n5\\nAttention Is All You Need But You Dont Need All Of It\\nabs/2104.10350, 2021. URL https://arxiv.org/\\nabs/2104.10350.\\nPeng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\\nS., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\\net al. Rwkv: Reinventing rnns for the transformer era.\\narXiv preprint arXiv:2305.13048, 2023.\\nReid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\\ncrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\\nO., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\\nmodal understanding across millions of tokens of context.\\narXiv preprint arXiv:2403.05530, 2024.\\nSajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\\neffect of dropping layers of pre-trained transformer mod-\\nels.\\nComputer Speech & Language, 77:101429, jan\\n2023. doi: 10.1016/j.csl.2022.101429. URL https:\\n//doi.org/10.1016%2Fj.csl.2022.101429.\\nSchuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\\nTran, V., Tay, Y., and Metzler, D. Confident adaptive\\nlanguage modeling. Advances in Neural Information\\nProcessing Systems, 35:1745617472, 2022.\\nTeerapittayanon, S., McDanel, B., and Kung, H. T.\\nBranchynet: Fast inference via early exiting from deep\\nneural networks, 2017.\\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\\nAzhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\\nple, G. Llama: Open and efficient foundation language\\nmodels, 2023a.\\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\\nand Scialom, T. Llama 2: Open foundation and fine-tuned\\nchat models, 2023b.\\nWang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\\nSkipbert: Efficient inference with shallow layer skipping.\\nIn Proceedings of the 60th Annual Meeting of the Asso-\\nciation for Computational Linguistics (Volume 1: Long\\nPapers), pp. 72877301, 2022a.\\nWang, P., Zheng, W., Chen, T., and Wang, Z.\\nAnti-\\noversmoothing in deep vision transformers via the fourier\\ndomain analysis:\\nFrom theory to practice.\\nIn In-\\nternational Conference on Learning Representations,\\n2022b. URL https://openreview.net/forum?\\nid=O476oWmiNNp.\\nWu, H. and Tu, K. Layer-condensed kv cache for efficient\\ninference of large language models, 2024.\\nXia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\\nLi, W., and Sui, Z. Unlocking efficiency in large language\\nmodel inference: A comprehensive survey of speculative\\ndecoding. arXiv preprint arXiv:2401.07851, 2024.\\nZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\\nHellaswag: Can a machine really finish your sentence?,\\n2019.\\nZhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\\nRamapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\\nbilizing transformer training by preventing attention en-\\ntropy collapse, 2023.\\nZhang, M. and He, Y. Accelerating training of transformer-\\nbased language models with progressive layer dropping.\\nAdvances in neural information processing systems, 33:\\n1401114023, 2020.\\n6\\n'),\n",
       "   Document(metadata={'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}, page_content='All the attention you need:\\nGlobal-local, spatial-channel attention for image retrieval\\nChull Hwan Song\\nOdd Concepts\\nHye Joo Han\\nOdd Concepts\\nYannis Avrithis\\nInria, Univ Rennes, CNRS, IRISA\\nAbstract\\nWe address representation learning for large-scale\\ninstance-level image retrieval. Apart from backbone, train-\\ning pipelines and loss functions, popular approaches have\\nfocused on different spatial pooling and attention mecha-\\nnisms, which are at the core of learning a powerful global\\nimage representation. There are different forms of attention\\naccording to the interaction of elements of the feature tensor\\n(local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses\\nonly one or two forms of attention and applies it to different\\nproblems like classication, detection or retrieval.\\nWe present global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network and\\nincorporates all four forms of attention: local and global,\\nspatial and channel. We obtain a new feature tensor and, by\\nspatial pooling, we learn a powerful embedding for image\\nretrieval. Focusing on global descriptors, we provide em-\\npirical evidence of the interaction of all forms of attention\\nand improve the state of the art on standard benchmarks.\\n1. Introduction\\nInstance-level image retrieval is at the core of visual rep-\\nresentation learning and is connected with many problems\\nof visual recognition and machine learning, for instance\\nmetric learning [30, 26], few-shot learning [42] and unsu-\\npervised learning [8]. Many large-scale open datasets [3,\\n37, 16, 29, 53], and competitions1 have accelerated progress\\nin instance-level image retrieval, which has been trans-\\nformed by deep learning [3].\\nMany studies on instance-level image retrieval focus\\non learning features from convolutional neural networks\\n(CNN), while others focus on re-ranking, for instance by\\ngraph-based methods [11]. The former can be distinguished\\naccording to feature types: local descriptors, reminiscent of\\nSIFT [27], where an image is mapped to a few hundred vec-\\ntors; and global descriptors, where an image is mapped to a\\n1https://www.kaggle.com/c/landmark-retrieval-2020\\nsingle vector. In fact, deep learning has brought global de-\\nscriptors with astounding performance, while allowing ef-\\ncient search. Our study belongs to this type.\\nStudies on global descriptors have focused on spatial\\npooling [2, 37]. The need for compact, discriminative rep-\\nresentations that are resistant to clutter has naturally given\\nrise to spatial attention methods [24, 28]. Different kinds\\nof attention have been studied in many areas of computer\\nvision research. There is also channel attention [20, 9]; lo-\\ncal attention, applied independently to elements of the rep-\\nresentation (feature map) [54, 25]; global attention, based\\non interaction between elements [52, 9]; and combinations\\nthereof. Unfortunately, each study has been limited to one or\\ntwo kinds of attention only; attention is not always learned;\\nand applications vary.\\nIt is the objective of our work to perform a compre-\\nhensive study of all forms of attention above, apply them\\nto instance-level image retrieval and provide a detailed ac-\\ncount of their interaction and impact on performance. As\\nshown in Figure 1, we collect contextual information from\\nimages with both local and global attention, giving rise to\\ntwo parallel network streams. Importantly, each operates\\non both spatial locations and feature channels. Local at-\\ntention is about individual locations and channels; global is\\nabout interaction between locations and between channels.\\nThe extracted information is separately embedded in local\\nand global attention feature maps, which are combined in a\\nglobal-local attention feature map before pooling.\\nOur contributions can be summarized as follows:\\n1. We propose a novel network that consists of both\\nglobal and local attention for image retrieval. This is\\nthe rst study that employs both mechanisms.\\n2. Each of the global and local attention mechanisms\\ncomprises both spatial and channel attention.\\n3. Focusing on global descriptors, we provide empirical\\nevidence of the interaction of all forms of attention and\\nimprove the state of the art on standard benchmarks.\\n1\\narXiv:2107.08000v1  [cs.CV]  16 Jul 2021\\nAl\\nc\\nc  1  1\\n\\n+\\nFl\\nc\\nAl\\ns\\n1  h  w\\n\\n+\\nFl\\n\\nc  h  w\\nF\\n\\n+\\nc  h  w\\nFgl\\nAg\\nc\\nc  c\\n\\nFg\\nc\\nAg\\ns\\nhw  hw\\n\\n+\\nFg\\n\\nwl\\nw\\nwg\\nchannel attention\\nspatial attention\\nfusion\\nlocal attention\\nglobal attention\\nFigure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\\ntion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\\n(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\\nc),\\nlocal spatial (Al\\ns), global channel (Ag\\nc) and global spatial (Ag\\ns). The input feature map F is weighted into local (Fl) and\\nglobal (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\\nis abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\\n2. Related work\\nInstance-level image retrieval\\nStudies on instance-level\\nimage retrieval can be roughly, but not exclusively, di-\\nvided into three types: (1) studies on global descriptors\\n[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\\ngeometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\\nby graph-based methods [11, 21, 55]. The rst two types\\nof studies focus on the feature representation, while the last\\ntype focuses on re-ranking extracted features.\\nStudies on global descriptors focus on spatial pooling\\nof CNN feature maps into vectors, including MAC [38],\\nSPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\\nand NetVLAD [1, 25], as well as learning the representa-\\ntion [3, 15, 16, 36, 37]. Studies before deep learning dom-\\ninated image retrieval were mostly based on local descrip-\\ntors like SIFT [27] and bag-of-words representation [32] or\\naggregated descriptors like VLAD [22] or ASMK [46]. Lo-\\ncal descriptors have been revived in deep learning, e.g. with\\nDELF [29], DELG [5] and ASMK extensions [45, 47].\\nWe focus on learning a global descriptor in this work, be-\\ncause it is the most efcient in terms of storage and search.\\nHowever, our generic attention mechanism produces a fea-\\nture tensor and could be applicable to local descriptors as\\nwell, if global pooling were replaced by local feature detec-\\ntion. Re-ranking methods are complementary to the repre-\\nsentation and we do not consider them in this work.\\nAttention\\nAttention mechanisms have been rst proposed\\nin image classication studies focusing on channel at-\\nMETHOD\\nLOCAL\\nGLOBAL\\nLRN RET\\nSpatial Channel Spatial Channel\\nSENet [20]\\n\\n\\nECA-Net [51]\\n\\n\\nGCNet [6]\\n\\n\\nCBAM [54]\\n\\n\\n\\nGE [19]\\n\\n\\nNL-Net [52]\\n\\n\\nAA-Net [4]\\n\\n\\nSAN [59]\\n\\n\\nN3Net [34]\\n\\n\\nA2-Net [9]\\n\\n\\nGSoP [14]\\n\\n\\nOnA [23]\\n\\n\\nAGeM [17]\\n\\n\\nCroW [24]\\n\\n\\n\\nCRN [25]\\n\\n\\n\\nDELF [29]\\n\\n\\n\\nDELG [5]\\n\\n\\n\\nTolias et al. [47]\\n\\n\\n\\nSOLAR [28]\\n\\n\\n\\nOurs\\n\\n\\n\\n\\n\\n\\nTable 1: Related work on attention. LRN: learned; RET: ap-\\nplied to instance-level image retrieval.\\ntention [20, 51, 6], spatial attention [19] or both, like\\nCBAM [54]. In image retrieval, CroW [24] also employs\\n2\\nfeature map\\nGAP\\nconv1d(k)\\nsigmoid\\nattention map\\nc  h  w\\nc  1  1\\nc  1  1\\nF\\nAl\\nc\\nFigure 2: Local channel attention.\\nboth spatial and channel attention and can be seen as a pre-\\ncursor of CBAM, but, like other studies of spatial attention\\non retrieval [41, 23, 17], it is not learned. CRN [25] ap-\\nplies spatial attention for feature reweighting and is learned.\\nLearned spatial attention mechanisms are common for local\\ndescriptors [29, 5, 47].\\nWe call the above methods local attention, in the sense\\nthat elements of the feature tensor (channels / spatial loca-\\ntions), are weighted independently, based on contextual in-\\nformation obtained by pooling or learned. By constrast, by\\nglobal attention we refer to mechanisms that model inter-\\naction between elements of the feature tensor, for example\\nbetween channels or between locations.\\nIn image classication, non-local neural network (NL-\\nNet) [52] is maybe the rst global attention mechanism, fol-\\nlowed by similar studies [4, 59, 34]. It is global spatial at-\\ntention, allowing interaction between any pair of spatial lo-\\ncations. Similarly, there are studies of global channel atten-\\ntion, allowing interaction between channels [9, 14]. Global\\nattention has focused mostly on image recognition and has\\nbeen applied to either spatial or channel attention so far, not\\nboth. In image retrieval, SOLAR [28] is a direct application\\nof the global spatial attention mechanism of [52].\\nTable 1 attempts to categorize related work on atten-\\ntion according to whether attention is local or global, spa-\\ntial or channel, whether it is learned and whether it is ap-\\nplied to instance-level image retrieval. We observe that all\\nmethods limit to one or two forms of attention only. Of\\nthose studies that focus on image retrieval, many are not\\nlearned [23, 17, 24], and of those that are, some are de-\\nsigned for local descriptors [29, 47].\\nBy contrast, we provide a comprehensive study of all\\nforms of attention, global and local, spatial and channel, to\\nobtain a learned representation in the form of a tensor that\\ncan be used in any way. We spatially pool it into a global\\ndescriptor and we study the relative gain of different forms\\nof attention in image retrieval.\\nfeature map\\nconv 1  1\\nconv 3  3\\nconv 5  5\\nconv 7  7\\nconcat\\nconv 1  1\\nattention map\\nc  h  w\\n4c  h  w\\n1  h  w\\nc  h  w\\ndilated\\nconv\\nF\\nF\\nAl\\ns\\nFigure 3: Local spatial attention. Convolutional layers in\\nblue implemented by dilated convolutions with kernel size\\n3  3 and dilation factors 1, 3, 5.\\n3. Global-local attention\\nWe design a global-local attention module (GLAM),\\nwhich is attached at the end of a backbone network. Figure 1\\nillustrates its main components. We are given a c  h  w\\nfeature tensor F, where c is the number of channels, and\\nh  w is the spatial resolution. Local attention collects con-\\ntext from the image and applies pooling to obtain a c11\\nlocal channel attention map Al\\nc and a 1  h  w local spa-\\ntial attention map Al\\ns. Global attention allows interaction\\nbetween channels, resulting in a c  c global channel at-\\ntention map Ag\\nc, and between spatial locations, resulting in\\na hw  hw global spatial attention map Ag\\ns. The feature\\nmaps produced by the two attention streams are combined\\nwith the original one by a learned fusion mechanism into\\nthe global-local attention feature map Fgl before being spa-\\ntially pooled into a global image descriptor.\\n3.1. Local attention\\nWe extract an 1D channel and a 2D spatial attention map\\nto weigh the feature map in the corresponding dimensions.\\nLocal channel attention\\nFollowing ECA-Net [51], this\\nattention captures local channel information. As shown in\\nFigure 2, we are given a chw feature tensor F from our\\nbackbone. We rst reduce it to a c  1  1 tensor by global\\naverage pooling (GAP). Channel attention is then captured\\nby a 1D convolution of kernel size k along the channel di-\\nmension, where k controls the extent of cross-channel inter-\\naction. This is followed by a sigmoid function, resulting in\\nthe c  1  1 local channel attention map Al\\nc.\\nLocal spatial attention\\nInspired by the inception mod-\\nule [43] and similar to [25], this attention map captures local\\nspatial information at different scales. As shown in Figure 3,\\n3\\nfeature map\\nGAP\\nconv1d(k)\\nconv1d(k)\\nsigmoid\\nsigmoid\\n\\n\\nsoftmax\\nattention feature map\\n1  c\\n1  c\\n1  c\\nQc\\nc  c\\nhw  c\\nVc\\nAg\\nc\\nc  h  w\\n1  c\\n1  c\\nKc\\nF\\nGc\\nFigure 4: Global channel attention.\\ngiven the same c  h  w feature tensor F from our back-\\nbone, we obtain a new tensor F with channels reduced to\\nc, using a 1  1 convolution. We then extract local spatial\\ncontextual information using convolutional lters of kernel\\nsize 3  3, 5  5, and 7  7, which are efciently imple-\\nmented by 3  3 dilated convolutions [7, 57] with dilation\\nparameter 1, 2, and 3 respectively. The resulting features,\\nalong with one obtained by 1  1 convolution on F, are\\nconcatenated into a 4c  h  w tensor. Finally, we obtain\\nthe 1  h  w local spatial attention map Al\\ns by a 1  1\\nconvolution that reduces the channel dimension to 1.\\nThe middle column of Figure 6 shows heat maps of local\\nspatial attention, localizing target objects in images.\\nLocal attention feature map\\nWe use the local channel\\nattention map Al\\nc to weigh F in the channel dimension\\nFl\\nc := F Al\\nc + F.\\n(1)\\nWe then use local spatial attention map Al\\ns to weigh Fl\\nc\\nin the spatial dimensions, resulting in the c  h  w local\\nattention feature map\\nFl = Fl\\nc Al\\ns + Fl\\nc.\\n(2)\\nHere, AB denotes an element-wise multiplication of ten-\\nsors A and B, with broadcasting when one tensor is smaller.\\nWe adopt the choice of applying channel followed by spa-\\ntial attention from convolutional block attention module\\nCBAM [54]. However, apart from computing Al\\ns at differ-\\nent scales, both attention maps are obtained from the orig-\\ninal tensor F rather than sequentially. In addition, both (1)\\nand (2) include residual connections, while CBAM includes\\na single residual connection over both steps.\\n3.2. Global attention\\nWe extract two matrices capturing global pairwise chan-\\nnel and spatial interaction to weigh the feature map.\\nfeature map\\nconv 1  1\\nconv 1  1\\nconv 1  1\\n\\n\\nsoftmax\\nconv 1  1\\nattention feature map\\nc  hw\\nQs\\nhw  hw\\nc  h  w\\nc  hw\\nVs\\nc  h  w\\nAg\\ns\\nc  h  w\\nc  hw\\nKc\\nF\\nGs\\nFigure 5: Global spatial attention.\\nGlobal channel attention\\nWe introduce a global channel\\nattention mechanism that captures global channel interac-\\ntion. This mechanism is based on the non-local neural net-\\nwork [52], but with the idea of 1D convolution from ECA-\\nNet [51]. As shown in Figure 4, we are given the c  h  w\\nfeature tensor F from our backbone. We apply GAP and\\nsqueeze spatial dimensions, followed by a 1D convolution\\nof kernel size k and a sigmoid function, to obtain 1c query\\nQc and key Kc tensors. The value tensor Vc is obtained by\\nmere reshaping of F to hwc, without GAP. Next, we form\\nthe outer product of Kc and Qc, followed by softmax over\\nchannels to obtain a c  c global channel attention map\\nAg\\nc = softmax(Kc\\nQc).\\n(3)\\nFinally, this attention map is multiplied with Vc and the ma-\\ntrix product VcAg\\nc is reshaped back to chw to give the\\nglobal channel attention feature map Gc. In GSoP [14] and\\nA2-Net [9], a cc global channel attention map is obtained\\nby multiplication of hw  c matrices; (3) is more efcient,\\nusing only an outer product of 1  c vectors.\\nGlobal spatial attention\\nSince ordinary convolution ap-\\nplies only a local neighborhood at a time, it cannot capture\\nglobal contextual information. Thus, we apply non-local l-\\ntering [52], which is a form of self-attention [49] in the spa-\\ntial dimensions. As shown in Figure 5, we are given the\\nsame c  h  w feature tensor F from our backbone. By\\nusing three 11 convolutions, which reduce channels to c,\\nand attening spatial dimensions to hw, we obtain c  hw\\nquery Qs, key Ks, and value Vs tensors, where each col-\\numn is a feature vector corresponding to a particular spatial\\nlocation. We capture pairwise similarities of these vectors\\nby matrix multiplication of Ks and Qs, followed by soft-\\nmax over locations to obtain a hw  hw global spatial at-\\ntention map:\\nAg\\ns = softmax(K\\ns Qs).\\n(4)\\n4\\nThis attention map is multiplied with Vs and the matrix\\nproduct VsAg\\ns is reshaped back to c hw by expanding\\nthe spatial dimensions. Finally, using a 1  1 convolution,\\nwhich increases channels back to c, we obtain the chw\\nglobal spatial attention feature map Gs.\\nThe right column of Figure 6 shows heat maps for global\\nspatial attention, localizing target objects in images.\\nGlobal attention feature map\\nWe use the global channel\\nattention feature map Fc to weigh F element-wise\\nFg\\nc = F Gc.\\n(5)\\nWe then use global spatial attention feature map Gs to\\nweigh Fg\\nc element-wise, resulting in the c  h  w global\\nattention feature map\\nFg = Fg\\nc Gs + Fg\\nc.\\n(6)\\nSimilarly to Fl in (1) and (2), we apply channel attention\\nrst, followed by spatial attention. However, unlike (1),\\nthere is no residual connection in (5). This choice is sup-\\nported by early experiments.\\n3.3. Global-local attention\\nFeature fusion\\nAs shown in Figure 1, we combine the\\nlocal and global attention feature maps, Fl and Fg, with\\nthe original feature F. While concatenation and summation\\nare common operations for feature combination, we use a\\nweighted average with weights wl, wg, w respectively, ob-\\ntained by softmax over three learnable scalar parameters, to\\nobtain a c  h  w global-local attention feature map\\nFgl = wlFl + wgFl + wF.\\n(7)\\nEfcientDet [44] has shown that this is the most effective,\\namong a number of choices, for fusion of features across\\ndifferent scales.\\nPooling\\nWe apply GeM [37], a learnable spatial pooling\\nmechanism, to feature map Fgl (7), followed by a fully-\\nconnected (FC) layer with dropout and batch normalization.\\nThe nal embedding is obtained by 2-normalization.\\n4. Experiments\\n4.1. Datasets\\nTraining set\\nThere are a number of open landmark\\ndatasets commonly used for training in image retrieval stud-\\nies, including neural code (NC) [3], neural code clean (NC-\\nclean) [16], as well as Google Landmarks v1 (GLDv1) [29]\\nand v2 (GLDv2) [53]. Table 2 shows relevant statistics.\\nThese datasets can be categorized into noisy and clean. The\\nclean sets were obtained from the original noisy sets for\\nmore effective training [16, 53]. The original noisy datasets\\nare much larger, but they have high intra-class variability.\\n(a) input\\n(b) local\\n(c) global\\nFigure 6: Local and global spatial attention. Left: input\\nimages. Middle: local spatial attention heat maps. Right:\\nglobal spatial attention heat maps. Red (blue) means higher\\n(lower) attention weight.\\nEach class can include visually dissimilar images such as\\nexterior and interior views of a building or landmark, in-\\ncluding oor plans and paintings inside. The clean datasets\\nfocus on views directly relevant to landmark recognition but\\nhave a much smaller number of images.\\nEvaluation set and metrics\\nWe use four common eval-\\nuation datasets for landmark image retrieval: Oxford5k\\n(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\\nford (ROxford or ROxf) and Paris (RParis or RPar) [35].\\nROxford and RParis are used with and without one million\\ndistractors (R1M) [28] and evaluated using the Medium and\\nHard protocols [35]. We evaluate using mean Average Pre-\\ncision (mAP) and mean precision at 10 (mP@10).\\n4.2. Implementation details\\nWe train on 8 TITAN RTX 2080Ti GPUs. All models are\\npre-trained on ImageNet [39] and implemented in PyTorch\\n[31]. For fair comparisons, we set a training environment\\n5\\nFigure 7: Examples of our ranking results. In each row, the rst image on the left (pink dotted outline) is a query image with a\\ntarget object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\\nfor the query; red solid outline: negative.\\nsimilar to the those of compared studies [56, 53, 28, 35]. We\\nemploy ResNet101 [18] as a backbone model. The kernel\\nsize k of ECANet in subsection 3.1 is set to 3. The param-\\neter p of GeM in subsection 3.3 is set to 3 and the dimen-\\nsion d of nal embeddings to 512. We adopt ArcFace [10],\\na cosine-softmax based loss, with a margin of 0.3. We use\\nstochastic gradient descent with initial learning rate 103,\\nmomentum 0.9 and weight decay 105.\\nWe adopt the batch sampling of Yokoo et al. [56] where\\nmini-batch samples with similar aspect ratios are resized to\\na particular size. Here, we use a batch size of 64. For image\\naugmentation, we apply scaling, random cropping, and var-\\nied illumination. At inference, we apply a multi-resolution\\nrepresentation [16] to query and database images.\\nOur method is denoted as GLAM (global-local atten-\\ntion module). Using the backbone model alone is referred\\nto as baseline. It is compatible with recent models based\\non ResNet101-GeM trained with ArcFace [53, 28]. Adding\\nour local attention (subsection 3.1) to the baseline model is\\ndenoted +local, while adding our global attention (subsec-\\ntion 3.2) is denoted +global. Since we focus on representa-\\ntion learning, we do not consider post-processing methods\\nlike geometry-based re-ranking [29, 40, 53] or graph-based\\nre-ranking [11, 21, 55].\\n4.3. Benchmarking\\nNoisy vs. clean training sets\\nWe begin by training our\\nbest model (baseline+local+global) on all training sets of\\nTable 2, except NC-noisy because some images are cur-\\nrently unavailable. As shown in Table 3, even though\\nTRAIN SET\\n#IMAGES\\n#CLASSES\\nNC-noisy\\n213,678\\n672\\nNC-clean\\n27,965\\n581\\nSfM-120k\\n117,369\\n713\\nGLDv1-noisy\\n1,225,029\\n14, 951\\nGLDv2-noisy\\n4,132,914\\n203,094\\nGLDv2-clean\\n1,580,470\\n81,313\\nTable 2: Statistics of different training sets.\\nMETHOD\\nTRAIN SET\\nDIM OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGeM-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7\\n77.2\\n38.5\\n56.3\\nSOLAR [28]\\nGLDv1-noisy 2048\\n\\n\\n69.9\\n81.6\\n47.9\\n64.5\\nGLDv2 [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n84.9\\n51.6\\n70.3\\nGLAM (Ours)\\nNC-clean\\n512\\n77.8\\n85.8\\n51.6\\n68.1\\n20.9\\n44.7\\nGLDv1-noisy 512\\n92.8\\n95.0\\n73.7\\n83.5\\n49.8\\n69.4\\nGLDv2-noisy 512\\n93.3\\n95.3\\n75.7\\n86.0\\n53.1\\n73.8\\nGLDv2-clean 512\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 3: mAP comparison of our best model (base-\\nline+local+global) trained on different training sets against\\n[53, 28]. All models use ResNet101-GeM. Red: best results.\\nBlue: GLAM higher than SOLAR [28] on GLDv1-noisy.\\nGLDv2-noisy has 2.6 times more images than GLDv2-\\nclean, the latter is superior by a large margin. This shows\\nthat, in training, a cleaner dataset can be more important\\nthan a larger one. By contrast, NC-clean has the worst\\nperformance despite being clean, aparently because it is\\n6\\nMETHOD\\nTRAIN SET\\nDIM\\nBASE\\nMEDIUM\\nHARD\\nOx5k Par6k\\nROxf\\n+R1M\\nRPar\\n+R1M\\nROxf\\n+R1M\\nRPar\\n+R1M\\nmAP\\nmAP mAP mP mAP mP mAP mP mAP mP\\nmAP mP mAP mP mAP mP mAP mP\\nSPoC-V16 [2, 35]\\n[O]\\n512\\n53.1\\n\\n38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\\n0.9\\n2.9\\n32.4 69.7\\n7.6\\n30.6\\nSPoC-R101 [35]\\n[O]\\n2048\\n\\n\\n39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\\n2.8\\n5.6\\n44.7 78.0 15.3 54.4\\nCroW-V16 [24, 35]\\n[O]\\n512\\n70.8\\n79.7\\n41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\\n3.0\\n6.6\\n36.9 77.9 10.3 45.1\\nCroW-R101 [35]\\n[O]\\n2048\\n\\n\\n42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\\n3.3\\n9.3\\n47.2 83.6 16.3 61.6\\nMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.0\\n82.9\\n37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\\n7.4\\n11.9 35.9 78.4 13.2 54.7\\nMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\\n5.7\\n14.4 44.1 86.3 18.2 67.7\\nRMAC-V16-Siamese [36, 35]\\n[O]\\n512\\n80.1\\n85.0\\n42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\\n1.7\\n5.8\\n40.9 77.1 14.8 54.0\\nRMAC-R101-Siamese [35]\\n[O]\\n2048\\n\\n\\n49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\\n4.5\\n13.0 52.1 87.1 21.3 67.4\\nRMAC-R101-Triplet [16, 35]\\nNC-clean\\n2048\\n86.1\\n94.5\\n60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\\nGeM-R101-Siamese [37, 35]\\nSfM-120k\\n2048\\n87.8\\n92.7\\n64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\\nAGeM-R101-Siamese [17]\\nSfM-120k\\n2048\\n\\n\\n67.0\\n\\n\\n\\n78.1\\n\\n\\n\\n40.7\\n\\n\\n\\n57.3\\n\\n\\n\\nSOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\\n\\n\\n69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\\nDELG-GeM-R101-ArcFace [5]\\nGLDv1-noisy 2048\\n\\n\\n73.2\\n\\n54.8\\n\\n82.4\\n\\n61.8\\n\\n51.2\\n\\n30.3\\n\\n64.7\\n\\n35.5\\n\\nGeM-R101-ArcFace [53]\\nGLDv2-clean 2048\\n\\n\\n74.2\\n\\n\\n\\n84.9\\n\\n\\n\\n51.6\\n\\n\\n\\n70.3\\n\\n\\n\\nGLAM-GeM-R101-ArcFace baseline\\nGLDv2-clean\\n512\\n91.9\\n94.5\\n72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\\n+local\\nGLDv2-clean\\n512\\n91.2\\n95.4\\n73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\\n+global\\nGLDv2-clean\\n512\\n92.3\\n95.3\\n77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\\n+global+local\\nGLDv2-clean\\n512\\n94.2\\n95.6\\n78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\\nTable 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\\nVGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). : dimension d = 256 [2]. mP: mP@10. Red:\\nbest results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\\nmodel other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\\ntoo small. To achieve best possible performance, we use\\nGLDv2-clean as a training set in the remaining experiments.\\nComparisons on same training set\\nIt is common to com-\\npare methods regardless of training sets as more become\\navailable, e.g., [35, 28]. Since GLDv2-clean is relatively\\nnew, Weyand et al. [53], which introduced the dataset, is the\\nonly study that has trained the same backbone with the same\\nsettings (ResNet101-GeM with ArcFace) on GLDv2-clean.\\nOur baseline is lower than [53], because our dimensinality is\\n512, while other models based on ResNet101 use 2048. Yet,\\nTable 3 shows that our best model trained on GLDv2-clean\\noutperforms [53] by a large margin. But the most impor-\\ntant comparison is with SOLAR [28], also based on self-\\nattention, which has trained ResNet101-GeM on GLDv1-\\nnoisy. On this training set, our best model clearly outper-\\nforms [28] despite lower dimensionality.\\nComparison with state of the art\\nTable 4 shows the\\nperformance of four variants of our model, i.e. baseline\\nwith or without local/global attention, and compares them\\nagainst state-of-the-art (SOTA) methods based on global de-\\nscriptors without re-ranking on the complete set of bench-\\nmarks, including distractors. Both local and global atten-\\ntion bring signicant gain over the baseline. The effect\\nof global is stronger, while the gain of the two is addi-\\ntive in the combination. The best results are achieved by\\nthe global-local attention network (baseline+global+local).\\nWith this model, we outperform previous best methods\\non most benchmarks except mP@10 on RParis (medium)\\nand RParis+R1M (medium), where we are outperformed\\nby [37, 35]. These results demonstrate that our approach is\\neffective for landmark image retrieval. Figure 7 shows some\\nMETHOD\\nOXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nGLAM baseline\\n91.9\\n94.5\\n72.8\\n84.2\\n49.9\\n69.7\\n+local-channel\\n91.3\\n95.3\\n72.2\\n85.8\\n48.3\\n73.1\\n+local-spatial\\n91.0\\n95.1\\n72.1\\n85.3\\n48.3\\n71.9\\n+local\\n91.2\\n95.4\\n73.7\\n86.5\\n52.6\\n75.0\\n+global-channel\\n92.5\\n94.4\\n73.3\\n84.4\\n49.8\\n70.1\\n+global-spatial\\n92.4\\n95.1\\n73.2\\n86.3\\n50.0\\n72.7\\n+global\\n92.3\\n95.3\\n77.2\\n86.7\\n57.4\\n75.0\\n+global+local\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 5: mAP comparison of spatial and channel variants\\nof our local (+local, subsection 3.1) and global (+global,\\nsubsection 3.1) attention modules to the baseline.\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nCBAM style\\n93.8\\n95.7\\n75.6\\n88.4\\n53.3\\n76.8\\nGLAM (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 6: mAP comparison between CBAM style and our\\nlocal spatial attention.\\nexamples of our ranking results.\\n4.4. Ablation study\\nOur ablation study uses the Google Landmark v2 clean\\ndataset (GLDv2-clean) [53] for training, which is shown to\\nbe the most effective in Table 3.\\n7\\nMETHOD\\nOXF5K\\nPAR6K\\nRMEDIUM\\nRHARD\\nROxf\\nRPar\\nROxf\\nRPar\\nConcatenate\\n89.5\\n95.1\\n73.6\\n86.5\\n54.0\\n73.7\\nSum (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 7: mAP comparison between weighted concatenation\\nand weighted average for feature fusion.\\nMETHOD\\nOXF5K PAR6K\\nRMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nFixed-size\\n76.1\\n82.6\\n55.7\\n68.4\\n29.2\\n47.5\\nGroup-size (Ours)\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 8: mAP comparison between xed-size (224  224)\\nand group-size sampling methods.\\nQUERY DATABASE OXF5K PAR6K RMEDIUM\\nRHARD\\nROxf RPar ROxf RPar\\nSingle\\nSingle\\n93.3\\n95.2\\n76.9\\n87.1\\n58.6\\n74.7\\nMulti\\nSingle\\n93.9\\n95.4\\n78.0\\n87.7\\n59.0\\n75.5\\nSingle\\nMulti\\n93.6\\n95.6\\n77.0\\n87.8\\n57.1\\n76.0\\nMulti\\nMulti\\n94.2\\n95.6\\n78.6\\n88.5\\n60.2\\n76.8\\nTable 9: mAP comparison of using multiresolution repre-\\nsentation (Multi) or not (Single) on query or database.\\nEffect of attention modules\\nWe ablate the effect of our\\nlocal and global attention networks as well as their com-\\nbination. Table 5 shows the results, which are more ne-\\ngrained than those of Table 4. In particular, it shows the ef-\\nfect of the channel and spatial variants of both local and\\nglobal attention. We observe that, when used alone, the\\nchannel and spatial variants of local attention are harmful\\nin most cases. Even the combination, baseline+local, is not\\nalways effective. By contrast, when used alone, the channel\\nand spatial variants of global attention are mostly benecial,\\nespecially the latter. Their combination, baseline+global, is\\nimpressive, bringing gain of up to 7.5%. Importantly, the\\ncombination baseline+global+local improves further by up\\nto another 2.8%. This result shows the necessity of local\\nattention in the nal model.\\nCBAM vs. our local spatial attention\\nWe experiment\\nwith the local spatial attention of CBAM [54]. CBAM ap-\\nplies average and max-pooling to input features and con-\\ncatenates the two for spatial attention. We apply this vari-\\nant to our local spatial attention module for comparison.\\nFor the CBAM style module, we keep the overall design\\nof our module as shown in Figure 3, but apply average and\\nmax-pooling to each of the four convolutional layer outputs\\nbefore concatenation. Table 6 shows that the CBAM style\\nmodule is considerably worse than ours on all benchmarks\\nexcept Paris6k, where it is only slightly better.\\nConcatenation vs. sum for feature fusion\\nWe use a\\nsoftmax-based weighted average of local and global atten-\\ntion feature maps with the original feature map (7). Here,\\nwe compare this weighted average with weighted concate-\\nnation, where concatenation replaces the sum operation\\nin (7). As shown in Table 7, the weighted average outper-\\nforms the weighted concatenation.\\nFixed-size vs. group-size sampling\\nNumerous studies\\nhave proposed methods for constructing batches according\\nto image size for efcient training. For instance, Gordo et\\nal. [16], DELF [29], and Yokoo et al. [56] employed dif-\\nferent image sizes per batch for training instead of a single\\nxed size. We adopt the method of Yokoo et al., which con-\\nstructs a batch with images of similar aspect ratio, so that\\nthe images can be resized to a size with an aspect ratio that\\nis similar to their own. We call this method group-size sam-\\npling. Table 8 compares xed-size (224  224) with group-\\nsize sampling. We observe that maintaining aspect ratios by\\nusing dynamic input sizes is much more effective.\\nMulti-resolution\\nWe use the multi-resolution representa-\\ntion [16] for the nal feature of an image at inference time.\\nThis method: (1) resizes an image into multiple scales; (2)\\nextracts features from the resized images; and (3) averages\\nthe features to obtain the nal feature of the image. The\\nmethod is applied to both query and database images to en-\\nhance ranking results, especially for small target objects.\\nTable 9 compares the four cases of applying this method or\\nnot to query or database images.\\n5. Conclusion\\nWe have introduced a novel approach that extracts global\\nand local contextual information using attention mecha-\\nnisms for instance-level image retrieval. It is manifested as\\na network architecture consisting of global and local atten-\\ntion components, each operating on both spatial and chan-\\nnel dimensions. This constitutes a comprehensive study and\\nempirical evaluation of all four forms of attention that have\\npreviously been studied only in isolation. Our ndings indi-\\ncate that the gain (or loss) brought by one form of attention\\nalone strongly depends on the presence of the others, with\\nthe maximum gain appearing when all forms are present.\\nThe output is a modied feature tensor that can be used in\\nany way, for instance with local feature detection instead of\\nspatial pooling for image retrieval.\\nWith the advent of vision transformers [12, 58] and their\\nrecent application to image retrieval [13], attention is ex-\\npected to play a more and more signicant role in vi-\\nsion. According to our classication, transformers perform\\nglobal spatial attention alone. It is of great interest to in-\\nvestigate the role of the other forms of attention, where our\\n8\\napproach may yield a basic building block of such archi-\\ntectures. One may even envision an extension to language\\nmodels, where transformers originate from [50].\\nReferences\\n[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-\\njdla, and Josef Sivic.\\nNetVLAD: CNN architecture for\\nweakly supervised place recognition. In CVPR, 2016. 2\\n[2] Artem Babenko and Victor Lempitsky. Aggregating Local\\nDeep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\\n[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\\nVictor Lempitsky.\\nNeural Codes for Image Retrieval.\\nIn\\nECCV, 2014. 1, 2, 5\\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\\nand Quoc V. Le.\\nAttention augmented convolutional net-\\nworks. In ICCV, 2019. 2, 3\\n[5] Bingyi Cao, Andre Araujo, and Jack Sim. Unifying deep\\nlocal and global features for image search. In ECCV, 2020.\\n2, 3, 7\\n[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\\nGCNet: Non-Local Networks Meet Squeeze-Excitation Net-\\nworks and Beyond. In ICCV, 2019. 2\\n[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\\nHartwig Adam. Rethinking atrous convolution for seman-\\ntic image segmentation. arXiv preprint arXiv:1706.05587,\\n2017. 4\\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\\noffrey Hinton. A simple framework for contrastive learning\\nof visual representations. In ICML, 2020. 1\\n[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\\nYan, and Jiashi Feng. A2-nets: Double attention networks.\\nIn NeurIPS, 2018. 1, 2, 3, 4\\n[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\\nZafeiriou. ArcFace: Additive Angular Margin Loss for Deep\\nFace Recognition. In CVPR, 2019. 6\\n[11] Michael Donoser and Horst Bischof. Diffusion Processes for\\nRetrieval Revisited. In CVPR, 2013. 1, 2, 6\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale.\\narXiv preprint\\narXiv:2010.11929, 2020. 8\\n[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\\nHerve Jegou.\\nTraining vision transformers for image re-\\ntrieval. Technical report, 2021. 8\\n[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\\nsecond-order pooling convolutional networks.\\nIn CVPR,\\n2019. 2, 3, 4\\n[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. Deep image retrieval: Learning global representations\\nfor image search. In ECCV, 2016. 2\\n[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\\nlus. End-to-end learning of deep visual representations for\\nimage retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\\n[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\\nAttention-\\naware generalized mean pooling for image retrieval. arXiv\\npreprint arXiv:1811.00202, 2018. 2, 3, 7\\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDeep residual learning for image recognition.\\nIn CVPR,\\n2016. 6\\n[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\\nVedaldi. Gather-excite: Exploiting feature context in con-\\nvolutional neural networks. In NeurIPS, 2018. 2\\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\\nSqueeze-and-Excitation Networks. In CVPR, 2018. 1, 2\\n[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\\nand Ondrej Chum. Efcient diffusion on region manifolds:\\nRecovering small objects with compact cnn representations.\\nIn CVPR, 2017. 2, 6\\n[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\\nC. Schmid. Aggregating local image descriptors into com-\\npact codes. PAMI, (99):11, 2011. 2\\n[23] Albert Jimenez, Jose M. Alvarez, and Xavier Giro-i-Nieto.\\nClass weighted convolutional features for visual instance\\nsearch. In BMVC, 2017. 2, 3\\n[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\\nCrossdimensional weighting for aggregated deep convolu-\\ntional features. In ECCV, 2016. 1, 2, 3, 7\\n[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\\nLearned Contextual Feature Reweighting for Image Geo-\\nLocalization. In CVPR, 2017. 1, 2, 3\\n[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\\nProxy anchor loss for deep metric learning. In CVPR, 2020.\\n1\\n[27] David G. Lowe.\\nDistinctive image features from scale-\\ninvariant keypoints. In IJCV, 2004. 1, 2\\n[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\\nMikolajczyk. SOLAR: Second-Order Loss and Attention for\\nImage Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\\n[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\\nand Bohyung Han. Large Scale Image Retrieval with Atten-\\ntive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\\n[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\\nSavarese. Deep metric learning via lifted structured feature\\nembedding. In CVPR, 2016. 1\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\\nAndreas Kopf, Edward Yang, Zach DeVito, Martin Raison,\\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\\nFang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\\nperative style, high-performance deep learning. In NeurIPS,\\n2019. 5\\n[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Object retrieval with large vocabularies\\nand fast spatial matching. In CVPR, 2007. 2, 5\\n[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\\nAndrew Zisserman. Lost in quantization:Improving particu-\\nlar object retrieval in large scale image databases. In CVPR,\\n2008. 5\\n9\\n[34] Tobias Plotz and Stefan Roth. Neural nearest neighbors net-\\nworks. In NeurIPS, 2018. 2, 3\\n[35] Filip Radenovic, Ahmet Iscen, Giorgos Tolias, Yannis\\nAvrithis, and Ondrej Chum. Revisiting Oxford and Paris:\\nLarge-Scale Image Retrieval Benchmarking. In CVPR, 2018.\\n5, 6, 7\\n[36] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. CNN\\nimage retrieval learns from BoW: Unsupervised ne-tuning\\nwith hard examples. In ECCV, 2016. 2, 7\\n[37] Filip Radenovic, Giorgos Tolias, and Ondrej Chum. Fine-\\nTuning CNN Image Retrieval with No Human Annotation.\\nIn TPAMI, 2019. 1, 2, 5, 6, 7\\n[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\\nand Atsuto Maki. Visual Instance Retrieval with Deep Con-\\nvolutional Networks. In CoRR, 2015. 2\\n[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\\nLi Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\\nlenge. In International booktitle of Computer Vision, 2015.\\n5\\n[40] Oriane Simeoni, Yannis Avrithis, and Ondrej Chum. Local\\nfeatures and visual words emerge in activations. In CVPR,\\n2019. 2, 6\\n[41] O. Simeoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\\nGraph-based particular object discovery. Machine Vision and\\nApplications, 30(2):243254, 3 2019. 3\\n[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\\nical networks for few-shot learning. In NeurIPS, 2017. 1\\n[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\\nVanhoucke, and Andrew Rabinovich.\\nGoing deeper with\\nconvolutions. In CVPR, 2015. 3\\n[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfcientDet:\\nScalable and Efcient Object Detection. In CVPR, 2020. 5\\n[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\\nSim.\\nDetect-to-retrieve: Efcient regional aggregation for\\nimage search. In CVPR, 2019. 2\\n[46] Giorgios Tolias, Yannis Avrithis, and Herve Jegou. To aggre-\\ngate or not to aggregate: Selective match kernels for image\\nsearch. In ICCV, 2013. 2\\n[47] Giorgos Tolias, Tomas Jenicek, and Ondrej Chum. Learn-\\ning and aggregating deep local descriptors for instance-level\\nrecognition. In ECCV, 2020. 2, 3\\n[48] Giorgos Tolias, Ronan Sicre, and Herve Jegou. Particular ob-\\nject retrieval with integral max-pooling of CNN activations.\\nIn ICLR, 2016. 2\\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 4\\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 9\\n[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\\nmeng Zuo, and Qinghua Hu.\\nECA-Net: Efcient Chan-\\nnel Attention for Deep Convolutional Neural Networks. In\\nCVPR, 2020. 2, 3, 4\\n[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\\ning He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\\n3, 4\\n[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\\nGoogle Landmarks Dataset v2 - A Large-Scale Benchmark\\nfor Instance-Level Recognition and Retrieval.\\nIn CVPR,\\n2020. 1, 2, 5, 6, 7\\n[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\\nKweon. CBAM: Convolutional Block Attention Module. In\\nECCV, 2018. 1, 2, 4, 8\\n[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\\nShinichi Satoh. Efcient image retrieval via decoupling dif-\\nfusion into online and ofine processing. In AAAI, 2019. 2,\\n6\\n[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\\nIizuka. Two-stage Discriminative Re-ranking for Large-scale\\nLandmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\\n[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\\nresidual networks. In CVPR, 2017. 4\\n[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\\nto-token vit: Training vision transformers from scratch on\\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 8\\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\\nself-attention for image recognition. In CVPR, 2020. 2, 3\\n10\\n'),\n",
       "   Document(metadata={'Published': '2023-06-02', 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics', 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li', 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\"}, page_content='RITA: Group Attention is All You Need for Timeseries Analytics\\nJiaming Liang\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nliangjm@seas.upenn.edu\\nLei Cao\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nlcao@csail.mit.edu\\nSamuel Madden\\nMassachusetts Institute of Technology\\nCambridge, MA, USA\\nmadden@csail.mit.edu\\nZachary Ives\\nUniversity of Pennsylvania\\nPhiladelphia, PA, USA\\nzives@cis.upenn.edu\\nGuoliang Li\\nTsinghua University\\nBeijing, China\\nliguoliang@tsinghua.edu.cn\\nABSTRACT\\nTimeseries analytics is of great importance in many real-world\\napplications. Recently, the Transformer model, popular in natu-\\nral language processing, has been leveraged to learn high quality\\nfeature embeddings from timeseries, core to the performance of\\nvarious timeseries analytics tasks. However, the quadratic time and\\nspace complexities limit Transformers scalability, especially for\\nlong timeseries. To address these issues, we develop a timeseries an-\\nalytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dy-\\nnamically clusters the objects based on their similarity into a small\\nnumber of groups and approximately computes the attention at\\nthe coarse group granularity. It thus significantly reduces the time\\nand space complexity, yet provides a theoretical guarantee on the\\nquality of the computed attention. The dynamic scheduler of RITA\\ncontinuously adapts the number of groups and the batch size in the\\ntraining process, ensuring group attention always uses the fewest\\ngroups needed to meet the approximation quality requirement. Ex-\\ntensive experiments on various timeseries datasets and analytics\\ntasks demonstrate that RITA outperforms the state-of-the-art in\\naccuracy and is significantly faster  with speedups of up to 63X.\\n1\\nINTRODUCTION\\nMotivation. Many data driven applications involve processing\\nmassive timeseries data, including IoT [11], medical AI [14], stock\\nmarket [27], and so on. As such, there is a great need for timeseries\\nanalytics, such as forecasting [8], classification [20], clustering [31],\\nsimilarity search [39], and anomaly detection [50], with applications\\nranging from automatically diagnosing diseases [5], recognizing\\nhuman activities [29], to stopping financial fraud [59].\\nEffective feature extraction [40] lies at the core of almost all\\nthese timeseries analytics tasks. Recently researchers [61] have\\nstarted leveraging the self-supervised pre-training methodology of\\nTransformers [4, 16, 52], which have proven remarkably successful\\nin natural language processing (NLP), to automatically learn high\\nquality feature embeddings from timeseries. In NLP, self-supervised\\npre-training exploits the sequential patterns (correlations) among\\nthe words in sentences to produce contextualized feature embed-\\ndings. Timeseries bear similarity to natural language, because in\\ntimeseries data the sequential order among the values (stock price,\\nvolume, etc.) over time matters. That is, each value is highly cor-\\nrelated with other values observed before or after it. Therefore,\\nCorresponding Author\\npre-training a Transformer model which takes the correlations\\namong different observations into account is a natural idea to learn\\nfeature embeddings from timeseries. Indeed, the experiments in [61]\\nconfirm that Transformer-based methods outperform traditional\\ntimeseries analytics techniques.\\nHowever, existing work [61] that directly applies Transformers\\nto learn features from timeseries data have been shown not to be\\nscalable to long timeseries [30]. The idea of self-attention [52] is\\ncentral to pre-training methods in NLP: It computes pairwise cor-\\nrelations among different semantic units in a sequence (in NLP, a\\nsentence); as such, it has quadratic time and space complexity in\\nthe length of the input sequence. Such an approach places limits on\\nthe models scalability, especially when handling large sequences,\\nwhich are common in real-world timeseries applications such as\\nIoT, medical AI, and finance [6, 34, 62]. Predictions about timeseries\\nmay need to look at months or years of historical data to make ac-\\ncurate predictions, spanning hundreds of thousands of samples. As\\nan example, in collaboration with a research hospital we have been\\ndeveloping a seizure classifier that automatically detects seizures\\nbased on EEG signals (timeseries) collected during the clinical ob-\\nservation of patients. As seizures last only a few seconds, we chunk\\nlong EEG data into many 2 second segments and detect seizures at\\na segment level. However, the classification of a particular segment\\ndepends on up to 12 hours of prior signal to determine if one 2\\nsecond segment indicates seizure or not, because seizure diagnosis\\nneeds to consider long-term trends in the EEG data [6]. The number\\nof segments in 12 hours is more than 21k. This is far larger than\\nthe number of semantic units the typical NLP tasks expect. For\\nexample, BERT [16] limits the number of units to 512 and even\\nmassive models like GPT-3 [4] limit the number of units to 2048.\\nAlthough in NLP some lower-complexity methods have been\\nproposed to approximately compute self-attention [10, 26, 54], their\\nperformance degrades dramatically when used on timeseries, due\\nto the gap between natural language and timeseries, as we will\\nshow in our experiments.\\nProposed Approach. To tackle the aforementioned problem, we\\ndevelop RITA, a Transformer-based timeseries analytics tool, which\\nuses a novel attention mechanism, called group attention, to scale\\nto long timeseries.\\nLeveraging the periodicity of timeseries, RITA chunks the input\\ntimeseries into segments and dynamically clusters the segments\\ninto a small number (denoted as ) of groups. Segments in the\\nsame group possess similar feature embeddings during the current\\ntraining iteration, thus enabling them to approximately share the\\n1\\narXiv:2306.01926v1  [cs.LG]  2 Jun 2023\\ncomputation of attention. As the timeseries increases in length,\\nmore sharing opportunities become available. RITA then computes\\nthe self-attention at a group level and produces a compressed group\\nattention matrix. In this way, group attention eliminates both com-\\nputation and memory bottlenecks in Transformer-style models and\\nthus more scalable to long timeseries.\\nHowever, making this idea effective and efficient in Transformer\\narchitectures is challenging for several reasons:\\n Efficiently Producing High Quality Feature Embeddings.\\nAlthough RITA computes the attention matrix at a group level, to\\npreserve the quality of the feature embeddings, it still has to pro-\\nduce different embeddings for different segments. This is because\\neven if some segments share the attention score temporally, it does\\nnot mean they should have the same feature embedding. However,\\nusing the group attention matrix, the existing self-attention mech-\\nanism will only produce a single feature vector for each group. A\\nnaive solution would be to restore the original attention matrix\\nfrom the group attention matrix. However, in this case we again\\nget an attention matrix with quadratic space complexity. Because\\nGPUs have limited memory, GPU memory will remain a bottleneck\\nin group attention.\\n The Number of Groups N. In RITA, the number of groups\\nis a crucial factor that balances the speed up and the quality of\\nattention approximation. A small will lead to a large speedup,\\nbut the approximation errors can also be significant. On the other\\nhand, although a large tends to produce high-quality approxima-\\ntions, it inevitably slows down the training process. Therefore, an\\nappropriate is essential to the performance of group attention.\\nHowever, depends on the distributional properties of the dataset.\\nFurthermore, like the classical transformer models, RITA stacks\\nmultiple attention layers to produce better embeddings. Ideally,\\ndifferent layers should also use different values of . In addition,\\nduring the model training phrase, group attention should use dif-\\nferent values of at different iterations to adapt to the varying\\nfeature embeddings. This makes manually setting appropriate \\nalmost impossible.\\n Batch Size. Moreover, as we want to dynamically adjust \\nduring training, a fixed batch size is sub-optimal: as decreases,\\nthe memory usage of a single sample decreases. This allows a larger\\nbatch size which is beneficial, because: (1) it makes full use of GPU\\nmemory; (2) high-parallelism across the samples in a big batch\\nbrings better performance. Our experimental study shows that\\ndoubling the batch size reduces the training time by 30%, while still\\npreserving the quality of the model. Thus, RITA should dynamically\\nadjust batch size as changes.\\nTo address the above problems, we first propose an embedding\\naggregation strategy and a customized group softmax function to\\nreplace the classical softmax function [52]. Together they ensure\\nRITA is able to directly use the compressed attention matrix to\\nproduce different feature embeddings for different segments. We\\ntheoretically show the embeddings RITA produces in this way are\\nidentical to those produced by first re-storing the original large\\nattention matrix. Thus RITA is able to produce high quality embed-\\ndings without introducing extra overhead. Further, we design a GPU\\nfriendly algorithm to group the segments in parallel, effectively\\nminimizing the grouping cost.\\nP0\\nPosition\\nEmbedding\\nW1\\n+\\n+\\n+\\nWindow \\nEmbedding\\n+\\nE0\\nRaw\\nTimeseries\\nTime-aware \\nConvolution\\nW[CLS]\\nW2\\n\\n.....\\nWn\\nP1\\nP2\\n.....\\nPn\\n.....\\nE1\\nE2\\nEn\\n.....\\nO0\\nO1\\nO2\\nOn\\n.....\\nRITA Encoder\\nScale & Input\\nFigure 1: RITA Architecture\\nSecond, we design an adaptive scheduler which dynamically de-\\ncides an appropriate for each group attention layer during the\\ntraining process. It starts with a large and iteratively merges\\ngroups that are similar to each other. Guided by an error bound on\\nthe approximated self-attention that users can tolerate, it automati-\\ncally determines if two groups are mergeable, performing merging\\nefficiently in a GPU-friendly way.\\nMoreover, we propose a learning-based method to model the\\ncorrelation between the number of groups and the batch size .\\nThis model is used to predict for a given when training RITA.\\nSpecifically, we first sample some values in a reasonable range.\\nFor each sampled , we find a batch size that consumes up to a\\ncertain percentage of GPU memory in a cost-efficient way. Using a\\nsmall set of mathematical functions as a prior, RITA learns a model\\nwith only a few <N, B> pairs as ground truth labels.\\nOur experiments on public timeseries benchmarks and the MGH\\nEEG data [6] confirm that RITA outperforms state-of-the-art meth-\\nods in accuracy on various timeseries analytics tasks, while our\\ngroup attention mechanism achieves a 63X speedup with much\\nless memory required, compared to existing self-attention mecha-\\nnisms [10, 52, 54].\\nContributions. The key contributions of this work include:\\n Our group attention mechanism leverages the periodicity of\\ntimeseries, reducing the time and space complexity of the self-\\nattention mechanism with accuracy guarantees, allowing RITA to\\nscale to long timeseries data.\\n Guided by an approximation error bound, our adaptive sched-\\nuler dynamically adapts the number of groups and the batch size\\nto the distribution properties of the evolving feature embeddings,\\nmaking group attention efficient and easily tunable.\\n We conduct experiments on various datasets and different ana-\\nlytics tasks, demonstrating that RITA is 4 to 63 times faster than\\nthe state-of-the-art while achieving better accuracy when handling\\nlong timeseries (length 2000).\\n2\\n2\\nBACKGROUND\\nWe provide some background on the canonical self-attention mod-\\nule in the Transformer[52]. A self-attention module takes hidden\\nembedding vectors Ras input, then projects them to\\nqueries (), keys () and values () and performs Scaled-dot Prod-\\nuct Attention, which given input hidden state , is computed by:\\n= , = ,= \\n= = ( \\n\\n\\n)\\n(1)\\nWhere R,R,Rare projection\\nmatrices for generating , ,. Ris also regarded as the\\npacking of query vectors {1, ...,} with dimension into a\\nmatrix. R,Rare regarded as the packing of key\\nvectors {1, ...,} and value vectors {1, ..., } in the same way.\\nGiven a matrix R, the softmax function normalizes \\nto ensure the sum of each row equals to 1, as shown below.\\n(,) =\\n(,)\\n1\\n=0 (,)\\n(2)\\nNote the attention matrix A is an matrix, where represents\\nthe number of elements in the input sequence (e.g. words in NLP).\\n3\\nRITA OVERVIEW\\nGiven a collection of unlabeled timeseries, RITA first pre-trains\\na Transformer-style model to produce high quality feature em-\\nbeddings for timeseries data. This pre-trained model is then used\\nto support various downstream tasks, similar to BERT [16]. Next,\\nwe overview the model architecture of RITA. We show how RITA\\nsupports various downstream tasks in Appendix A.7.\\nAs shown in Fig. 1, RITA is consist of two components: (1) Time-\\naware Convolution Layer (2) RITA Encoder.\\nTime-aware Convolution Layer fills the gap between timeseries\\nand natural language. Despite their high-level similarity, there is a\\nbig gap between timeseries and natural language. First, in natural\\nlanguage each word, as a discrete semantic unit, has an indepen-\\ndent meaning, while each element in a timeseries is a continuous,\\nnumerical value and does not necessarily constitute an independent\\nevent. Furthermore, the input sequences are single-channeled in\\nNLP, but often multi-channeled in timeseries (i.e., sensor data often\\nconsists of several related channels).\\nRITA leverages the classical convolution [28] strategy to solve\\nthis problem. Convolution is widely used to capture the local struc-\\ntures of an image. We use convolution to chunk one input timeseries\\ninto a sequence of windows and learn the local structure of each\\nwindow, similar to the discrete semantic units in natural language.\\nIt also discovers the correlations across different channels, thus\\nnaturally solving the multi-channel problem.\\nMore specifically, treating a multi-variate timeseries of length \\nand withvariables as an n  m matrix, RITA usesconvolution\\nkernels to chunkinto n windows and produce one d-dimensional\\nembedding per window using the convolution operation [28]. Each\\nconvolution kernel corresponds to a w  m matrix, where defines\\nthe number of timestamps that each convolution kernel covers,\\nidentical to the window size in sliding window.\\nRITA Encoder functions as Transformer Encoder as described in\\nthe original Transformer work[52]. It takes the embeddings of \\nsemantic units 1,2, ...,() as input (e.g. embeddings of\\nwindows for a timeseries), then models the correlations between\\nthe semantic units and outputs 1, ...,() as the context-\\naware embedding of each unit.\\nWhat makes RITA Encoder different from Transformer Encoder\\nis that: at the core of Transformer Encoder lies self-attention mech-\\nanism which incurs a (2) time complexity and memory usage.\\nThis quadratic cost becomes prohibitive for long timeseries and\\nlimits the scalablity of Transformer-based models. To make the\\nattention computation efficient yet high-quality, we replace the\\ncanonical self-attention with our proposed group attention.\\nSelf-supervised Pretraining. Inspired by the cloze text pre-\\ntraining task in NLP, we designed a mask-and-predict task as the\\npretraining task for our model. The timeseries is randomly masked\\nand the model should recover the masked values based on corre-\\nsponding contextual information.\\nTo be specific, we generate masks on time-stamps, with a mask\\nrate . The timeseries is scaled to be non-negative and the values\\nacross all the channels on the masked timestamps are set to be -1,\\nan impossible value on normal timestamps. Then the masked time-\\nseries is fed into RITA and the output representation is translated\\nto the recovered timeseries by a Transpose Convolution layer.\\n4\\nGROUP ATTENTION MECHANISM\\nGroup attention, a novel and efficient approximate attention mecha-\\nnism, addresses the performance bottleneck of self-attention in the\\nvanilla Transformer. In this section, we first introduce the frame-\\nwork of group attention and then theoretically establish the bound\\nof its approximation error.\\n4.1\\nThe Idea of Group Attention\\nAs periodicity is a natural property of timeseries [56], similar\\nwindows frequently occur. Similar windows result in similar\\nqueries/keys for attention computation, bringing opportunities for\\nsaving computation.\\nAs discussed in Sec. 2, , the attention score of window onto\\nwindow , is determined by the inner product between the query\\nvector of window and the key vector of window , that is,  .\\nGiven another window , if window has the similar key vector\\nto window , that is, , then   . In other words,\\nwhen .\\nThis observation inspires our group attention mechanism. That\\nis, we group the windows by their similarity in keys. Assuming\\nall windows in the same group have the same attention score onto\\nanother window , we then only compute the attention once by\\nusing one single key to represent this group, for example the centroid\\nof the group of keys. This thus saves significant computation cost.\\nBetter yet, after grouping windows into groups, group atten-\\ntion compresses the attention matrix from anmatrix to an\\nmatrix. Because (number of groups) tends to be much smaller\\nthan (number of windows) due to the periodicity of timeseries,\\ngroup attention consumes much less memory than the original\\nself-attention mechanism, successfully eliminating the memory\\nbottleneck. Note that it also doesnt hurt quality all that much, as\\nconfirmed in our experiments (Sec. 6.2).\\n3\\nGrouping\\nAverage\\nK\\nQ\\nMatMul\\nAttention Matrix\\nWeighted\\nSoftMax\\nV\\nSum\\nAggregate\\nTranspose\\nMatMul\\nOutput\\nQ \\nK \\nV\\nFigure 2: Group Attention\\n4.2\\nComputing the Output Feature Embedding\\nWe now discuss how to efficiently compute the output feature\\nembeddings using the small compressed group attention matrix.\\n4.2.1\\nProblem: Producing Embeddings w/ Group Attention Matrix\\nAs described in the Background, once we have acquired the at-\\ntention matrix , canonical self-attention computes the output\\nembedding as O = AV. Because is an  matrix and is an\\nmatrix, the matrix product operation still produces an \\nmatrix . That is, it produces a dimensional feature vector for\\neach window. However, our group attention will produce an  \\nattention matrix e\\n, where corresponds to the number of groups.\\nIn this case the matrix product will produce a matrix e\\n. That\\nis, it produces a feature vector for each group. However, our goal\\nis to produce different embeddings for different windows, because\\neven if some windows share the attention score temporally, it does\\nnot mean they should have the same feature embedding.\\nA Naive Solution. A naive solution would be to restore the full\\nattention matrix from the group attention matrix e\\n. For example,\\ngiven one group composed of and , we map its group\\nattention vector in e\\ninto two rows that correspond to and\\nin . However, in this case we again get a  attention\\nmatrix; and GPU memory remains a bottleneck in group attention.\\n4.2.2\\nSolution: Embedding Aggregation and Group SoftMax\\nUsing an embedding aggregation operation and a group softmax\\nfunction, RITA produces embeddings without restoring the full\\nattention matrix. Fig. 2 shows the workflow of group attention.\\nEmbedding Aggregation. The idea is inspired by the observation\\non the matrix product operation O = AV conducted on the fully\\nrestored attention matrix .\\nGiven an element,ofcorresponding to the dimension of\\ns feature vector,,= , where vector ai Rn denotes the\\nrow of the attention matrix and vector vj Rn denotes the \\ndimension of all the feature vectors. Given ai =< a1\\ni , a2\\ni ,    , an\\ni >\\nand vj =< v1\\nj , v2\\nj ,    , vn\\nj >, ,= n\\nk=1 ak\\ni vk\\nj .\\nAs an example, assume 1 and 2 belong to the same group\\n1. Then 1\\n= 2\\n= e1\\n, where e1\\ne\\ncorresponds to the attention\\nof group 1 onto . Therefore, 1\\n1\\n+ 2\\n2\\n= e1\\n(1\\n+ 2\\n).\\nAs an immediate generalization of the above analysis, if we ag-\\ngregate up the windows that belong to the same group and convert\\nthe n-dimensional feature vector into a -dimensional group fea-\\nture vectorebeforehand, we could directly use the group attention\\nvector eand the group feature vector eto compute ,.\\nUsing embedding aggregation, RITA is able to produce the fea-\\nture embedding e\\nthat is identical to the embedding produced\\nby using the full attention matrix and the embedding matrix .\\nGroup Softmax Function. In canonical self-attention the atten-\\ntion matrix is computed as = SoftMax( QKT\\n\\ndk ). To compute ,\\nwe have to first compute (denoted as ) which is an  \\nmatrix. Then normalizing the matrix with softmax produces the\\nattention matrix .\\nGroup attention follows the same procedure. But after grouping\\nkeys into e, eproduces an  matrix e. Due to the non-\\nlinearity of the softmax function, applying softmax directly on e\\nwill result in a group attention matrix e\\nfrom which we are not able\\nto recover a full attention matrix that is identical to first restoring\\neto and then applying softmax on . The matrix produced\\nby the latter is desirable, as we want to approximate the original\\nattention matrix as accurately as possible. However, restoring the\\nsmall  ematrix is not memory efficient, as it will end up with\\na full  matrix .\\nTo solve the above problems, we introduce a new group softmax\\nfunction to replace the original softmax function (Eq. 2).\\n(g\\n,) =\\n(,)\\n1\\n=0 (,)\\n(3)\\nIn Eq. 3, represents the number of windows that Group\\ncontains. Compared to the original softmax, our group softmax\\nconsiders each group as elements and counts it \\ntimes when summing up the exponential of each groups ,. In\\nthis way, the group softmax function operating on the small e\\nmatrix will produce exactly the same result to the softmax function\\noperating on the full matrix.\\nTheoretical Guarantee. In Appendix A.4, we prove that the group\\nsoftmax function and the embedding aggregation operation produce\\nthe same output feature embedding with the naive method that has\\nto first restore the big full attention matrix.\\nWe show an efficient implementation of the embedding aggrega-\\ntion operation and group softmax function in Appendix A.2, Alg. 1.\\nTime Complexity. The time complexity of Alg. 1 is () and\\nthe space complexity is(), while the time and space complexity\\nof the original self-attention mechanism are (2) and (2).\\n4.3\\nError Bound\\nGroup attention produces a group attention matrix e\\nwhich approxi-\\nmates the attention matrixproduced by the classical self-attention\\nwith a bounded error, as shown in Lemma 1.\\nLemma 1. Let be the radius of the ball where all key vectors\\nlive; ebe the representative of the group that contains key . Let \\ndenote the full attention matrix restored from e\\n. Suppose the distance\\nbetween eand (||ekk||) satisfies: ||ekk|| d.\\nThen > 1, if d ln()\\n2R , 1\\nAi,j\\nAi,j \\nLemma 1 shows that the error bound of the group attention is\\ndetermined by the distance . As discussed in Sec. 5.1, it inspires\\nus to design a strategy to dynamically determine the number of\\ngroups  the most critical parameter of group attention. Please\\nrefer to Appendix A.5 for the proof.\\n4\\n4.4\\nGPU Friendly Grouping Method\\nIn this section, we discuss the implementation of a grouping method.\\nTo make group attention efficient and effective, the grouping\\nmethod has to satisfy the following requirements:\\n(1) Tight distance bound: to ensure the approximation quality,\\nthe distance between each key and its group representative should\\nbe minimized according to Lemma 1.\\n(2) Lightweight: to ensure the performance gain, the grouping\\nmethod must be lightweight, at worst not exceeding the complexity\\nof group attention itself (()).\\n(3) GPU friendly: to take advantage of GPUs, we prefer a group-\\ning method that mainly consists of matrix operations, which can\\nbe efficiently executed on a GPU.\\nTo satisfy the above requirements, after thorough investigation\\non various clustering algorithms, we design a GPU friendly K-\\nmeans [35] as the grouping method.\\nFirst, K-means minimizes the overall distance between any object\\nand its cluster center, hence naturally satisfying Requirement 1.\\nSecond, given centers, in each iteration the time and space\\ncomplexity of K-means is (). Usually, the iteration goes until\\nconvergence. However, we observe that rather than seeking a per-\\nfect K-means clustering, training a few iterations is sufficient to\\nget a good grouping for group attention, because typically the later\\niterations only slightly update the clustering and group attention\\nis robust to such imperfection.\\nThird, we design a GPU-friendly implementation of K-means.\\nThe performance bottleneck of K-means comes from the dis-\\ntance computation between each vector and its center, that is,\\n|vi cj| =\\n\\n(vi cj)2, i [1, n], j [1, N]. The performance bot-\\ntleneck is . We instead use a different formulation: |\\n| = |vi cj| =\\n\\n|vi|2 + |cj|2 2vi  cj, i [1, n], j [1, N]. This is\\nbecause in this formulation, the performance bottleneck is  ,\\nwhich could be implemented as a matrix product operation. Al-\\nthough the complexity of the two formulations is the same, in GPUs\\nmatrix product is much more efficient than pairwise difference.\\n5\\nADAPTIVE SCHEDULER\\nNext, we present the adaptive scheduler of RITA which addresses\\nthe challenges of determining an appropriate number of groups\\nand accordingly the batch size , as described in Introduction.\\nUsing a dynamic scheduling method we propose, the scheduler\\nautomatically determines and adjusts and based on the distri-\\nbutional properties of the feature embeddings produced over the\\niterative training process, while guaranteed to produce high quality\\nattention approximation that meets the requirement of users.\\nIn Sec. 5.1 we show how RITA automatically determines . Then\\nwe introduce in Sec. 5.2 the learning-based method which given an\\n, immediately predicts a good batch size.\\n5.1\\nDynamically Determining the Number of\\nGroups N\\nWithout loss of generality, we use one group attention module as\\nan example to show how RITA automatically gets an appropriate .\\nThe adaptive scheduler of RITA starts with a large and decreases\\nit dynamically. This is because in the training process of RITA, the\\nfeature embeddings produced epoch by epoch tend to get stabler\\nand stabler and gradually converge, thus no need to increase .\\nRITA reduces the number of groups by merging similar groups.\\nIntuitively, given two groups, we could measure their similarity\\nbased on the distance of their centers. If the distance between\\ntheir centers is smaller than a distance threshold, then the two\\ngroups could be merged. However, setting an appropriate distance\\nthreshold seems hard  as difficult as setting an appropriate .\\nTo solve this problem, RITA leverages the error bound of group\\nattention introduced in Sec. 4.3. It only requires users to set an\\nerror bound , and then uses Lemma 1 to translate to a distance\\nthreshold . RITA then uses Lemma 2 to determine if merging some\\ngiven clusters still meets the error bound threshold .\\nLemma 2. Denote to be the cluster center of . Assume\\nthe existing grouping satisfies k,\\nmax\\nxclusterk\\n|ck x| d , thus satis-\\nfying an error bound by Lemma 1. If there exist clusters, namely,\\n1,2, ...,, satisfying that:\\n\\n\\n|| + || ,, [1,]\\n(4)\\nmerging them into one cluster still meets the error bound .\\nPlease refer to Appendix A.6 for the proof.\\nFinding the Mergable Clusters. We formulate the problem of\\nfinding mergeable clusters using graph theory:\\n(1) each cluster is a node in the graph;\\n(2) if and satisfy:\\n\\n\\n||+|| , and\\n\\n\\n||+|| \\nthere is an undirected edge between and ;\\nIn this scenario, finding the maximum number of mergeable\\nclusters is equivalent to finding the minimal clique cover in the\\ncorresponding graph, which is an NP-hard problem [24]. Such\\nheavy computation overhead is not acceptable for RITA. We thus\\noffer a simplified solution:\\n(1) Halve the clusters into two sets 1,2;\\n(2) If 1 and 2 satisfy:\\n\\n\\n|| + || ,\\n\\n\\n|| + || \\n2\\n(5)\\nis marked.\\n(3) Decrease the number of clusters by counting the masks in 2.\\nIn this solution, clusters in 1 can be regarded as transfer nodes.\\nIf (5) holds for (1,1 2) and (\\n1,2 2), respectively, we have,\\n\\n1\\n|1 2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 |\\n\\n\\n1\\n|1 | + |2 | + |1 | + |2 | \\n(6)\\nThus (4) holds when merging several clusters in 2 with one\\ncluster in 1. As a result, we can greedily merge clusters in 2, as\\nillustrated in step(3).\\nAssume the number of clusters decreases by after merging,\\nwe apply a momentum update [42] on the number of clusters , as\\nis commonly used in machine learning to smooth the changing of\\nand avoid sample selection bias. To be specific: = (\\n) + (1 ), where is a hyper-parameter for momentum.\\n5\\n5.2\\nDynamically Determining the Batch Size\\nBecause of the dynamic grouping operation, the computational\\ngraph in deep learning training [1] varies from sample to sample. As\\na result, it is impossible to precisely compute a batchs GPU memory\\nusage without indeed feeding it into the model. To overcome this\\nproblem, RITA learns a batch size prediction function offline; then\\nat the RITA training time, given a number of groups , RITA uses\\nthis function to predict a proper batch size.\\nWhen the model architecture and hardware are fixed, the batch\\nsize depends on the length of the timeseries and the average\\ngroup number among all attention module . So RITA samples\\nseveral (, ) pairs and estimate a proper batch size for each pair.\\nMore specifically, given a user-defined timeseries maximal length\\n, we randomly sample integral points (, ) from plane\\n{1 , 1 }. Then we use a binary search based\\nalgorithm to find the maximal batch size that consumes less than\\n90% available GPU memory, aiming to avoid wasting GPU memory\\nand the risks of out of memory (OOM).\\nTreating these pairs as ground truth labels, we use function\\nfitting [18] to learn the batch size predicting function B = f (L, N),\\nwhere B is a function of two variables and .\\nLearning the Prediction Function. We apply curve fit from\\nSciPy [53] as the function fitting tool to fit the two-variable function\\n= (, ) on plane {1 , 1 }.\\nWe observe that applying one function to the whole plane incurs\\na huge estimation error. So we develop a dynamic-programming\\n(DP) method to divide the plane into several sub-planes and apply\\na distinct function to each sub-plane respectively. It is optimal in\\nminimizing the total estimation error on all sub-planes\\nWith the learned prediction function , we can estimate a proper\\nbatch size for any (, ) during training, even if it is not seen in\\nthe sampled (, ) pairs.\\nThe Algorithms and Optimality Proof. Please refer to Appen-\\ndix A.3 for the pseudo code of the binary search-based algorithm\\nand the description of the DP method for plane-division and the\\nproof for its optimality.\\n6\\nEVALUATION\\nOur experimental study focuses on the following questions:\\n1. Effectiveness and efficiency of RITA: How does RITA com-\\npare with other Transformer-based methods and traditional time-\\nseries representation learning methods in accuracy and efficiency?\\n2. Ablation Study: How do the key techniques of RITA work?\\n6.1\\nExperimental Setup\\nDatasets. We evaluate RITA on classification and imputation tasks\\nusing 5 multi-variate and 3 uni-variate timeseries datasets.\\n WISDM [55] is a popular multivariate timeseries dataset gen-\\nerated from the accelerometer in the mobile phone. The subjects\\nperformed 18 daily activities (e.g. walking, jogging). The dataset\\nwas collected from 51 subjects and the sampling rate is 20 Hz.\\n HHAR dataset [46] contains sensing data of accelerometer col-\\nlected from 9 users performing 5 activities with 12 different smart-\\nphones (varying in sampling rate). This increases the complexity\\nof the task and thus can test the models robustness.\\n RWHAR RealWorld HAR dataset [48] covers 15 subjects per-\\nforming 8 locomotion-style activities. Each subject wears the sen-\\nsors for approximately ten minutes. The sampling rate is 50 Hz.\\n ECG dataset [34] consists of 10,000 EEG recordings for arrhyth-\\nmia classification. Each recording has an uncertain length ranging\\nfrom 6 to 60 seconds sampled at 500 Hz. The ECG recordings corre-\\nspond to 9 types of heart problems such as atrial fibrillation (AF)\\nand premature atrial contraction (PAC), etc.\\n MGH [6] is a EEG dataset collected by Mass. General Hospital.\\nEach timeseries corresponds to the EEG data observed from one\\npatient during their stay in ICU for a couple of days. The EEG\\nmonitoring produced data with 20 channels. The sampling rate is\\n200 HZ. So it produces very long timeseries.\\n WISDM*/HHAR*/RWHAR* are three uni-variate datasets de-\\nrived by picking one channel from WISDM/HHAR/RWHAR.\\nTraining/Validation Data Generation. We apply a sliding win-\\ndow on the raw timeseries to get training/validation samples. The\\nsize of the sliding window is set as 200 on small datasets (WISDM,\\nHHAR, RWHAR), 2000 on medium size dataset (ECG), and 10,000\\non the large dataset (MGH). Table 1 shows the statics of the gen-\\nerated datasets. They are randomly split into training/validation\\nset in a proportion of 0.9/0.1. In pretraining + few-label finetun-\\ning scenario, we use 100 labeled data per class for finetuning. We\\nguarantee that training set does not overlap with the validation set.\\nDataset\\nTrain. Size\\nValid. Size\\nLength\\nChannel\\nClasses\\nWISDM\\n28,280\\n3,112\\n200\\n3\\n18\\nHHAR\\n20,484\\n2,296\\n200\\n3\\n5\\nRWHAR\\n27,253\\n3,059\\n200\\n3\\n8\\nECG\\n31,091\\n3,551\\n2000\\n12\\n9\\nMGH\\n8,550\\n950\\n10000\\n21\\nN/A\\nTable 1: The statistics of the datasets\\nAlternative Methods. We compare RITA against the SOTA Trans-\\nformer based timeseries representation learning method TST [61].\\nTo evaluate our group attention (referred to as Group Attn.), we\\ndevelop three baselines by replacing the group attention compo-\\nnent in RITA with the classic vanilla Self-Attention [52](referred\\nto as Vanilla) and two SOTA methods that reduce the complexity\\nof self-attention by approximation in NLP, namely, Performer [10]\\n(referred to as Performer) and Linformer [54] (referred to as Lin-\\nformer). Similar to our proposed Group Attn., Vanilla, Performer,\\nLinformer all use RITAs time-aware convolution operation (Sec. 3)\\nto turn timeseries segments into input feature vectors.\\nWe also compare Group Attn. against GRAIL [40], which is\\nthe SOTA of the non-deep learning methods for timeseries repre-\\nsentation learning. GRAIL supports classification tasks by feeding\\nthe learned representations into a Support-Vector Machine [12]\\nor K-Nearest Neighbor [17] classifier. Note GRAIL only targets\\nuni-variate timeseries and cannot support imputation tasks.\\nMethodology. We mainly focus on two downstream tasks:\\n(1) Classification. First, we train Group Attn. and the base-\\nlines with full labels from scratch to test the effectiveness of RITA\\nframework and the approximation quality of our group attention.\\nSecond, to measure the effectiveness of self-supervised pretrain-\\ning, we evaluate the accuracy of training on few labeled timeseries\\nwith/without pretraining on large scales of unlabeled timeseries. To\\nbe specific, we split the training set into a pretraining set and a fine-\\ntuning set, with very few data in the latter (100 labeled samples per\\n6\\n(a) Effectiveness \\n(b) Efficiency\\nTraining Time/sec\\nFigure 3: Full-label classification results (multi-variate data).\\nclass in our experiment). We train the model on the cloze pretrain-\\ning task with a mask rate = 0.2. Then we train two classification\\nmodels using the finetuning set, either based on the pretrained\\nversion or from scratch. We repeat the experiment 5 times with\\nrandom data splits and report the median accuracy.\\n(2) Imputation. We run the imputation task on the datasets used\\nin classification as well as the large unlabeled MGH dataset, and\\nmeasure the mean square error and absolute imputation error. To\\nget timeseries with missing values, we randomly mask the values\\nwith an expected mask rate of = 0.2. The masked values are\\nreplaced with a special value.\\nFinally, to evaluate Group Attn.s benefit on efficiency, the total\\ntime of forward computation, backward propagation, and grouping\\nare measured for all methods in all the experiments.\\nTo save space, we only report the average training time per epoch\\nhere and refer readers to Appendix A.8 for the inference time.\\nWe first compare against the Transformer-based methods on\\nmulti-variate datasets (sec. 6.2, 6.3), then compare against the non-\\ndeep learning method GRAIL on uni-variate datasets (sec. 6.4).\\nConfiguration. Please refer to Appendix A.1 for the experiment\\nconfiguration and hyper-parameter settings.\\n6.2\\nEffectiveness: Transformer-Based Methods\\nWe first evaluate the quality of the models trained with full labels\\nfrom scratch. We then show how the pretraining of RITA increases\\nthe accuracy of the downstream tasks.\\n6.2.1\\nfull-label training (Multi-variate classification)\\nResults shown in Figure 3(a) get us the following observations:\\n(1) RITAs advantage over TST. On all four datasets for the clas-\\nsification tasks, Group Attn. and the other three baselines that use\\nRITA architecture (Vanilla, Performer, and Linformer) outperform\\nTST. In particular, Group Attn. outperforms TST by 49 percentage\\npoints on the ECG dataset (88.48% vs 39.93%) with long timeseries.\\nTwo deficiencies in TST may cause its poor performance on the long\\ntimeseries. Firstly, TST concatenates the output embedding vector\\nof each time stamp, then uses a linear classifier to do classification\\non the concatenated vector. When the timeseries is long, the linear\\nclassifier has so many parameters that it tends to overfit easily.\\nSecondly, TST replaces Layer Normalization in vanilla Transformer\\nwith Batch Normalization. When the timeseries is long, it can only\\naccommodate a small number of timeseries in each batch, leading\\nto bias in Batch Normalization.\\n(2) Group-attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on\\n3 out of 4 datasets for classification. Although Linformer works\\nslightly better than Group Attn. on the ECG dataset (90.37% vs\\n88.84%), its performance is the worst in all other cases compared\\nto any other RITA-based methods. Vanilla computes the attention\\nscores precisely. Thus it is expected to work well. However, Group\\nAttn. outperforms Vanilla on WISDM (87.50% vs 86.95%) and is very\\nclose to it on other 3 datasets. This suggests that group attentions\\napproximation quality is good.\\n6.2.2\\npretraining + few label finetune (Multi-variate classification)\\nThe results shown in Table 3 get us the following observation:\\n(1) Pretraining is effective. Pretraining always leads to better\\naccuracy than training with a few labels from scratch. In particular,\\non WISDM data all the methods using RITA architecture increase\\nthe accuracy by at least 10%. This is impressive considering we do\\nnot have a very large unlabeled pre-training set to use.\\n(2) RITAs advantage over TST. our Group Attn. and other\\nthree baselines using RITA architecture (Vanilla, Performer, and\\nLinformer) significantly outperform TST on all four classification\\ndatasets by 25 percentage points.\\n(3) Group Attentions advantage over other attention mech-\\nanisms. Group Attn. is better than Performer and Linformer on 3\\nout of 4 datasets. When compared to Vanilla, Group Attn. is better\\non HHAR and ECG, and comparable on the other two, further con-\\nfirming its high quality on approximation. Further, we notice that\\nLinformer struggles in this setting: in average its accuracy is worse\\nthan Vanilla by 8.22% and Group Attn. by 8.01%. This is because the\\nlow-rank projection operation introduces extra model parameters,\\nmaking Linformer more easily overfit, while overfitting is especially\\nharmful when there are only a few labeled training samples.\\n6.2.3\\nfull-dataset training (Multi-variate imputation)\\nSimilar to classification tasks, the results of imputation tasks\\n(Table.2) show that Group Attn. consistently outperforms the base-\\nlines in training time while achieving comparable/better MSE. Again,\\non the large dataset MGH (length = 10,000), TST and Vanilla fail due\\nto out of memory (OOM) errors. Methods using RITA framework\\n(Group Attn., Performer, Linformer) all achieve very low MSE (are\\nhighly accurate). Among them Linformer is the worst.\\n6.3\\nEfficiency: Transformer-based Methods\\nWe measure the efficiency by the average training time per epoch\\nincluding the cost of the forward computation + backward propaga-\\ntion and the grouping overhead. We first show the results on all the\\n5 datasets in Sec. 6.3.1. We then vary the length of the timeseries\\non the MGH dataset to show group attentions scalability on long\\ntimeseries in Sec. 6.3.2.\\n6.3.1\\nTraining Time: All Multi-variate Datasets\\nThe results in Fig. 3(b) and Table 2 lead to the below observations:\\n(1) Vanilla Self-Attention is not scalable. In average, it takes\\n2-3 minutes to train one epoch when the length of the timeseries is\\nonly 200 (WISDM, HHAR, RWHAR), takes over 15 minutes when\\nthe length increases to 2,000 (ECG), and fails on the long MGH data\\nwhen the length reaches 10,000 due to out of GPU memory.\\n(2) Group Attn.s advantage over all other attention mecha-\\nnisms. As we have shown in Sec. 6.2, Group Attn. is more accurate\\n7\\nDataset\\nLength\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nMSE\\nTime/s\\nWISDM\\n200\\n13.30\\n150.3\\n3.240\\n178.1\\n3.449\\n162.6\\n3.852\\n141.9\\n3.277\\n136.7\\nHHAR\\n200\\n1.085\\n78.2\\n0.2968\\n97.4\\n0.2980\\n82.6\\n0.3198\\n81.1\\n0.2974\\n73.3\\nRWHAR\\n200\\n0.0882\\n83.9\\n0.0478\\n108.1\\n0.0489\\n89.1\\n0.0572\\n98.4\\n0.0478\\n81.3\\nECG\\n2000\\n0.0905\\n696.3\\n0.0037\\n857.9\\n0.0033\\n270.2\\n0.0035\\n291.38\\n0.0038\\n164.36\\nMGH\\n10000\\nN/A\\nN/A\\nN/A\\nN/A\\n0.00014\\n356.2\\n0.00088\\n404.9\\n0.00042\\n54.4\\nTable 2: Imputation results (multi-variate data). The best results are marked with bold.\\nDataset\\nPretrain Size\\nTST [61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nScratch\\nPre.\\nWISDM\\n62,231\\n49.13%\\n50.03%\\n66.16%\\n75.89%\\n66.09%\\n73.97%\\n50.12%\\n67.44%\\n62.56%\\n75.06%\\nHHAR\\n68,294\\n72.56%\\n75.30%\\n75.60%\\n81.35%\\n76.52%\\n80.70%\\n65.94%\\n76.52%\\n76.17%\\n82.62%\\nRWHAR\\n63,599\\n69.46%\\n80.41%\\n85.68%\\n91.14%\\n87.54%\\n91.33%\\n81.03%\\n86.33%\\n86.13%\\n89.63%\\nECG\\n561,358\\n20.98%\\n27.99%\\n42.05%\\n46.16%\\n43.34%\\n45.58%\\n27.19%\\n31.34%\\n42.58%\\n46.39%\\nTable 3: Pretrain + few-label finetuning results. The best results are marked with bold.\\nTraining Time/sec\\nMSE\\n(a) Effectiveness\\n(b) Efficiency\\nFigure 4: Varying the lengths of timeseries.\\nthan Performer and Linformer in classification and imputation tasks,\\nwhile Group Attn. is always faster than Performer, Linformer, and\\nall other baselines on all 5 multi-variate datasets, thus a win-win.\\n(3) The longer the timeseries, the larger the speedup. On\\nthe medium sized ECG dataset with a length of 2,000, Group Attn.\\nhas a speedup of 3.86/1.36/2.27 compared to Vanilla/Performer/Lin-\\nformer. When the length increases to 10,000, the speedup on the\\nMGH dataset increases to 6.59/7.48 compared to Performer/Lin-\\nformer (Vanilla and TST failed in this case) on imputation task\\n(Table. 2). However, even on the short WISDM, HHAR, RWHAR\\ndatasets, Group Attn. still consistently outperforms other methods,\\nconfirming that it does not introduce much overhead. This is be-\\ncause when the length of the timeseries gets longer, Group Attn.\\ngets more opportunities to find windows with similar properties.\\n6.3.2\\nTraining time: Varying the Length\\nIn this experiment, we truncate the original MGH timseries into\\nsequences with the lengths at 2000/4000/6000/8000/10000, and com-\\npare Group Attn. against Vanilla and other attention mechanisms.\\nVanilla cannot handle sequences longer than 8000.\\nThe results in Fig. 4 again show that the longer the timeseries, the\\nlarger the speed up. With comparable MSE, Group Attn. outperforms\\nVanilla by 63X. Moreover, as the length increases from 2000 to 10000,\\nthe training time of Group Attn. only increases from 31.2 seconds\\nto 54.4 seconds per epoch. The reason is that as the timeseires\\nbecomes longer, there are more grouping opportunities because of\\nthe similarity of the timeseries segments.\\nAccuracy\\nTraining Time/sec\\n(a)\\n(b)\\nFigure 5: Comparison to non-deep learning method (uni-\\nvariate data).\\n6.4\\nComparison to Non-deep Learning Methods\\nWe compare against GRAIL, the SOTA of non-deep learning time-\\nseries representation learning. We use the three uni-variate datasets,\\nbecause GRAIL only targets uni-variate timeseries.\\nResults in Fig. 5 show that on all 3 datasets RITA significantly\\noutperforms GRAIL in accuracy by 45, 16, and 21 percentage points\\nbecause of the expressive power of Transformer. Moreover, thanks\\nto the GPU-friendly design of RITA, it is at least 2 faster than\\nGRAIL in training time.\\n6.5\\nAblation Study\\n6.5.1\\nAdaptive Scheduler\\nTo evaluate the effectiveness of RITAs adaptive scheduler (Sec. 5),\\nwe compare it against a baseline using a fixed group number . We\\nvary and the error bound threshold used by RITA.\\nFrom the results in Table 4 we get the following observations:\\n(1) Adaptive Scheduler is better than fixed . Training with\\nAdaptive Scheduler already achieves better or comparable perfor-\\nmance compared to the best performing . More specifically, on\\nthe MGH dataset, dynamic scheduler always achieves better accu-\\nracy and is much faster compared to fixed . On the ECG dataset,\\nalthough fixed is slightly better than adaptive scheduler in accu-\\nracy when setting the N as 512, it runs much slower than adaptive\\nscheduler. Of course, finding the best that balances the accuracy\\nand running time requires careful tuning.\\n(2) Adaptive Scheduler is tuning free. It is robust on both\\naccuracy and running time when varies, while the results of\\nfixed vary significantly when the value of changes. Therefore,\\nAdaptive Scheduler frees the users from tuning the threshold,\\nwhile it is hard to find an appropriate for a given dataset.\\n8\\nDataset\\nTask\\nScheduler\\nParameter\\nMetric\\nTime\\nECG\\nClass.\\nDynamic\\n1.5\\n88.34%\\n292.5\\n2\\n88.48%\\n236.8\\n3\\n87.83%\\n216.8\\nFixed\\n64\\n87.50%\\n255.2\\n128\\n88.96%\\n297.2\\n256\\n88.82%\\n414.1\\n512\\n90.03%\\n662.6\\n1024\\n88.65%\\n873.7\\nMGH\\nImput.\\nDynamic\\n1.5\\n0.00041\\n60.7\\n2\\n0.00040\\n57.9\\n3\\n0.00042\\n54.4\\nFixed\\n128\\n0.00054\\n128.6\\n256\\n0.00053\\n190.2\\n512\\n0.00049\\n240.8\\n1024\\n0.00046\\n323.3\\nTable 4: Adaptive Scheduling VS Fixed N.\\nPretrain Data size\\nFew-label Accuracy\\nN/A\\n62.56%\\n12,446\\n72.94%\\n24,892\\n72.78%\\n37,338\\n74.10%\\n49,784\\n74.22%\\n62,231\\n75.06%\\nTable 5: RITA Pretraining: increasing sizes of pretrain set.\\n6.5.2\\nThe Sizes of the Pretraining Data\\nNext, we evaluate how the number of unlabeled data influences the\\neffectiveness of pretraining. To get empirical results, we pretrain\\nRITA on WISDM dataset with 20%/40%/60%/80% of the pretraining\\ndata and finetune each pretrained model with 100 labels per class.\\nThe results in Table 5 show that: (1) The more pretraining data,\\nthe larger the improvement. The accuracy increases with the\\nsizes of the pretraining data; (2) Marginal utility diminishing.\\nThe first 20% pretraining data gives a 10.38% improvement in accu-\\nracy (72.94% vs 62.56%), while the remaining 80% pretraining data\\nonly gives an additional improvement of 2.12% (75.06% vs 72.94%).\\n7\\nRELATED WORK\\n7.1\\nTimeseries Analytics\\nThere is a great deal of prior work on timeseries analytics methods.\\nThis work can be divided into three categories: (1) non-deep learn-\\ning methods; (2) CNN/RNN-based deep learning methods; and (3)\\nTransformer-based deep learning methods.\\nTraditional Methods. These methods, such as TS-CHIEF [45],\\nHIVE-COTE [33], ROCKET [15] have achieved notable performance\\non public datasets. Despite that, traditional methods suffer from\\none or more issues: they (1) rely on expert knowledge for feature\\nextraction; (2) incur heavy computation cost and are inappropriate\\nfor GPU devices; (3) support only uni-variate timeseries; (4) perform\\nclassification solely. Some work [61] shows that the transformed-\\nbased methods outperform these traditional methods especially on\\nmulti-variate timeseries.\\nIn particular, as the SOTA of timeseries representation learn-\\ning, GRAIL [40] extracts landmarks from data and computes the\\nrepresentations with the combination of the landmarks. However,\\nGRAIL only supports uni-variate timeseries. Our experiments (Sec. 6.4)\\nshow that RITA significantly outperforms GRAIL in both effective-\\nness and efficiency on uni-variate timeseries.\\nCNN/RNN-based Deep Learning Methods. CNN-based methods,\\nsuch as InceptionTime [21] and Resnet [19], are good at classifica-\\ntion tasks, but can not handle generative tasks such as forecasting\\nbecause of the inductive bias of convolution networks. RNN-based\\nmethods, such as Brit [7] and deepAR [44], are capable for classifi-\\ncation, regression and generation. However, the recurrent structure\\nbrings a lot of problems: (1) limiting the models ability in captur-\\ning long-range correlation; (2) notoriously difficult to train [41]\\nbecause of gradient vanishing and exploding problem. As a result,\\nsuch methods can hardly scale to very long timeseries.\\nTransformer-based Deep Learning Methods. Given that Trans-\\nformer is the best choice for backbone in almost all sequence mod-\\neling tasks, some effort has been made to apply Transformer to\\ntimeseries analytics. Targeting forecasting of uni-variate timeseries,\\nLogTrans [30] introduced a log sparsity assumption to attention\\ncomputation. Informer [62] pushes LogTrans a step further and\\nscales forecasting to multi-variate timeseries. Autoformer [57] per-\\nforms forecasting by decomposing timeseries into two parts, i.e.\\nthe trend part and the seasonal part.\\nFor imputation tasks, CDSA [37] outperforms statistical meth-\\nods and the SOTA of RNN-based method Brit [7] on 3 public and\\n2 competition datasets. For timeseries classification, AutoTrans-\\nformer [43] performs architecture search to adapt to the tasks\\nin different domains. For timeseries anomaly detection, Anomaly\\nTransformer [58] outperforms many widely-used methods such\\nas OmniAnomaly [47], assuming the attention score maps show\\nGaussian distribution.\\nAll of these works are designed for specific tasks, rather than\\nfunctioning as a representation learning framework to serve\\ndifferent downstream tasks. To fill this gap, some researchers pro-\\nposed a Transformer-based architecture, called TST [61]. Like RITA,\\nTST supports regression, classification, and unsupervised learning\\nthrough the cloze test pretraining task on timeseries. However,\\nTST directly uses the classical Vanilla self-attention, thus not scal-\\nable to long timeseries as shown in our experiments (Sec. 6.3.2).\\n7.2\\nEfficient Transformers\\nThe need of improving the scalability of Transformers has led to\\nmore efficient variations of Transformers, especially for accommo-\\ndating long text data in NLP [49].\\nIntroducing fixed/random patterns to self-attention mechanism\\nis an intuitive idea. Sparse Transformer [9] and Longformer [3] only\\ncompute attention at fixed intervals. ETC [2] and BigBird [60] use\\nglobal-local attention: the attention computation is limited within\\na fixed radius, while some auxiliary tokens are added to attend/get\\nattended globally. The deficiencies of fixed attention patterns are\\nobvious: it heavily depends on users to give an optimal setting.\\nTo decrease the reliance on human labor, some works seek to\\nintroduce learnable/adaptive attention patterns instead of fixed\\npatterns. Reformer [26] proposed only computing the dominant\\nattention terms based on their observation of sparsity in atten-\\ntion matrix from language/image data. Such sparsity is intuitive\\nin language data, in which a words attention mainly focuses on\\nthe nearby sentences. However, attention in timeseries data shows\\nstrong seasonal patterns rather than sparse patterns, mainly as\\n9\\nresult of the periodicity of timeseries data. Therefore, such works\\ndo not work well for timeseries.\\nApart from introducing attention patterns, some works seek\\nto solve this problem with applied mathematics techniques. Lin-\\nformer [54] performs a projection to decrease the size of query,\\nkey and value matrices before attention computation, because the\\nattention matrix tends to be low-ranked. Performer [10] uses linear\\nfunctions to approximate the kernel function softmax, making at-\\ntention computation commutative. When the sequence length is far\\ngreater than the dimension of embedding vectors, Performer ben-\\nefits from changing the order of matrix multiplication. Linformer\\nand Performer do not depend on the unique properties of language\\ndata, thus potentially fitting timeseries better than other techniques,\\nwhich is why we compared against them in our experiments. How-\\never as shown in Sec. 6, our group attention significantly outper-\\nforms them in both accuracy and efficiency (training time), because\\ngroup attention fully leverages the periodicity of timeseries.\\n8\\nCONCLUSION\\nIn this work, we presented RITA, an automatic, self-supervised, and\\nscalable timeseries analytics tool. RITA effectively adapts Trans-\\nformer, popular in NLP, into timeseries analytics. As the key com-\\nponent of RITA, group attention eliminates the performance bottle-\\nneck of the classical self-attention mechanisms, thus successfully\\nscaling RITA to highly complex, long timeseries data. Our experi-\\nments confirm that RITA significantly speeds up the state-of-the-art\\nby 63X with a better accuracy.\\nREFERENCES\\n[1] Martn Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,\\nCraig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al.\\n2016. Tensorflow: Large-scale machine learning on heterogeneous distributed\\nsystems. arXiv preprint arXiv:1603.04467 (2016).\\n[2] Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020.\\nETC: Encoding long and structured inputs in transformers.\\narXiv preprint\\narXiv:2004.08483 (2020).\\n[3] Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-\\ndocument transformer. arXiv preprint arXiv:2004.05150 (2020).\\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\\ninformation processing systems 33 (2020), 18771901.\\n[5] C Bui, N Pham, A Vo, A Tran, A Nguyen, and T Le. 2017. Time series forecasting\\nfor healthcare diagnosis and prognostics with the focus on cardiovascular dis-\\neases. In International conference on the development of biomedical engineering in\\nVietnam. Springer, 809818.\\n[6] Lei Cao, Wenbo Tao, Sungtae An, Jing Jin, Yizhou Yan, Xiaoyu Liu, Wendong\\nGe, Adam Sah, Leilani Battle, Jimeng Sun, Remco Chang, M. Brandon Westover,\\nSamuel Madden, and Michael Stonebraker. 2019. Smile: A System to Support\\nMachine Learning on EEG Data at Scale. Proc. VLDB Endow. 12, 12 (2019), 2230\\n2241.\\n[7] Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. 2018.\\nBrits:\\nBidirectional recurrent imputation for time series. Advances in neural information\\nprocessing systems 31 (2018).\\n[8] Chris Chatfield. 2000. Time-series forecasting. Chapman and Hall/CRC.\\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating\\nlong sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).\\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,\\nAndreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\\nLukasz Kaiser, et al. 2020. Rethinking attention with performers. arXiv preprint\\narXiv:2009.14794 (2020).\\n[11] Andrew A Cook, Gksel Msrl, and Zhong Fan. 2019. Anomaly detection for IoT\\ntime-series data: A survey. IEEE Internet of Things Journal 7, 7 (2019), 64816494.\\n[12] Corinna Cortes and Vladimir Vapnik. 1995. Support-vector networks. Machine\\nlearning 20, 3 (1995), 273297.\\n[13] David R Cox. 1958. The regression analysis of binary sequences. Journal of the\\nRoyal Statistical Society: Series B (Methodological) 20, 2 (1958), 215232.\\n[14] Benjamin F Crabtree, Subhash C Ray, Priscilla M Schmidt, Patrick T OConnor,\\nand David D Schmidt. 1990. The individual over time: time series applications in\\nhealth care research. Journal of clinical epidemiology 43, 3 (1990), 241260.\\n[15] Angus Dempster, Franois Petitjean, and Geoffrey I. Webb. 2020. ROCKET: excep-\\ntionally fast and accurate time series classification using random convolutional\\nkernels. Data Min. Knowl. Discov. 34, 5 (2020), 14541495.\\n[16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\\nPre-training of Deep Bidirectional Transformers for Language Understanding. In\\nProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, NAACL-HLT 2019,\\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). 4171\\n4186.\\n[17] Evelyn Fix and Joseph Lawson Hodges. 1989. Discriminatory analysis. Nonpara-\\nmetric discrimination: Consistency properties. International Statistical Review/Re-\\nvue Internationale de Statistique 57, 3 (1989), 238247.\\n[18] Philip George Guest and Philip George Guest. 2012. Numerical methods of curve\\nfitting. Cambridge University Press.\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\\nlearning for image recognition. In Proceedings of the IEEE conference on computer\\nvision and pattern recognition. 770778.\\n[20] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassane Idoumghar,\\nand Pierre-Alain Muller. 2019. Deep learning for time series classification: a\\nreview. Data mining and knowledge discovery 33, 4 (2019), 917963.\\n[21] Hassan Ismail Fawaz, Benjamin Lucas, Germain Forestier, Charlotte Pelletier,\\nDaniel F Schmidt, Jonathan Weber, Geoffrey I Webb, Lhassane Idoumghar, Pierre-\\nAlain Muller, and Franois Petitjean. 2020. Inceptiontime: Finding alexnet for\\ntime series classification. Data Mining and Knowledge Discovery 34, 6 (2020),\\n19361962.\\n[22] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization\\nfor nearest neighbor search. IEEE transactions on pattern analysis and machine\\nintelligence 33, 1 (2010), 117128.\\n[23] Jeff Johnson, Matthijs Douze, and Herv Jgou. 2019. Billion-scale similarity\\nsearch with gpus. IEEE Transactions on Big Data 7, 3 (2019), 535547.\\n[24] Richard M Karp. 1972. Reducibility among combinatorial problems. In Complexity\\nof computer computations. Springer, 85103.\\n[25] Eamonn Keogh, Kaushik Chakrabarti, Michael Pazzani, and Sharad Mehrotra.\\n2001. Dimensionality reduction for fast similarity search in large time series\\ndatabases. Knowledge and information Systems 3, 3 (2001), 263286.\\n[26] Nikita Kitaev, ukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient\\ntransformer. arXiv preprint arXiv:2001.04451 (2020).\\n[27] John Kraft and Arthur Kraft. 1977. Determinants of common stock prices: A time\\nseries analysis. The journal of finance 32, 2 (1977), 417425.\\n[28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Clas-\\nsification with Deep Convolutional Neural Networks. In Advances in Neural\\nInformation Processing Systems, F. Pereira, C.J. Burges, L. Bottou, and K.Q. Wein-\\nberger (Eds.), Vol. 25. Curran Associates, Inc. https://proceedings.neurips.cc/\\npaper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf\\n[29] Oscar D Lara and Miguel A Labrador. 2012. A survey on human activity recog-\\nnition using wearable sensors. IEEE communications surveys & tutorials 15, 3\\n(2012), 11921209.\\n[30] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang,\\nand Xifeng Yan. 2019. Enhancing the locality and breaking the memory bottle-\\nneck of transformer on time series forecasting. Advances in Neural Information\\nProcessing Systems 32 (2019).\\n[31] T Warren Liao. 2005. Clustering of time series dataa survey. Pattern recognition\\n38, 11 (2005), 18571874.\\n[32] Rake& Agrawal King-lp Lin and Harpreet S Sawhney Kyuseok Shim. 1995. Fast\\nsimilarity search in the presence of noise, scaling, and translation in time-series\\ndatabases. In Proceeding of the 21th International Conference on Very Large Data\\nBases. 490501.\\n[33] Jason Lines, Sarah Taylor, and Anthony Bagnall. 2018. Time Series Classification\\nwith HIVE-COTE: The Hierarchical Vote Collective of Transformation-Based\\nEnsembles. ACM Trans. Knowl. Discov. Data 12, 5, Article 52 (jul 2018), 35 pages.\\n[34] Feifei Liu, Chengyu Liu, Lina Zhao, Xiangyu Zhang, Xiaoling Wu, Xiaoyan\\nXu, Yulin Liu, Caiyun Ma, Shoushui Wei, Zhiqiang He, et al. 2018. An open\\naccess database for evaluating the algorithms of electrocardiogram rhythm and\\nmorphology abnormality detection. Journal of Medical Imaging and Health\\nInformatics 8, 7 (2018), 13681373.\\n[35] Stuart Lloyd. 1982. Least squares quantization in PCM. IEEE transactions on\\ninformation theory 28, 2 (1982), 129137.\\n[36] Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization.\\narXiv preprint arXiv:1711.05101 (2017).\\n[37] Jiawei Ma, Zheng Shou, Alireza Zareian, Hassan Mansour, Anthony Vetro, and\\nShih-Fu Chang. 2019. CDSA: cross-dimensional self-attention for multivariate,\\ngeo-tagged time series imputation. arXiv preprint arXiv:1905.09904 (2019).\\n10\\n[38] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\\nnearest neighbor search using hierarchical navigable small world graphs. IEEE\\ntransactions on pattern analysis and machine intelligence 42, 4 (2018), 824836.\\n[39] Tripti Negi and Veena Bansal. 2005. Time series: Similarity search and its appli-\\ncations. In Proceedings of the International Conference on Systemics, Cybernetics\\nand Informatics: ICSCI-04, Hyderabad, India. 528533.\\n[40] John Paparrizos and Michael J Franklin. 2019. Grail: efficient time-series repre-\\nsentation learning. Proceedings of the VLDB Endowment 12, 11 (2019), 17621777.\\n[41] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2013. On the difficulty\\nof training recurrent neural networks. In International conference on machine\\nlearning. PMLR, 13101318.\\n[42] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms.\\nNeural networks 12, 1 (1999), 145151.\\n[43] Yankun Ren, Longfei Li, Xinxing Yang, and Jun Zhou. 2022. AutoTransformer:\\nAutomatic Transformer Architecture Design for Time Series Classification. In\\nPacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 143\\n155.\\n[44] David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. 2020.\\nDeepAR: Probabilistic forecasting with autoregressive recurrent networks. Inter-\\nnational Journal of Forecasting 36, 3 (2020), 11811191.\\n[45] Ahmed Shifaz, Charlotte Pelletier, Franois Petitjean, and Geoffrey I. Webb. 2020.\\nTS-CHIEF: a scalable and accurate forest algorithm for time series classification.\\nData Mining and Knowledge Discovery 34 (2020), 742775.\\n[46] Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow,\\nMikkel Baun Kjrgaard, Anind Dey, Tobias Sonne, and Mads Mller Jensen.\\n2015. Smart devices are different: Assessing and mitigatingmobile sensing het-\\nerogeneities for activity recognition. In Proceedings of the 13th ACM conference\\non embedded networked sensor systems. 127140.\\n[47] Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust\\nanomaly detection for multivariate time series through stochastic recurrent\\nneural network. In Proceedings of the 25th ACM SIGKDD international conference\\non knowledge discovery & data mining. 28282837.\\n[48] Timo Sztyler and Heiner Stuckenschmidt. 2016. On-body localization of wearable\\ndevices: An investigation of position-aware activity recognition. In 2016 IEEE\\nInternational Conference on Pervasive Computing and Communications (PerCom).\\nIEEE, 19.\\n[49] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020. Efficient\\ntransformers: A survey. ACM Computing Surveys (CSUR) (2020).\\n[50] Mingyan Teng. 2010. Anomaly detection on time series. In 2010 IEEE International\\nConference on Progress in Informatics and Computing, Vol. 1. IEEE, 603608.\\n[51] Patrick A Thompson. 1990. An MSE statistic for comparing forecast accuracy\\nacross series. International Journal of Forecasting 6, 2 (1990), 219227.\\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\\nyou Need. In Advances in Neural Information Processing Systems 30: Annual Con-\\nference on Neural Information Processing Systems 2017, December 4-9, 2017, Long\\nBeach, CA, USA. 59986008.\\n[53] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\\nReddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser,\\nJonathan Bright, Stfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern,\\nEric Larson, C J Carey, lhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas,\\nDenis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\\nCharles R. Harris, Anne M. Archibald, Antnio H. Ribeiro, Fabian Pedregosa,\\nPaul van Mulbregt, and SciPy 1.0 Contributors. 2020. SciPy 1.0: Fundamental Al-\\ngorithms for Scientific Computing in Python. Nature Methods 17 (2020), 261272.\\nhttps://doi.org/10.1038/s41592-019-0686-2\\n[54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. 2020. Lin-\\nformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768\\n(2020).\\n[55] Gary M Weiss, Kenichi Yoneda, and Thaier Hayajneh. 2019. Smartphone and\\nsmartwatch-based biometrics using activities of daily living. IEEE Access 7 (2019),\\n133190133202.\\n[56] Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke, and Huan Xu. 2021.\\nRobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity Detection.\\nIn Proceedings of the 2021 International Conference on Management of Data (Virtual\\nEvent, China) (SIGMOD 21). Association for Computing Machinery, New York,\\nNY, USA, 23282337. https://doi.org/10.1145/3448016.3452779\\n[57] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021. Autoformer: De-\\ncomposition transformers with auto-correlation for long-term series forecasting.\\nAdvances in Neural Information Processing Systems 34 (2021), 2241922430.\\n[58] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2021. Anomaly\\nTransformer: Time Series Anomaly Detection with Association Discrepancy.\\narXiv preprint arXiv:2110.02642 (2021).\\n[59] Dianmin Yue, Xiaodan Wu, Yunfeng Wang, Yue Li, and Chao-Hsien Chu. 2007. A\\nreview of data mining-based financial fraud detection research. In 2007 Interna-\\ntional Conference on Wireless Communications, Networking and Mobile Computing.\\nIeee, 55195522.\\n[60] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris\\nAlberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\\net al. 2020. Big bird: Transformers for longer sequences. Advances in Neural\\nInformation Processing Systems 33 (2020), 1728317297.\\n[61] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha Bhamidipaty, and\\nCarsten Eickhoff. 2021. A Transformer-based Framework for Multivariate Time\\nSeries Representation Learning. In KDD 21: The 27th ACM SIGKDD Conference\\non Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-18,\\n2021. 21142124.\\n[62] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\\nquence time-series forecasting. In Proceedings of AAAI.\\nA\\nAPPENDIX: SUPPLEMENTARY MATERIAL\\nA.1\\nExperiment Configuration and\\nHyper-parameter Settings\\nConfiguration. All models were trained on an NVIDIA Tesla V100\\n16GB GPU. All the methods are optimized with AdamW [36] of\\nwhich the starting learning rate and weight decay parameter are\\nboth 14. In full-label training scenario, we train the models for\\n100 epochs. In pretraining + few-label finetuning scenario, as the\\npretrained models require fewer epochs to converge [61], we train\\nthe model for 50 epochs. For a fair comparison, the baselines use a\\nmaximal batch size within GPUs capacity during training.\\nAs for model hyper-parameter setting, RITA and the baselines\\nuse a Transformer structure balancing Vanilla s accuracy and\\nefficiency: 8-layer stack of 2-head attention with hidden vectors\\nin dimension of 64. Convolution kernel size is set to 5 by default.\\nWe set the error bound threshold (, Sec. 5.1) of Group Attention\\nto 2, as it balances the accuracy and the efficiency in general on\\nall datasets. Because Linformer requires the users to set the sizes\\nof projection matrix, in different settings we choose an accuracy-\\nefficiency balancing one among {64,128,256,512}.\\nA.2\\nEfficient Computation of Group Attention\\nAlgorithm 1 Efficient Computation of Group Attention\\nRequire: ,, ,, \\nEnsure: ,R,R,N,N\\n1: function group_attention(,, )\\n2:\\nfor = 0 1 do\\n3:\\ne1\\n=0 (== )\\n4:\\ne\\n5:\\nfor = 0 1 do\\n6:\\nfor = 0 1 do\\n7:\\n,(e,)\\n8:\\nfor = 0 1 do\\n9:\\n1\\n=0 ,\\n10:\\nfor = 0 1 do\\n11:\\n1\\n=0\\n( e,)\\n\\ne\\n12:\\nreturn \\nIn Alg. 1, we denoteto be the size of the group, to\\nbe the number of groups, rto be the representative key of the \\ngroup and R to be the matrix consisting of all r, to be\\nthe group that kbelongs to. ,are the packing matrices of query\\nvectors and value vectors as described in Sec.2. Alg. 1 outputs the\\n11\\npacking matrix for new feature emebddings {1, ...,}, where \\ncorresponds to the feature embedding of . Lines 2-3 implement\\nthe embedding aggregation operation, while Lines 8-11 implement\\nthe group softmax function.\\nA.3\\nThe Algorithms and Optimality Proof for\\nDynamically Determing Batch Size\\nAlgorithm 2 Binary Search for Batch Size\\nRequire: , \\nEnsure: 1 , 1 \\n1: function binary_search(, )\\n2:\\n1\\n3:\\n\\n4:\\n\\n5:\\n\\n6:\\nwhile do\\n7:\\n \\n8:\\n()\\n9:\\n\\n10:\\n\\n\\n11:\\nif 0.9 > then\\n12:\\n+ 1\\n13:\\n\\n14:\\nelse\\n15:\\n1\\n16:\\n+\\n2\\n17:\\nreturn \\nAlgorithm 3 Dynamic Programming for Plane Division\\nRequire: , , , \\nEnsure: 1 , 1 \\n1: function cost(S)\\n2:\\nif || < then return +\\n3:\\n, , \\n4:\\n(|, )\\nreturn (, , |)\\n5: function dynamic_programming(, , )\\n6:\\nfor 1 = 1 do\\n7:\\nfor 2 = 1 1 do\\n8:\\nfor = 1 1 do\\n9:\\n{2 1, }\\n10:\\n() ()\\n11:\\nfor = 1 do\\n12:\\n{2 1,}\\n13:\\n() ((),() + ())\\n14:\\n2,1 (1)\\n15:\\n16:\\nfor = 1 do\\n17:\\n() (1,)\\n18:\\nfor = 1 do\\n19:\\n() ((),() + (,))\\nreturn ()\\nWe describe Alg. 3 and intuitively show its optimality. We assume\\nthat Scipy [53] learns an optimal function in Line 4 so that function\\nCOST gives the optimal estimation error when fitting the points in\\nset . When fitting very few points, we assign an infinite cost to\\nprevent a biased fitting function (Line 2). () denotes the minimal\\nestimation error for points in sub-plane {2 1, }. In\\nLines 11-13, we enumerate all possible ways of cutting {2 \\n1, } horizontally into two sub-plane {2 1, } and\\n{2 1,} by iterating from 1 to n. Choosing the\\ncutting strategy that minimizes estimation error gets us a(1) with\\nminimal estimation error for sub-plane {2 1, 1}, which\\nis recorded as 1,2 in Line 14. () denotes the minimal estimation\\nerror for sub-plane {}. We enumerate all the possible ways\\nof cutting {} vertically into two sub-plane {} and {\\n} by iterating from 1 to (Line 17-19). Finally, we have the\\nminimal estimation error for the whole plane as (). Based\\non the above discussion, this algorithm guarantees to not miss any\\nbetter solution, hence optimal.\\nA.4\\nThe Correctness of Group Attention\\nLemma 3. Assuming the windows belonging to the same group \\nhave the same key vector, i.e. = (), then the feature\\nembedding produced by the original self-attention mechanism is\\nidentical to the output of our group attention mechanism implemented\\nin Algorithm 1.\\nProof. Denote e\\nto be the representative vectors of , i.e. e\\n=\\n= (). Algorithm 1 gives that\\ne=\\n1\\n\\n=0\\n(== )v, e,= q r\\n=\\n1\\n\\n=0\\n(e,), e=\\n1\\n\\n=0\\ne,\\n\\ne\\n(7)\\nBy the canonical self-attention mechanism introduced in Sec. 2,\\nwe get:\\n,= q kj, ,=\\n(,)\\n1\\n=0 (,)\\n, o=\\n1\\n\\n=0\\n,v\\n(8)\\nWith 7 and 8, we have\\n1\\n\\n=0\\n(,) =\\n1\\n\\n=0\\n(q k)\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )(q k)\\n=\\n1\\n\\n=0\\n(q r)\\n1\\n\\n=0\\n(== )\\n=\\n1\\n\\n=0\\n(q r)\\n=\\n1\\n\\n=0\\n(e,)\\n= \\n(9)\\n12\\nFurther,\\no=\\n1\\n\\n=0\\n,vj\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== ),v\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(,)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q k)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n1\\n\\n=0\\n(== )\\n(q rj)\\n1\\n=0 (,)\\nv\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\n1\\n\\n=0\\n(== )v\\n=\\n1\\n\\n=0\\n(q rj)\\n1\\n=0 (,)\\ne\\n(10)\\nCombining (7), (9) (10), we have oi = N 1\\nj=0\\nePi,j\\nsi evj = eoi.\\nThis concludes that the output of our group attention is identical\\nto vanilla self-attentions.\\n\\nA.5\\nThe Proof of Error Bound (Lemma 1)\\nProof. We have\\n(,)\\n(,) = (q ek)\\n(q k) = (q (ekk))\\n= (||q||  ||ekk||  (q,ekk))\\n(11)\\nSo\\n() (,)\\n(,) ()\\n(12)\\nThen we have:\\n,\\n,\\n=\\n(,)\\n\\n=1 (,)\\n/\\n(,)\\n\\n=1 (,)\\n= (,)\\n(,)\\n\\n=1 (,)\\n\\n=1 (,)\\n(13)\\nCombining (12) (13), the error is bounded by\\n(2) ,\\n,\\n(2)\\n(14)\\nThus, if d ln()\\n2R , 1\\nAi,j\\nAi,j . This proves Lemma 1.\\nA.6\\nThe Proof of Merge Operation (Lemma 2)\\nProof. Denote the cluster size of to be .After merge-\\ning, the new center will be:\\n =\\n\\n=1 \\n\\n=1 \\nFor [1,], , it holds that:\\n|| || + || ()\\n= || + |\\n\\n=1 \\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n|\\n= || + |\\n\\n=1 ()\\n\\n=1 \\n|\\n= || +\\n| \\n=1 () |\\n\\n=1 \\n|| +\\n\\n=1 ||\\n\\n=1 \\n=\\n\\n=1 (|| + ||)\\n\\n=1 \\n\\n\\n=1 \\n\\n=1 \\n= \\n(15)\\nA.7\\nDownstream Tasks\\nRITA supports a variety of downstream tasks. In this section, we\\nshow that with minimal modification RITA can effectively support\\nclassification, imputation and forecasting tasks. Other unsupervised\\ntasks such as similarity search or clustering are naturally supported\\nby extracting feature embeddings from RITA.\\nA.7.1\\nClassification\\nTo classify timeseries, we input timeseries to the model as described\\nin Sec. 3 and attach a special token [CLS] as the first input em-\\nbedding. [CLS]s embedding acts as the embedding for the entire\\ntimeseries, and the output representation of [CLS] is fed into a\\nclassifier: y = Softmax(WclsZ[CLS] + Bcls), where [] Ris\\nthe output representation of [CLS], C is the number of classes, and\\nWcls RCd, Bcls RC are learnable parameters for classification\\ntask. The result vector Rrepresents the possibility that the\\ninput timeseries belongs to each class.\\nWe apply Cross Entropy Loss as the loss function of the classi-\\nfication task [13]: L = 1\\nC\\nC\\ni=1 y(i)log(y(i)), where is a binary\\nindicator for ground truth label:\\n() =\\n(\\n1\\nis ground truth label\\n0\\n\\n(16)\\nA.7.2\\nImputation\\nTimeseries are mainly generated by sensors, a common problem\\nof which is missing values. This becomes a challenge when many\\ndownstream analytics require the missing values to be recovered.\\nThe recovering task is imputation.\\nDenote the real timeseries asR, the observed timeseries\\nwith missing values as R, and the set of missing values\\npositions as . We scale the values of all timeseries to non-negative\\nand use a special value (-1) to indicate missing values:\\n(, ) =\\n(\\n1\\n(, ) \\n(, )\\n(, ) \\n(17)\\nis fed into the RITA as input, and the output representa-\\ntions are concatenated and fed into a Transpose Convolution layer\\nwhich decodes the output embedding vectors from hidden space to\\ntimeseries values, corresponding to the convolution operation in\\n13\\nthe input stage, i.e., Y = TransposeCNN (Z1 +Z2 +... +Zn), where\\nRis the recovered timeseries, and Ris the output of\\neach position.\\nHere Mean Square Error is chosen as the loss function [51]:\\n=\\n1\\n||\\n\\n(,)((, ) (, ))2.\\nA.7.3\\nForecasting\\nForecasting can be regarded as a special case of imputation, in\\nwhich all missing values are at the end of timeseries.\\nSo like in imputation task, we scale the timeseries to non-\\nnegative and use a special value (-1) to indicate the values to be\\npredicted:\\n(, ) =\\n(\\n(, )\\n\\n1\\n\\n(18)\\nWhere is the observed timestamp. Then the output\\nrepresentations are fed into a Transpose Convolution layer using\\nMean Squared Error as loss function, as described above.\\nA.7.4\\nOther Unsupervised Tasks\\nRITA naturally supports other unsupervised tasks, such as similar-\\nity search and clustering [25, 31, 32], by producing the embedding\\nof one timeseries (output representation of the special token [CLS]).\\nClustering can be performed on the embeddings with flexible choice\\nof distance metrics. Similarly, a high dimensional similarity search\\nsystem [22, 23, 38] can be built on the embeddings.\\nA.8\\nInference Time\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.18\\n2.26\\n2.35\\n2.22\\n2.17\\nHHAR\\n200\\n1.19\\n1.23\\n1.28\\n1.21\\n1.18\\nRWHAR\\n200\\n1.32\\n1.37\\n1.42\\n1.34\\n1.31\\nECG\\n2000\\n18.44\\n15.26\\n5.80\\n6.08\\n5.16\\nTable 6: Inference time: Classification on multi-variate data\\n(seconds).\\nDataset\\nLength\\nTST[61]\\nVanilla\\nPerformer\\nLinformer\\nGroup Attn.\\nWISDM\\n200\\n2.03\\n2.11\\n2.19\\n2.07\\n2.02\\nHHAR\\n200\\n1.11\\n1.14\\n1.19\\n1.12\\n1.10\\nRWHAR\\n200\\n1.23\\n1.27\\n1.32\\n1.25\\n1.22\\nECG\\n2000\\n17.22\\n14.32\\n4.73\\n4.99\\n4.11\\nMGH\\n10000\\nN/A\\nN/A\\n6.58\\n6.88\\n1.35\\nTable 7: Inference time: Imputation on multi-variate data\\n(seconds).\\nIn this section, we present the average inference time on valida-\\ntion sets. The results in Table. 6 and 7 correspond to the average\\ninference time on validation sets of classification and imputation\\ntasks, respectively. Consistent with the results in Section. 6.3, our\\nmethod Group Attn. outperforms the baselines on both classifica-\\ntion and imputation tasks, particularly on the datasets comprising\\nlong timeseries (ECG and MGH).\\n14\\n')],\n",
       "  Document(metadata={'source': 'Wikipedia', 'title': 'Attention is all you need'}, page_content='Page: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 140,000 times.\\n\\nPage: All You Need Is Kill\\nSummary: All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Rysuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\nPage: All You Need Is Love\\nSummary: \"All You Need Is Love\" is a song by the English rock band the Beatles that was released as a non-album single in July 1967, with \"Baby, You\\'re a Rich Man\" as its B-side. It was written by John Lennon and credited to the LennonMcCartney partnership. The song was Britain\\'s contribution to Our World, the first live global television link, for which the band were shown performing it at EMI Studios in London on 25 June. The programme was broadcast via satellite and seen by an audience of over 400 million in 25 countries. Lennon\\'s lyrics were deliberately simplistic, to allow for broad appeal to the show\\'s international audience, and captured the utopian ideals associated with the Summer of Love. The single topped sales charts in Britain, the United States and many other countries, and became an anthem for the counterculture\\'s embrace of flower power philosophy.\\nOur World coincide'),\n",
       "  Document(metadata={'source': 'https://arxiv.org/abs/2501.05730v1', 'title': '1. What is the concept of \"Attention is all you need\" and how is it relevant in the field of artificial intelligence?'}, page_content='In just 3 minutes help us improve arXiv: cs arXiv:2501.05730v1 arXiv author ID The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms. Subjects:   Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as:    arXiv:2501.05730 [cs.LG] (or arXiv:2501.05730v1 [cs.LG] for this version) From: Guoxin Feng [view email] Access Paper: cs.LG cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status '),\n",
       "  Document(metadata={'source': 'https://research.google/pubs/attention-is-all-you-need/', 'title': '2. Can you provide a detailed explanation or summary of the \"Attention is all you need\" principle and its significance in neural network models?'}, page_content='The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms')],\n",
       " 'new_questions': ['1. What is the concept of \"Attention is all you need\" and how is it relevant in the field of artificial intelligence?',\n",
       "  '2. Can you provide a detailed explanation or summary of the \"Attention is all you need\" principle and its significance in neural network models?'],\n",
       " 'vector': ['c8cb216c-488a-4fba-b8c4-a41f9c81afd1',\n",
       "  '71d53a04-8e06-4cdc-8d1c-4cd1ffb0f900',\n",
       "  '868669fe-00d2-47ae-a69a-73db04f35401',\n",
       "  '1372a52e-cf76-4e8d-8037-834b254617a9',\n",
       "  '35688aa4-9b03-4b1a-bd7d-59368b9941e0',\n",
       "  'a520d2b8-fbfa-405d-80dd-18b30b620dc4']}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def rag_result(question, document, summary):\n",
    "    template = \"\"\"Generate answer of the given question: {question} \\n based on the following summary: {summary} \\n\n",
    "                Also adding full document: {document}\"\"\"\n",
    "\n",
    "    document = \"\\n\".join([doc.metadata.get('Summary', '') if 'Summary' in doc.metadata else doc.page_content for doc in document ])\n",
    "    # prompt = prompt.format(summary=result['summary'], document=document)\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # print(prompt)\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    rag_chain =  (RunnablePassthrough().bind(\n",
    "                    question=RunnablePassthrough(),\n",
    "                    summary=RunnablePassthrough(),\n",
    "                    document=RunnablePassthrough()\n",
    "                ) \n",
    "                | prompt\n",
    "                | llm\n",
    "                | StrOutputParser())\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 00:34:28.316 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:28.317 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.090 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.098 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.105 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.107 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.108 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.110 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.114 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.118 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.120 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.121 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.123 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.124 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.125 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.126 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.127 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.130 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.137 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.138 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.140 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.141 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.143 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.144 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.150 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.150 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.158 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.211 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.212 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.213 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.215 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.216 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.217 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.218 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.220 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.221 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.228 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.230 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.231 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.275 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.276 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.278 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.280 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.290 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.291 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.301 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.324 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.326 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.327 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.328 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.353 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.354 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.355 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.356 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.356 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.357 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.357 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.370 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.371 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.371 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.372 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.404 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.405 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.406 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.452 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.453 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.453 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.454 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.455 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.468 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.469 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.518 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.551 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.560 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.603 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.631 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.641 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.644 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.646 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.652 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.666 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.704 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.722 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.785 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.789 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.791 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.792 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.795 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.802 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.809 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.810 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.812 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.840 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.841 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.841 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.842 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.844 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.844 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.845 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.874 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.874 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.880 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.883 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.884 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.911 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.923 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.924 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.925 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.926 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.928 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.965 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.966 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.967 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.967 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.967 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.988 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:32.989 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.018 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.019 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.020 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.042 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.043 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.043 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.044 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.045 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.045 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.046 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.064 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.065 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.066 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.067 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.100 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.101 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.101 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.103 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.104 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.202 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.208 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.235 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.239 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.240 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.244 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.244 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.262 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.265 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.265 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.271 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.312 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.312 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.319 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.320 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.320 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.407 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.408 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.411 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.412 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.438 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.440 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.507 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.512 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.513 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.536 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.578 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.582 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.623 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.626 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.627 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.654 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.655 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.658 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.660 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.661 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.715 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.716 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.719 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.740 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.742 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.744 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.804 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.805 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.814 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.821 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.822 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.828 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.829 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.878 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.878 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.880 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.886 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.887 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.888 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.894 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.894 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.895 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.908 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.908 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.909 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.910 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.912 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.913 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.930 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.930 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.930 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.933 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.961 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.961 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.962 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.962 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.964 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:33.965 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.000 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.001 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.001 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.001 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.005 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.006 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.070 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.072 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.073 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.076 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.077 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.078 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.088 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.089 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.154 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.161 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.167 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.171 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.203 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.204 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.205 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.206 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.207 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.254 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.255 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.255 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.256 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.257 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.266 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.267 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.269 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.270 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.281 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.282 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.322 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.323 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.325 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.326 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.383 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.384 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.393 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.393 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.394 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.394 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.397 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.398 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.399 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.400 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.459 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.459 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.460 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.460 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.472 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.474 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.475 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.477 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.478 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.480 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.481 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.483 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.484 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.487 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.543 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.548 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.549 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.593 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.594 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.596 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.597 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.598 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.599 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.600 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.601 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.616 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.617 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.618 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.619 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.619 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.642 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.643 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.645 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.675 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.736 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.737 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.738 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.765 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.770 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.798 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.800 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.801 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.825 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.826 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.872 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.873 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.875 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.876 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.897 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.898 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.929 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.930 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.931 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.931 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.932 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.978 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.979 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.980 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.981 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.982 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.983 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:34.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.024 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.025 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.025 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.027 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.028 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.079 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.079 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.080 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.081 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.082 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.084 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.086 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.086 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.086 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.087 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.112 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.113 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.115 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.116 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.131 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.132 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.133 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.134 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.225 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.226 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.227 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.229 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.258 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.259 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.259 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.260 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.262 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.263 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.264 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.265 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.286 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.295 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.300 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.304 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.305 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.306 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.310 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.311 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.313 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.314 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.348 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.349 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.350 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.372 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.373 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.374 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.376 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.377 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.456 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.457 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.458 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.463 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.465 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.466 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.485 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.486 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.488 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.489 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.490 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.490 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.493 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.494 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.504 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.505 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.506 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.508 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.509 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.511 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.558 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.564 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.564 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.565 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.610 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.610 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.613 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.663 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.664 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.665 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.667 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.668 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.705 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.706 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.720 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.721 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.763 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.764 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.766 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.767 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.767 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.793 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.794 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.796 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.797 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.823 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.824 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.826 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.826 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.827 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.833 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.834 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.836 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.837 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.867 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.868 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.868 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.868 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.870 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.871 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.899 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.900 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.902 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.903 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.914 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.915 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.916 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.917 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.919 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.920 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.920 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.950 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.951 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.951 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.951 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.953 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.954 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.954 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.970 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.970 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.970 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.971 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.972 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.972 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.973 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:35.973 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.007 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.008 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.011 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.051 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.054 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.054 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.055 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.128 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.129 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.148 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.151 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.152 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.156 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.163 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.169 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.175 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.177 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.179 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.180 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.181 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.195 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.196 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.196 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.197 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.199 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.200 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.233 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.234 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.236 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.237 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.237 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.240 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.240 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.241 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.242 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.244 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.245 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.245 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.246 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.283 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.284 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.285 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.285 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.287 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.288 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.293 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.294 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.296 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.297 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.298 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.335 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.336 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.337 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.338 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.339 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.340 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.345 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.346 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.347 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.351 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.352 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.378 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.379 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.380 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.381 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.382 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.497 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.500 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.556 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.584 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.586 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.604 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.605 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.605 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.606 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.608 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.609 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.611 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.612 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.614 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.615 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.621 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.622 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.624 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.625 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.628 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.629 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.630 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.632 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.633 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.635 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.636 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.636 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.636 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.638 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.639 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.684 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.707 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.708 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.709 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.711 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.712 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.731 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.768 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.769 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.769 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.772 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.812 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.813 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.815 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.816 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.816 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.816 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.984 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.985 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.986 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.987 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.990 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.990 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.991 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.998 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:36.999 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-01 00:34:37.000 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "followup = \"What are the 3 advantages of attention?\"\n",
    "rag_model = rag_result(followup, result['flattened_docs'], result['summary'])\n",
    "\n",
    "async def stream_response():\n",
    "    response_placeholder = st.empty()\n",
    "    full_response = \"\"\n",
    "\n",
    "    async for chunk in rag_model.astream({\n",
    "        'question': followup, \n",
    "        'document': result['flattened_docs'], \n",
    "        'summary': result['summary']\n",
    "    }):\n",
    "        full_response += chunk  # Accumulate streamed content\n",
    "        response_placeholder.write(full_response)  # Update Streamlit UI in real time\n",
    "\n",
    "await (stream_response())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.\\nWe address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.\\nTimeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers\\' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\\nPage: Attention Is All You Need\\nSummary: \"Attention Is All You Need\" is a 2017 landmark research paper in machine learning authored by eight scientists working at Google. The paper introduced a new deep learning architecture known as the transformer, based on the attention mechanism proposed in 2014 by Bahdanau et al. It is considered a foundational paper in modern artificial intelligence, as the transformer approach has become the main architecture of large language models like those based on GPT. At the time, the focus of the research was on improving Seq2seq techniques for machine translation, but the authors go further in the paper, foreseeing the technique\\'s potential for other tasks like question answering and what is now known as multimodal Generative AI.\\nThe paper\\'s title is a reference to the song \"All You Need Is Love\" by the Beatles. The name \"Transformer\" was picked because Jakob Uszkoreit, one of the paper\\'s authors, liked the sound of that word.\\nAn early design document was titled \"Transformers: Iterative Self-Attention and Processing for Various Tasks\", and included an illustration of six characters from the Transformers animated show. The team was named Team Transformer.\\nSome early examples that the team tried their Transformer architecture on included English-to-German translation, generating Wikipedia articles on \"The Transformer\", and parsing. These convinced the team that the Transformer is a general purpose language model, and not just good for translation.\\nAs of 2024, the paper has been cited more than 140,000 times.\\n\\nPage: All You Need Is Kill\\nSummary: All You Need Is Kill is a Japanese science fiction light novel by Hiroshi Sakurazaka with illustrations by Yoshitoshi Abe. The book was published in Japanese by Shueisha under their Super Dash Bunko imprint in December 2004, and was later released in English by Viz Media under their Haikasoru imprint. All You Need Is Kill follows a soldier named Keiji Kiriya, who, after dying in a battle with extraterrestrials, is caught in a time loop that makes him live the same day repeatedly, allowing Kiriya to improve his fighting skills.\\nA manga adaptation, written by Rysuke Takeuchi and illustrated by Takeshi Obata, was serialized in Shueisha\\'s Weekly Young Jump magazine between January and May 2014 and was also published by Viz Media in its Weekly Shonen Jump magazine. In November 2014, the Viz translation was released in a collected edition that included the entire series. A separate graphic novel adaptation, written by Nick Mamatas and illustrated by Lee Ferguson, was released in North America in May 2014. A film adaptation from director Doug Liman starring Tom Cruise and Emily Blunt, titled Edge of Tomorrow, was released in May 2014. The English-language film tie-in edition of the novel also uses this title.\\nThe novel was Sakurazaka\\'s breakthrough science fiction novel, earning wide praise from fellow novelists including Yasutaka Tsutsui and Chhei Kanbayashi and was entered in contention for the Best Japanese Long Work in the 36th Seiun Awards in 2005.\\n\\nPage: All You Need Is Love\\nSummary: \"All You Need Is Love\" is a song by the English rock band the Beatles that was released as a non-album single in July 1967, with \"Baby, You\\'re a Rich Man\" as its B-side. It was written by John Lennon and credited to the LennonMcCartney partnership. The song was Britain\\'s contribution to Our World, the first live global television link, for which the band were shown performing it at EMI Studios in London on 25 June. The programme was broadcast via satellite and seen by an audience of over 400 million in 25 countries. Lennon\\'s lyrics were deliberately simplistic, to allow for broad appeal to the show\\'s international audience, and captured the utopian ideals associated with the Summer of Love. The single topped sales charts in Britain, the United States and many other countries, and became an anthem for the counterculture\\'s embrace of flower power philosophy.\\nOur World coincide\\nIn just 3 minutes help us improve arXiv: cs arXiv:2501.05730v1 arXiv author ID The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms. Subjects:   Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as:    arXiv:2501.05730 [cs.LG] (or arXiv:2501.05730v1 [cs.LG] for this version) From: Guoxin Feng [view email] Access Paper: cs.LG cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status \\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\\n\".join([doc.metadata.get('Summary', '') if 'Summary' in doc.metadata else doc.page_content for doc in result['flattened_docs'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In just 3 minutes help us improve arXiv: cs arXiv:2501.05730v1 arXiv author ID The next-generation architecture, aiming at retaining the competitive performance of SA while achieving low-cost inference and efficient long-sequence training, primarily focuses on three approaches: linear attention, linear RNNs, and state space models. Furthermore, the element-wise attention circumvents the performance degradation factors present in these approaches and achieves performance comparable to SA in both causal and non-causal forms. Subjects:   Machine Learning (cs.LG); Artificial Intelligence (cs.AI) Cite as:    arXiv:2501.05730 [cs.LG] (or arXiv:2501.05730v1 [cs.LG] for this version) From: Guoxin Feng [view email] Access Paper: cs.LG cs cs.AI References & Citations Bibliographic and Citation Tools Bibliographic Explorer Toggle Connected Papers Toggle Which authors of this paper are endorsers? arXiv Operational Status '"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['flattened_docs'][4].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc_academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
